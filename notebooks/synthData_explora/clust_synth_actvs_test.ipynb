{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5I3Ss__1VVEZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_blobs, make_circles\n",
        "from scipy.stats import norm\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# %pip install einops\n",
        "# from einops import einsum"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ground_truth_feats(\n",
        "    # distrb_type: str = 'gauss_mix_model',\n",
        "    model_dims: int = 256,\n",
        "    feature_dims: int = 512,\n",
        "    num_clusters: int = 10,\n",
        "    device: torch.device = torch.device('cpu'),\n",
        ") -> torch.Tensor:\n",
        "    dtype = torch.float32\n",
        "    # if distrb_type == 'gauss_mix_model':\n",
        "    cluster_data, _ = make_blobs(n_samples=model_dims, centers=num_clusters, n_features=feature_dims)\n",
        "    grTrue_feats = torch.tensor(cluster_data, device=device, dtype=dtype)\n",
        "    return grTrue_feats"
      ],
      "metadata": {
        "id": "Tdr-XiFEGDgR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_synth_actvs(\n",
        "        grTrue_feats: torch.Tensor,\n",
        "        total_data_points: int = 100000,\n",
        "        avg_active_features: int = 16,\n",
        "        batch_size: int = 1000\n",
        "    ) -> torch.Tensor:\n",
        "    device = grTrue_feats.device\n",
        "    dtype = torch.float32\n",
        "\n",
        "    h = grTrue_feats.shape[0]  # model dimensions\n",
        "    G = grTrue_feats.shape[1]  # number of ground truth features\n",
        "\n",
        "    # for step 1; same for each batch\n",
        "    # created a random covariance matrix for a multivariate normal distribution with zero mean\n",
        "    A = np.random.rand(G, G)  # rand correlations for each feature\n",
        "    cov_matrix = np.dot(A, A.transpose())\n",
        "    mean = np.zeros(G)\n",
        "\n",
        "    # for step 3; same for each batch\n",
        "    decay_rate = 0.99  # lambda (may put this in loop for ease of code reading)\n",
        "    feat_indices = np.arange(G)\n",
        "    decayed_indices = feat_indices * decay_rate\n",
        "\n",
        "    synth_actvs_batches = []\n",
        "    # for batch_ind in tqdm(range(0, total_data_points, batch_size), desc=\"Generating Activation Samples\"):\n",
        "    for _ in tqdm(range(0, total_data_points, batch_size), desc=\"Generating Activation Samples\"):\n",
        "        # 1. Correlated (for each feature in sample vector of size G)\n",
        "        # a single sample from a correlated multivariate normal distribution and,\n",
        "        # samples are on a scale defined by the normal distribution's probability density fn (PDF)\n",
        "        batch_size_current = min(batch_size, total_data_points - len(synth_actvs_batches))\n",
        "        samples = np.random.multivariate_normal(mean, cov_matrix, batch_size_current) # (batchNumSamps, G) with correlations\n",
        "\n",
        "        # for each dimension of that sample, found where that sample lay on the standard normal cumulative distribution function\n",
        "        uniform_samples = norm.cdf(samples)  # (batchNumSamps, G) is where each samp lies on (0,1) range in cumulative dist fn (CDF)\n",
        "\n",
        "        # 2. Decayed (for each feature in sample vector of size G)\n",
        "        # probability of the G-dimensional random variable exponentially decayed with the feature’s index\n",
        "        # Since uniform_samps is a (batchNumSamps, G) matrix, and indices * decay_rate is a 1D array of length G,\n",
        "            # NumPy will broadcast the operation across each row of uniform_samps.\n",
        "        decayed_probs = uniform_samples ** (decayed_indices)  # prob of each samp's feature expo decays to power of ind*0.99\n",
        "\n",
        "        # 3. Rescaled (for each feature in sample vector of size G)\n",
        "        # Rescale probabilities to ensure on avg only \"avg_active_features\" num of ground truth features are active at a time.\n",
        "        # this changes the avg so (avg_active_features / G) are active\n",
        "        # scaling_factor: denom is what to cancel out (replace) and numer is what to replace with\n",
        "        mean_prob = np.mean(decayed_probs) #  calculated the mean probability of all features\n",
        "        scaling_factor = (avg_active_features / G) / mean_prob #  calculated the ratio of the number of ground truth features that are active at a time to the mean probability\n",
        "        rescaled_probs = decayed_probs * scaling_factor # multiplied each probability by this ratio to rescale them\n",
        "        rescaled_probs_tensor = torch.tensor(rescaled_probs, device=device, dtype=dtype)\n",
        "\n",
        "        # 4. parameterize a vector of Bernoulli random variables (for sparse coefficients):\n",
        "        # want expectation of this vector to have \"avg_active_features\" 1s\n",
        "        # given probs for each index, bernoulli draws a vector of 0s and 1s using those probs\n",
        "        binary_sparse_coeffs = torch.bernoulli(rescaled_probs_tensor)\n",
        "\n",
        "        # 5. use the sparse coefficients to linearly combine a sparse selection of the ground truth features\n",
        "        synth_activations = torch.matmul(binary_sparse_coeffs, grTrue_feats.T.to(dtype))\n",
        "        # synth_activations = torch.einsum('ij,kj->ik', binary_sparse_coeffs, grTrue_feats.to(dtype))\n",
        "\n",
        "        synth_actvs_batches.append(synth_activations)\n",
        "\n",
        "    return torch.cat(synth_actvs_batches, dim=0)  # stack batches along rows (dim=0)"
      ],
      "metadata": {
        "id": "gXqOmVxLQIgQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grTrue_feats = get_ground_truth_feats(256, 512, 10, device) # 'clust', # hxG\n",
        "total_data_points = 10000 # 100000000\n",
        "avg_active_features = 16\n",
        "\n",
        "synth_activations = get_synth_actvs(grTrue_feats, total_data_points, avg_active_features)\n",
        "print('\\n', synth_activations.shape)"
      ],
      "metadata": {
        "id": "MwAavBmFn8HN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6add1d26-9766-4d84-f2c3-ab88521c5e04"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generating Activation Samples: 100%|██████████| 10/10 [00:02<00:00,  4.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " torch.Size([10000, 256])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}