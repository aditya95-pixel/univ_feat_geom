{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","toc_visible":true,"authorship_tag":"ABX9TyNfyo1k3tlqORkILKgeeMUS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"b39e7b0ad7cc4a74aeb9557788828b6c":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_3661aebbb3ed42499e2d573df667d8e5","IPY_MODEL_1e2184565afa4dcb9a01a0bbdfaeace2","IPY_MODEL_84c4a32b1203414a9af4acc7f43bbb7a","IPY_MODEL_6983d842bd754d9c82056c40ed3348ea"],"layout":"IPY_MODEL_12a2089a8cc842288b04f21e6d07a8e6"}},"6424b878bd174f478f588a29f6381f8f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_300bc5ae3ed0408cb2c8bdfe52c9291b","placeholder":"​","style":"IPY_MODEL_8852677546c74feaa210b81adec34444","value":"<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"}},"098ead6b513143cda629bcc71085ff53":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"PasswordModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"PasswordView","continuous_update":true,"description":"Token:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_8d8c21d9a65541e2885a99bcb59b6d9e","placeholder":"​","style":"IPY_MODEL_3c071699dee64d43a07deecceed20379","value":""}},"1c2589c4d8c1456ea59e5b87b0083967":{"model_module":"@jupyter-widgets/controls","model_name":"CheckboxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"CheckboxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"CheckboxView","description":"Add token as git credential?","description_tooltip":null,"disabled":false,"indent":true,"layout":"IPY_MODEL_68b18d0721ea4d16ab893aad0c240a7c","style":"IPY_MODEL_9f5d9d1efcb14e9fa9f9eee759fc28ef","value":true}},"cca1f49505c64871bb9cf5d83f351df2":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Login","disabled":false,"icon":"","layout":"IPY_MODEL_93a1df240de04bc089437981160b2e10","style":"IPY_MODEL_92f34e8e49e24ebeb98d5572b84bca14","tooltip":""}},"bc09a7bcff89496b804f9652dd2eae60":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_298a17f8a9e0401092342ce60a7ac3a0","placeholder":"​","style":"IPY_MODEL_a52d20ee2c834c86804f61980cb4f0ac","value":"\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"}},"12a2089a8cc842288b04f21e6d07a8e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":"center","align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":"column","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"50%"}},"300bc5ae3ed0408cb2c8bdfe52c9291b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8852677546c74feaa210b81adec34444":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8d8c21d9a65541e2885a99bcb59b6d9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c071699dee64d43a07deecceed20379":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"68b18d0721ea4d16ab893aad0c240a7c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9f5d9d1efcb14e9fa9f9eee759fc28ef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93a1df240de04bc089437981160b2e10":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92f34e8e49e24ebeb98d5572b84bca14":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"298a17f8a9e0401092342ce60a7ac3a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a52d20ee2c834c86804f61980cb4f0ac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"657ac7bd409a4a02be205626922514b5":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b77ff9d59ab4a95b17331491acf82a1","placeholder":"​","style":"IPY_MODEL_73ea9e05f0674116a9b00abcb0ffa637","value":"Connecting..."}},"3b77ff9d59ab4a95b17331491acf82a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73ea9e05f0674116a9b00abcb0ffa637":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3661aebbb3ed42499e2d573df667d8e5":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1950423f3204c5c977279b8c3dcd70c","placeholder":"​","style":"IPY_MODEL_4525c2b50ace4eb3a664370352cf3095","value":"Token is valid (permission: write)."}},"1e2184565afa4dcb9a01a0bbdfaeace2":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b9c37cac9514bcfb7f159af08bff9fd","placeholder":"​","style":"IPY_MODEL_1a0e37492b014abf8c551a7d6a8fc71d","value":"Your token has been saved in your configured git credential helpers (store)."}},"84c4a32b1203414a9af4acc7f43bbb7a":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19b3f4818b549cfb1b3025e8ac172e0","placeholder":"​","style":"IPY_MODEL_6472f5b1188a42f6b1ff7c4ac32586d7","value":"Your token has been saved to /root/.cache/huggingface/token"}},"6983d842bd754d9c82056c40ed3348ea":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d060504927c34f0c812762cd20f7c32e","placeholder":"​","style":"IPY_MODEL_b721bc9f7c984c03ad0283a5114aa53f","value":"Login successful"}},"b1950423f3204c5c977279b8c3dcd70c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4525c2b50ace4eb3a664370352cf3095":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b9c37cac9514bcfb7f159af08bff9fd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0e37492b014abf8c551a7d6a8fc71d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19b3f4818b549cfb1b3025e8ac172e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6472f5b1188a42f6b1ff7c4ac32586d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d060504927c34f0c812762cd20f7c32e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b721bc9f7c984c03ad0283a5114aa53f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"09a7931afe4b44f09ef3980ae7056e29":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d523b0bc010b4c899763d89816a3cb5c","IPY_MODEL_c7bde1ec4ebc4fd98d88fabcab554c65","IPY_MODEL_342c5972ba9f47c5939b8ac474936580"],"layout":"IPY_MODEL_f684342c6c034177a6e1645f301b9fac"}},"d523b0bc010b4c899763d89816a3cb5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8ce0eaf3b9394576b669819fa5175d4e","placeholder":"​","style":"IPY_MODEL_3ea58ec0c57d4127baa394b7cfab8fb4","value":"Loading checkpoint shards: 100%"}},"c7bde1ec4ebc4fd98d88fabcab554c65":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d8c3f02062d74e378d6e669724abd974","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ad32527a0eb46a590b93f9c4004e515","value":3}},"342c5972ba9f47c5939b8ac474936580":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b343821de3434e6d87ddd027131cd7d3","placeholder":"​","style":"IPY_MODEL_89abc024fe844c8883ef7d8b87b5d6b9","value":" 3/3 [00:04&lt;00:00,  1.19s/it]"}},"f684342c6c034177a6e1645f301b9fac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8ce0eaf3b9394576b669819fa5175d4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ea58ec0c57d4127baa394b7cfab8fb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d8c3f02062d74e378d6e669724abd974":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ad32527a0eb46a590b93f9c4004e515":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b343821de3434e6d87ddd027131cd7d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89abc024fe844c8883ef7d8b87b5d6b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcfe563067884ac291d69de1364b0b9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_77d445127c1546829d415e538319b0de","IPY_MODEL_2dbda754987646179586d669faaacaee","IPY_MODEL_04105033ba4f4a22914f17c62b3864eb"],"layout":"IPY_MODEL_38b6fed3397b40aa9ad434cb9ddca8e4"}},"77d445127c1546829d415e538319b0de":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2179988cd919431195698e3bbd852377","placeholder":"​","style":"IPY_MODEL_fc4055b9799b44a4bd0891265d8e27fb","value":"params.npz: 100%"}},"2dbda754987646179586d669faaacaee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5a5b78f34eb4453a596859bd20543e3","max":302131416,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7ef3e7ab029f4e488407923180ef44bc","value":302131416}},"04105033ba4f4a22914f17c62b3864eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fdf9df74a3db4f88ad8cfe774cfaec48","placeholder":"​","style":"IPY_MODEL_56b059d0797e4bdeb729fab88f402208","value":" 302M/302M [00:01&lt;00:00, 264MB/s]"}},"38b6fed3397b40aa9ad434cb9ddca8e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2179988cd919431195698e3bbd852377":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc4055b9799b44a4bd0891265d8e27fb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b5a5b78f34eb4453a596859bd20543e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7ef3e7ab029f4e488407923180ef44bc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fdf9df74a3db4f88ad8cfe774cfaec48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56b059d0797e4bdeb729fab88f402208":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d66a6dcabf584889ac001907219fc8d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2510e86c8b99435d854c66289e7c0232","IPY_MODEL_34a4a3b75a3048bcbbd41d9a406aa8f0","IPY_MODEL_4914437272c94aa7a7c4776f232ae728"],"layout":"IPY_MODEL_a6bee4f0563748eabd4c89d7a9211c5b"}},"2510e86c8b99435d854c66289e7c0232":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_15a9fb4a0fe24b3e8958a1054f3f85bf","placeholder":"​","style":"IPY_MODEL_a46252fc50a744a8a5abc5e00a43e7cb","value":"Loading checkpoint shards: 100%"}},"34a4a3b75a3048bcbbd41d9a406aa8f0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3362157ac077431c80dd5421951f5c89","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_19131713e19a4624b2ceeccd8cb7867c","value":3}},"4914437272c94aa7a7c4776f232ae728":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8b2f493a6f64b3997cd6eab97e2cc38","placeholder":"​","style":"IPY_MODEL_f7c74b817ac44a668b8ce7c19999d752","value":" 3/3 [00:04&lt;00:00,  1.17s/it]"}},"a6bee4f0563748eabd4c89d7a9211c5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a9fb4a0fe24b3e8958a1054f3f85bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a46252fc50a744a8a5abc5e00a43e7cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3362157ac077431c80dd5421951f5c89":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"19131713e19a4624b2ceeccd8cb7867c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f8b2f493a6f64b3997cd6eab97e2cc38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7c74b817ac44a668b8ce7c19999d752":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"df10fd7618d94e4d82b550327262027e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_be89c1f746474a36886966b347d7da2d","IPY_MODEL_b3bbf8781c034182a5d10cf7e021508b","IPY_MODEL_3d7f272623624f86b638df8c2173def8"],"layout":"IPY_MODEL_349e77c234754accb437b462e89dfec8"}},"be89c1f746474a36886966b347d7da2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50cc32250e6b4c5d899baf92ba7d48f9","placeholder":"​","style":"IPY_MODEL_0193bc6a2ff7403ea9c2beb75ac183dc","value":"config.json: 100%"}},"b3bbf8781c034182a5d10cf7e021508b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06c7d590f3734392b062b6c661c93161","max":818,"min":0,"orientation":"horizontal","style":"IPY_MODEL_98b1af11da384a90a3bc03becb944080","value":818}},"3d7f272623624f86b638df8c2173def8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_891f778d7da44ce79af59068975e9c48","placeholder":"​","style":"IPY_MODEL_0bbe1506db6446cea742a33965a5b21e","value":" 818/818 [00:00&lt;00:00, 65.5kB/s]"}},"349e77c234754accb437b462e89dfec8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"50cc32250e6b4c5d899baf92ba7d48f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0193bc6a2ff7403ea9c2beb75ac183dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06c7d590f3734392b062b6c661c93161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98b1af11da384a90a3bc03becb944080":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"891f778d7da44ce79af59068975e9c48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bbe1506db6446cea742a33965a5b21e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ad1bb51252a4e38be6056e61a601cb3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fed3d9711b9f4cf5ac8199a9275f1dcf","IPY_MODEL_b1d87fae274d4142920b07a398bde156","IPY_MODEL_2d6cf6154b174ca0aaaf2801fc198616"],"layout":"IPY_MODEL_cc29ef7677da4ad98407c1769d348b0b"}},"fed3d9711b9f4cf5ac8199a9275f1dcf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dfe4278092b4c24bf1d52c039c3a6df","placeholder":"​","style":"IPY_MODEL_ea8dc13d679a4709b349352cbb4f8ab4","value":"model.safetensors.index.json: 100%"}},"b1d87fae274d4142920b07a398bde156":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa28ff5c942441b2af3c51801422ae3d","max":24224,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1ea01866dae4d40b6a5539ff8f17dcf","value":24224}},"2d6cf6154b174ca0aaaf2801fc198616":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f5d16a0fbd1424c8a30a8f6bcf0eea9","placeholder":"​","style":"IPY_MODEL_75c1db6ec810438ca1aad337942ede1c","value":" 24.2k/24.2k [00:00&lt;00:00, 1.89MB/s]"}},"cc29ef7677da4ad98407c1769d348b0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dfe4278092b4c24bf1d52c039c3a6df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ea8dc13d679a4709b349352cbb4f8ab4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fa28ff5c942441b2af3c51801422ae3d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1ea01866dae4d40b6a5539ff8f17dcf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9f5d16a0fbd1424c8a30a8f6bcf0eea9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75c1db6ec810438ca1aad337942ede1c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf64fb4c744441249afa9c12dab787ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f6d2291378b54eabbd90ab63ba29269a","IPY_MODEL_d8cb9fe352554550b22c75cbbdeec93f","IPY_MODEL_7042f4c5bdee489a9f20c71a75b81592"],"layout":"IPY_MODEL_9ad0348892d04d588c02ecd1e383ba48"}},"f6d2291378b54eabbd90ab63ba29269a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df989a6d89ee42b1a63a98c08cbe4faa","placeholder":"​","style":"IPY_MODEL_fac86c64d8f349c39b26c4b6c084c1e8","value":"Downloading shards: 100%"}},"d8cb9fe352554550b22c75cbbdeec93f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_170c943f64294423a4a44af5ffde8970","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1fea719e4e842e8b6f890836d41caf7","value":3}},"7042f4c5bdee489a9f20c71a75b81592":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_900a876482e9421897e1d8b1037f1ade","placeholder":"​","style":"IPY_MODEL_5c116d5a2e984a7e870d5929f20ce229","value":" 3/3 [01:28&lt;00:00, 24.91s/it]"}},"9ad0348892d04d588c02ecd1e383ba48":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df989a6d89ee42b1a63a98c08cbe4faa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fac86c64d8f349c39b26c4b6c084c1e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"170c943f64294423a4a44af5ffde8970":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1fea719e4e842e8b6f890836d41caf7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"900a876482e9421897e1d8b1037f1ade":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c116d5a2e984a7e870d5929f20ce229":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0974b18fde94e2aaded81eb0fff2b02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a733fd1a465f4284806232465e1c7016","IPY_MODEL_9253944d601743bfa4519e0f4fe521d6","IPY_MODEL_cff7a2ee929b4c59872566928fc91ca0"],"layout":"IPY_MODEL_5bc27551edea41f7b5d358685f06a7ef"}},"a733fd1a465f4284806232465e1c7016":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_12fb0a0406444ba089a9c7cfef9326b4","placeholder":"​","style":"IPY_MODEL_916ee5a51f7a4fd3aa6d98185dc40ae2","value":"model-00001-of-00003.safetensors: 100%"}},"9253944d601743bfa4519e0f4fe521d6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f72b4a5d01a847bda0c1111f62c3710c","max":4992576136,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ac0f6771f111495b962ebd9fcb0a9e66","value":4992576136}},"cff7a2ee929b4c59872566928fc91ca0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8df2798e6a0a4df287c0cbb4d421a5ae","placeholder":"​","style":"IPY_MODEL_fe53c338a5cb403ba0285983ce593afc","value":" 4.99G/4.99G [00:40&lt;00:00, 121MB/s]"}},"5bc27551edea41f7b5d358685f06a7ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12fb0a0406444ba089a9c7cfef9326b4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"916ee5a51f7a4fd3aa6d98185dc40ae2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f72b4a5d01a847bda0c1111f62c3710c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ac0f6771f111495b962ebd9fcb0a9e66":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8df2798e6a0a4df287c0cbb4d421a5ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe53c338a5cb403ba0285983ce593afc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fba7b00b60145c78ed88ab4457c2d33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ea40470ff04466a824d4cd01d120161","IPY_MODEL_386ad95352b24782b7f77cf0a0daa064","IPY_MODEL_6ea70caa9bf243c7b961af94f99ca5ee"],"layout":"IPY_MODEL_c6e852b868d54c74ba79d2beacb0ee4a"}},"0ea40470ff04466a824d4cd01d120161":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65b071bf21e54779a9e7a8ceb17adf8b","placeholder":"​","style":"IPY_MODEL_2604e449cd754ecb814452f390207660","value":"model-00002-of-00003.safetensors: 100%"}},"386ad95352b24782b7f77cf0a0daa064":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_20892b64e194499389b2748dcdae433a","max":4983443424,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7e957f74cf444cbe8ac40b7517580cb5","value":4983443424}},"6ea70caa9bf243c7b961af94f99ca5ee":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_aab3bc7260f44c37b04e262c2b0319e2","placeholder":"​","style":"IPY_MODEL_bfdc9cfaf5ba4fc29b69244ebdd2ccc4","value":" 4.98G/4.98G [00:42&lt;00:00, 124MB/s]"}},"c6e852b868d54c74ba79d2beacb0ee4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65b071bf21e54779a9e7a8ceb17adf8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2604e449cd754ecb814452f390207660":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"20892b64e194499389b2748dcdae433a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e957f74cf444cbe8ac40b7517580cb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"aab3bc7260f44c37b04e262c2b0319e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfdc9cfaf5ba4fc29b69244ebdd2ccc4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"959850ea258d4dc6a81cc8238e53ce33":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_10e3065a87154d22be886c26c0640d2e","IPY_MODEL_152631b0a7a146b88089c2f0201249d1","IPY_MODEL_e1707dd2527747dca53a28ed35c2206b"],"layout":"IPY_MODEL_ed60dc3946fc405a800f76ba4af0a402"}},"10e3065a87154d22be886c26c0640d2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10e369c1e338411f82766c23fd727729","placeholder":"​","style":"IPY_MODEL_a8cb9e9e71a54fc2aabd23304e525d6e","value":"model-00003-of-00003.safetensors: 100%"}},"152631b0a7a146b88089c2f0201249d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9601e519fac0401ea9e06a3c78fbb74d","max":481381384,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ab920d0dd6744949b5e0f91a076a7280","value":481381384}},"e1707dd2527747dca53a28ed35c2206b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a91d9d4d5734d348139c825c38cfe2e","placeholder":"​","style":"IPY_MODEL_d1a18238dc62457e84b5f3f9d19eb0c1","value":" 481M/481M [00:04&lt;00:00, 125MB/s]"}},"ed60dc3946fc405a800f76ba4af0a402":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10e369c1e338411f82766c23fd727729":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8cb9e9e71a54fc2aabd23304e525d6e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9601e519fac0401ea9e06a3c78fbb74d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab920d0dd6744949b5e0f91a076a7280":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a91d9d4d5734d348139c825c38cfe2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1a18238dc62457e84b5f3f9d19eb0c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dbc77b0202849139641db05c3e9379b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_306c7d6d143f43bd86ec119371a4521d","IPY_MODEL_6b7b99faefc44387be4e5ad64a2aa68f","IPY_MODEL_316d2d30c18b48b68f47e21b3a02b88e"],"layout":"IPY_MODEL_223a3752aae6429983bb7009e3fcafb2"}},"306c7d6d143f43bd86ec119371a4521d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8611a5c7a2c04ccf95cbaae523a4f807","placeholder":"​","style":"IPY_MODEL_91867d4503b24922b08bc4180722cae1","value":"Loading checkpoint shards: 100%"}},"6b7b99faefc44387be4e5ad64a2aa68f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d06aa763742c4b0d9aee2bc95f1a2ab8","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f4f9a9c3910c467fb413112fa9272f86","value":3}},"316d2d30c18b48b68f47e21b3a02b88e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaadf2f6f6ad4ed2aea7c1bfde84a8d6","placeholder":"​","style":"IPY_MODEL_b58354992ab84816b1268b0570091bee","value":" 3/3 [00:01&lt;00:00,  1.76it/s]"}},"223a3752aae6429983bb7009e3fcafb2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8611a5c7a2c04ccf95cbaae523a4f807":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91867d4503b24922b08bc4180722cae1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d06aa763742c4b0d9aee2bc95f1a2ab8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4f9a9c3910c467fb413112fa9272f86":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eaadf2f6f6ad4ed2aea7c1bfde84a8d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58354992ab84816b1268b0570091bee":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea5fd0e9e9b64b4cbb5163908a726ab2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45e735be694a49e78b8aae6e18f1f1c1","IPY_MODEL_a56d6922eafa42f2af7a2927526257f4","IPY_MODEL_1d2e10f83e97467c96669b2e8d9d854c"],"layout":"IPY_MODEL_22a3b323cf354fb5aa5964b96ebaa327"}},"45e735be694a49e78b8aae6e18f1f1c1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a3bde62c83d0420c8b82a99f2e96bf8f","placeholder":"​","style":"IPY_MODEL_6b322f0f469a42d0817490af4c150b99","value":"generation_config.json: 100%"}},"a56d6922eafa42f2af7a2927526257f4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_16eb25dbb16643e28de4bf0123ff66ee","max":168,"min":0,"orientation":"horizontal","style":"IPY_MODEL_35fb0928ffa6432a84514335cff59b8c","value":168}},"1d2e10f83e97467c96669b2e8d9d854c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_633321cdcf8c441ab761e7df86790e4f","placeholder":"​","style":"IPY_MODEL_38f2feb48fac48128f65a98d1be0c650","value":" 168/168 [00:00&lt;00:00, 15.6kB/s]"}},"22a3b323cf354fb5aa5964b96ebaa327":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3bde62c83d0420c8b82a99f2e96bf8f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b322f0f469a42d0817490af4c150b99":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16eb25dbb16643e28de4bf0123ff66ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"35fb0928ffa6432a84514335cff59b8c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"633321cdcf8c441ab761e7df86790e4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"38f2feb48fac48128f65a98d1be0c650":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1897a553232043f4b0d87a15a4986f03":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_49bc08a6fd95497bb431952b1af7af7d","IPY_MODEL_268e7cc20f0e488cacd6823368e38581","IPY_MODEL_aff281c01f99455b9f9d8776faff137f"],"layout":"IPY_MODEL_b8259e39942c444d8a0692839ac92377"}},"49bc08a6fd95497bb431952b1af7af7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_68551e7604b14f2881271441d2e567dc","placeholder":"​","style":"IPY_MODEL_5c9367e260b5439abcae842782dfee5f","value":"tokenizer_config.json: 100%"}},"268e7cc20f0e488cacd6823368e38581":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_163f1ff368f942c4aeb80277b9e473e1","max":46379,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2121e280f2b4643a444932b174cabaf","value":46379}},"aff281c01f99455b9f9d8776faff137f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d787a3873e264d47b178ccb28b5604ea","placeholder":"​","style":"IPY_MODEL_941a6e6eeda148898ff376196f963f83","value":" 46.4k/46.4k [00:00&lt;00:00, 3.33MB/s]"}},"b8259e39942c444d8a0692839ac92377":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68551e7604b14f2881271441d2e567dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5c9367e260b5439abcae842782dfee5f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"163f1ff368f942c4aeb80277b9e473e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2121e280f2b4643a444932b174cabaf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d787a3873e264d47b178ccb28b5604ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"941a6e6eeda148898ff376196f963f83":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c4c77bb05124888a4499a4ced1f9014":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0cc94fb85ee4cbf856dd82f8d118a58","IPY_MODEL_4fb59904c5974b0bb4c7b2a84e35b54a","IPY_MODEL_a067c5ab6a644b3a9a4dee3e231e728d"],"layout":"IPY_MODEL_08fc7e8c16e345ccb9aa451063f28241"}},"e0cc94fb85ee4cbf856dd82f8d118a58":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d04a3dabc625428cbb185b2d32c01d15","placeholder":"​","style":"IPY_MODEL_abceafcbb1c04ec88ffbaf7931951652","value":"tokenizer.model: 100%"}},"4fb59904c5974b0bb4c7b2a84e35b54a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9502e7ce48424bdab3350925a9cd2986","max":4241003,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd10d3777ccb4c6f8493f81bacbfda20","value":4241003}},"a067c5ab6a644b3a9a4dee3e231e728d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c546dfc37d5d4fb6b196d399580c01d9","placeholder":"​","style":"IPY_MODEL_3acf0139cba94ba580c7fa8b91438829","value":" 4.24M/4.24M [00:00&lt;00:00, 82.0MB/s]"}},"08fc7e8c16e345ccb9aa451063f28241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d04a3dabc625428cbb185b2d32c01d15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abceafcbb1c04ec88ffbaf7931951652":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9502e7ce48424bdab3350925a9cd2986":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd10d3777ccb4c6f8493f81bacbfda20":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c546dfc37d5d4fb6b196d399580c01d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3acf0139cba94ba580c7fa8b91438829":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1be8f31d191943198e128a64321f32dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6e6b45f4e33148a3a9468098bce438ac","IPY_MODEL_88a1164c9fe84227b5b6d5eebde3138e","IPY_MODEL_56deb9b37b9c468180dbf90248690409"],"layout":"IPY_MODEL_55bc2fa34a4045d4b52229d56e246f0a"}},"6e6b45f4e33148a3a9468098bce438ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a7792f2f19b4d30baab0634e150f2c5","placeholder":"​","style":"IPY_MODEL_ab9ecb65f9fa41749530aec5860bb962","value":"tokenizer.json: 100%"}},"88a1164c9fe84227b5b6d5eebde3138e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b36c3779ba484b88b04c0b211eacdae0","max":17525357,"min":0,"orientation":"horizontal","style":"IPY_MODEL_859085765d574de09940085226dd2b46","value":17525357}},"56deb9b37b9c468180dbf90248690409":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_28079e919820475b94ad495808852b7d","placeholder":"​","style":"IPY_MODEL_59bf11ad72774cd58cd39fa069572baf","value":" 17.5M/17.5M [00:00&lt;00:00, 116MB/s]"}},"55bc2fa34a4045d4b52229d56e246f0a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a7792f2f19b4d30baab0634e150f2c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab9ecb65f9fa41749530aec5860bb962":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b36c3779ba484b88b04c0b211eacdae0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"859085765d574de09940085226dd2b46":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"28079e919820475b94ad495808852b7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59bf11ad72774cd58cd39fa069572baf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3295b4bacffb47bab18a3f09f0e66a0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a4750440e9d4b3d914086496a10ec40","IPY_MODEL_0e14135fb69c48c4afbf9ba1772f9f78","IPY_MODEL_8e39d00c88bc469c9e1a681b83132810"],"layout":"IPY_MODEL_d6727a0c4825402d99a75d196de3c95c"}},"2a4750440e9d4b3d914086496a10ec40":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c33e1b0ea72f4925bd41d36526f8138a","placeholder":"​","style":"IPY_MODEL_fbd37e25a53d4840905ba30e817fa52c","value":"special_tokens_map.json: 100%"}},"0e14135fb69c48c4afbf9ba1772f9f78":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_51ffccab09844ec39942f2176f45c441","max":636,"min":0,"orientation":"horizontal","style":"IPY_MODEL_903b270fef4848569a217f88f88ecb36","value":636}},"8e39d00c88bc469c9e1a681b83132810":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_20db55e3f48e4f52b4f56fbd0f8c73fe","placeholder":"​","style":"IPY_MODEL_23389ae0071f4da28fc6457aaf447a2d","value":" 636/636 [00:00&lt;00:00, 54.6kB/s]"}},"d6727a0c4825402d99a75d196de3c95c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c33e1b0ea72f4925bd41d36526f8138a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fbd37e25a53d4840905ba30e817fa52c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"51ffccab09844ec39942f2176f45c441":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"903b270fef4848569a217f88f88ecb36":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"20db55e3f48e4f52b4f56fbd0f8c73fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23389ae0071f4da28fc6457aaf447a2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"kUYRF57KNxrL"},"source":["# setup"]},{"cell_type":"code","source":["%%capture\n","# %pip install sae-lens\n","# %pip install umap-learn # cant use with transformelns?"],"metadata":{"id":"lQ0BRnvvZjjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import pdb"],"metadata":{"id":"-S2-hpbiQrfa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4H-vBfRa1pOo"},"outputs":[],"source":["import pickle\n","import numpy as np\n","\n","import torch\n","import matplotlib.pyplot as plt\n","\n","# from sae_lens import SAE\n","\n","from torch import nn, Tensor\n","# from jaxtyping import Float, Int\n","from typing import Optional, Callable, Union, List, Tuple\n","\n","# from datasets import load_dataset"]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"metadata":{"id":"yUWzqfKmaRXj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YOYUhykrUbI8"},"source":["## corr fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SaotL1OHcZ3_"},"outputs":[],"source":["def batched_correlation(reshaped_activations_A, reshaped_activations_B, batch_size=100):\n","    # Ensure tensors are on GPU\n","    if torch.cuda.is_available():\n","        reshaped_activations_A = reshaped_activations_A.to('cuda')\n","        reshaped_activations_B = reshaped_activations_B.to('cuda')\n","\n","    # Normalize columns of A\n","    mean_A = reshaped_activations_A.mean(dim=0, keepdim=True)\n","    std_A = reshaped_activations_A.std(dim=0, keepdim=True)\n","    normalized_A = (reshaped_activations_A - mean_A) / (std_A + 1e-8)  # Avoid division by zero\n","\n","    # Normalize columns of B\n","    mean_B = reshaped_activations_B.mean(dim=0, keepdim=True)\n","    std_B = reshaped_activations_B.std(dim=0, keepdim=True)\n","    normalized_B = (reshaped_activations_B - mean_B) / (std_B + 1e-8)  # Avoid division by zero\n","\n","    num_batches = (normalized_B.shape[1] + batch_size - 1) // batch_size\n","    max_values = []\n","    max_indices = []\n","\n","    for batch in range(num_batches):\n","        start = batch * batch_size\n","        end = min(start + batch_size, normalized_B.shape[1])\n","        batch_corr_matrix = torch.matmul(normalized_A.t(), normalized_B[:, start:end]) / normalized_A.shape[0]\n","        max_val, max_idx = batch_corr_matrix.max(dim=0)\n","        max_values.append(max_val)\n","        # max_indices.append(max_idx + start)  # Adjust indices for the batch offset\n","        max_indices.append(max_idx)  # Adjust indices for the batch offset\n","\n","        del batch_corr_matrix\n","        torch.cuda.empty_cache()\n","\n","    return torch.cat(max_indices), torch.cat(max_values)"]},{"cell_type":"markdown","metadata":{"id":"vlKdEehFvC86"},"source":["## sim fns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rl7IYESN1irP"},"outputs":[],"source":["import functools\n","from typing import Any, Callable, Dict, List, Tuple, Union\n","\n","import numpy as np\n","import numpy.typing as npt\n","import torch\n","\n","\n","def to_numpy_if_needed(*args: Union[torch.Tensor, npt.NDArray]) -> List[npt.NDArray]:\n","    def convert(x: Union[torch.Tensor, npt.NDArray]) -> npt.NDArray:\n","        return x if isinstance(x, np.ndarray) else x.numpy()\n","\n","    return list(map(convert, args))\n","\n","\n","def to_torch_if_needed(*args: Union[torch.Tensor, npt.NDArray]) -> List[torch.Tensor]:\n","    def convert(x: Union[torch.Tensor, npt.NDArray]) -> torch.Tensor:\n","        return x if isinstance(x, torch.Tensor) else torch.from_numpy(x)\n","\n","    return list(map(convert, args))\n","\n","\n","def adjust_dimensionality(\n","    R: npt.NDArray, Rp: npt.NDArray, strategy=\"zero_pad\"\n",") -> Tuple[npt.NDArray, npt.NDArray]:\n","    D = R.shape[1]\n","    Dp = Rp.shape[1]\n","    if strategy == \"zero_pad\":\n","        if D - Dp == 0:\n","            return R, Rp\n","        elif D - Dp > 0:\n","            return R, np.concatenate((Rp, np.zeros((Rp.shape[0], D - Dp))), axis=1)\n","        else:\n","            return np.concatenate((R, np.zeros((R.shape[0], Dp - D))), axis=1), Rp\n","    else:\n","        raise NotImplementedError()\n","\n","\n","def center_columns(R: npt.NDArray) -> npt.NDArray:\n","    return R - R.mean(axis=0)[None, :]\n","\n","\n","def normalize_matrix_norm(R: npt.NDArray) -> npt.NDArray:\n","    return R / np.linalg.norm(R, ord=\"fro\")\n","\n","\n","def sim_random_baseline(\n","    rep1: torch.Tensor, rep2: torch.Tensor, sim_func: Callable, n_permutations: int = 10\n",") -> Dict[str, Any]:\n","    torch.manual_seed(1234)\n","    scores = []\n","    for _ in range(n_permutations):\n","        perm = torch.randperm(rep1.size(0))\n","\n","        score = sim_func(rep1[perm, :], rep2)\n","        score = score if isinstance(score, float) else score[\"score\"]\n","\n","        scores.append(score)\n","\n","    return {\"baseline_scores\": np.array(scores)}\n","\n","\n","class Pipeline:\n","    def __init__(\n","        self,\n","        preprocess_funcs: List[Callable[[npt.NDArray], npt.NDArray]],\n","        similarity_func: Callable[[npt.NDArray, npt.NDArray], Dict[str, Any]],\n","    ) -> None:\n","        self.preprocess_funcs = preprocess_funcs\n","        self.similarity_func = similarity_func\n","\n","    def __call__(self, R: npt.NDArray, Rp: npt.NDArray) -> Dict[str, Any]:\n","        for preprocess_func in self.preprocess_funcs:\n","            R = preprocess_func(R)\n","            Rp = preprocess_func(Rp)\n","        return self.similarity_func(R, Rp)\n","\n","    def __str__(self) -> str:\n","        def func_name(func: Callable) -> str:\n","            return (\n","                func.__name__\n","                if not isinstance(func, functools.partial)\n","                else func.func.__name__\n","            )\n","\n","        def partial_keywords(func: Callable) -> str:\n","            if not isinstance(func, functools.partial):\n","                return \"\"\n","            else:\n","                return str(func.keywords)\n","\n","        return (\n","            \"Pipeline(\"\n","            + (\n","                \"+\".join(map(func_name, self.preprocess_funcs))\n","                + \"+\"\n","                + func_name(self.similarity_func)\n","                + partial_keywords(self.similarity_func)\n","            )\n","            + \")\"\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgcXEjdcXAOj"},"outputs":[],"source":["from typing import List, Set, Union\n","\n","import numpy as np\n","import numpy.typing as npt\n","import sklearn.neighbors\n","import torch\n","\n","# from llmcomp.measures.utils import to_numpy_if_needed\n","\n","\n","def _jac_sim_i(idx_R: Set[int], idx_Rp: Set[int]) -> float:\n","    return len(idx_R.intersection(idx_Rp)) / len(idx_R.union(idx_Rp))\n","\n","\n","def jaccard_similarity(\n","    R: Union[torch.Tensor, npt.NDArray],\n","    Rp: Union[torch.Tensor, npt.NDArray],\n","    k: int = 10,\n","    inner: str = \"cosine\",\n","    n_jobs: int = 8,\n",") -> float:\n","    R, Rp = to_numpy_if_needed(R, Rp)\n","\n","    indices_R = nn_array_to_setlist(top_k_neighbors(R, k, inner, n_jobs))\n","    indices_Rp = nn_array_to_setlist(top_k_neighbors(Rp, k, inner, n_jobs))\n","\n","    return float(\n","        np.mean(\n","            [_jac_sim_i(idx_R, idx_Rp) for idx_R, idx_Rp in zip(indices_R, indices_Rp)]\n","        )\n","    )\n","\n","\n","def top_k_neighbors(\n","    R: npt.NDArray,\n","    k: int,\n","    inner: str,\n","    n_jobs: int,\n",") -> npt.NDArray:\n","    # k+1 nearest neighbors, because we pass in all the data, which means that a point\n","    # will be the nearest neighbor to itself. We remove this point from the results and\n","    # report only the k nearest neighbors distinct from the point itself.\n","    nns = sklearn.neighbors.NearestNeighbors(\n","        n_neighbors=k + 1, metric=inner, n_jobs=n_jobs\n","    )\n","    nns.fit(R)\n","    _, nns = nns.kneighbors(R)\n","    return nns[:, 1:]\n","\n","\n","def nn_array_to_setlist(nn: npt.NDArray) -> List[Set[int]]:\n","    return [set(idx) for idx in nn]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K8QY53-0umRk"},"outputs":[],"source":["import functools\n","import logging\n","from abc import ABC\n","from abc import abstractmethod\n","from dataclasses import dataclass\n","from dataclasses import field\n","from typing import Any\n","from typing import Callable\n","from typing import get_args\n","from typing import List\n","from typing import Literal\n","from typing import Optional\n","from typing import Protocol\n","from typing import Tuple\n","from typing import Union\n","\n","import numpy as np\n","import numpy.typing as npt\n","import torch\n","from einops import rearrange\n","# from loguru import logger\n","\n","log = logging.getLogger(__name__)\n","\n","\n","SHAPE_TYPE = Literal[\"nd\", \"ntd\", \"nchw\"]\n","\n","ND_SHAPE, NTD_SHAPE, NCHW_SHAPE = get_args(SHAPE_TYPE)[0], get_args(SHAPE_TYPE)[1], get_args(SHAPE_TYPE)[2]\n","\n","\n","class SimilarityFunction(Protocol):\n","    def __call__(  # noqa: E704\n","        self,\n","        R: torch.Tensor | npt.NDArray,\n","        Rp: torch.Tensor | npt.NDArray,\n","        shape: SHAPE_TYPE,\n","    ) -> float: ...\n","\n","\n","class RSMSimilarityFunction(Protocol):\n","    def __call__(  # noqa: E704\n","        self, R: torch.Tensor | npt.NDArray, Rp: torch.Tensor | npt.NDArray, shape: SHAPE_TYPE, n_jobs: int\n","    ) -> float: ...\n","\n","\n","@dataclass\n","class BaseSimilarityMeasure(ABC):\n","    larger_is_more_similar: bool\n","    is_symmetric: bool\n","\n","    is_metric: bool | None = None\n","    invariant_to_affine: bool | None = None\n","    invariant_to_invertible_linear: bool | None = None\n","    invariant_to_ortho: bool | None = None\n","    invariant_to_permutation: bool | None = None\n","    invariant_to_isotropic_scaling: bool | None = None\n","    invariant_to_translation: bool | None = None\n","    name: str = field(init=False)\n","\n","    def __post_init__(self):\n","        self.name = self.__class__.__name__\n","\n","    @abstractmethod\n","    def __call__(self, *args: Any, **kwds: Any) -> Any:\n","        raise NotImplementedError\n","\n","\n","class FunctionalSimilarityMeasure(BaseSimilarityMeasure):\n","    @abstractmethod\n","    def __call__(self, output_a: torch.Tensor | npt.NDArray, output_b: torch.Tensor | npt.NDArray) -> float:\n","        raise NotImplementedError\n","\n","\n","@dataclass(kw_only=True)\n","class RepresentationalSimilarityMeasure(BaseSimilarityMeasure):\n","    sim_func: SimilarityFunction\n","\n","    def __call__(\n","        self,\n","        R: torch.Tensor | npt.NDArray,\n","        Rp: torch.Tensor | npt.NDArray,\n","        shape: SHAPE_TYPE,\n","    ) -> float:\n","        return self.sim_func(R, Rp, shape)\n","\n","\n","class RSMSimilarityMeasure(RepresentationalSimilarityMeasure):\n","    sim_func: RSMSimilarityFunction\n","\n","    @staticmethod\n","    def estimate_good_number_of_jobs(R: torch.Tensor | npt.NDArray, Rp: torch.Tensor | npt.NDArray) -> int:\n","        # RSMs in are NxN (or DxD) so the number of jobs should roughly scale quadratically with increase in N (or D).\n","        # False! As long as sklearn-native metrics are used, they will use parallel implementations regardless of job\n","        # count. Each job would spawn their own threads, which leads to oversubscription of cores and thus slowdown.\n","        # This seems to be not fully correct (n_jobs=2 seems to actually use two cores), but using n_jobs=1 seems the\n","        # fastest.\n","        return 1\n","\n","    def __call__(\n","        self,\n","        R: torch.Tensor | npt.NDArray,\n","        Rp: torch.Tensor | npt.NDArray,\n","        shape: SHAPE_TYPE,\n","        n_jobs: Optional[int] = None,\n","    ) -> float:\n","        if n_jobs is None:\n","            n_jobs = self.estimate_good_number_of_jobs(R, Rp)\n","        return self.sim_func(R, Rp, shape, n_jobs=n_jobs)\n","\n","\n","def to_numpy_if_needed(*args: Union[torch.Tensor, npt.NDArray]) -> List[npt.NDArray]:\n","    def convert(x: Union[torch.Tensor, npt.NDArray]) -> npt.NDArray:\n","        return x if isinstance(x, np.ndarray) else x.numpy()\n","\n","    return list(map(convert, args))\n","\n","\n","def to_torch_if_needed(*args: Union[torch.Tensor, npt.NDArray]) -> List[torch.Tensor]:\n","    def convert(x: Union[torch.Tensor, npt.NDArray]) -> torch.Tensor:\n","        return x if isinstance(x, torch.Tensor) else torch.from_numpy(x)\n","\n","    return list(map(convert, args))\n","\n","\n","def adjust_dimensionality(R: npt.NDArray, Rp: npt.NDArray, strategy=\"zero_pad\") -> Tuple[npt.NDArray, npt.NDArray]:\n","    D = R.shape[1]\n","    Dp = Rp.shape[1]\n","    if strategy == \"zero_pad\":\n","        if D - Dp == 0:\n","            return R, Rp\n","        elif D - Dp > 0:\n","            return R, np.concatenate((Rp, np.zeros((Rp.shape[0], D - Dp))), axis=1)\n","        else:\n","            return np.concatenate((R, np.zeros((R.shape[0], Dp - D))), axis=1), Rp\n","    else:\n","        raise NotImplementedError()\n","\n","\n","def center_columns(R: npt.NDArray) -> npt.NDArray:\n","    return R - R.mean(axis=0)[None, :]\n","\n","\n","def normalize_matrix_norm(R: npt.NDArray) -> npt.NDArray:\n","    return R / np.linalg.norm(R, ord=\"fro\")\n","\n","\n","def normalize_row_norm(R: npt.NDArray) -> npt.NDArray:\n","    return R / np.linalg.norm(R, ord=2, axis=1, keepdims=True)\n","\n","\n","def standardize(R: npt.NDArray) -> npt.NDArray:\n","    return (R - R.mean(axis=0, keepdims=True)) / R.std(axis=0)\n","\n","\n","def double_center(x: npt.NDArray) -> npt.NDArray:\n","    return x - x.mean(axis=0, keepdims=True) - x.mean(axis=1, keepdims=True) + x.mean()\n","\n","\n","def align_spatial_dimensions(R: npt.NDArray, Rp: npt.NDArray) -> Tuple[npt.NDArray, npt.NDArray]:\n","    \"\"\"\n","    Aligns spatial representations by resizing them to the smallest spatial dimension.\n","    Subsequent aligned spatial representations are flattened, with the spatial aligned representations\n","    moving into the *sample* dimension.\n","    \"\"\"\n","    R_re, Rp_re = resize_wh_reps(R, Rp)\n","    R_re = rearrange(R_re, \"n c h w -> (n h w) c\")\n","    Rp_re = rearrange(Rp_re, \"n c h w -> (n h w) c\")\n","    if R_re.shape[0] > 5000:\n","        logger.info(f\"Got {R_re.shape[0]} samples in N after flattening. Subsampling to reduce compute.\")\n","        subsample = R_re.shape[0] // 5000\n","        R_re = R_re[::subsample]\n","        Rp_re = Rp_re[::subsample]\n","\n","    return R_re, Rp_re\n","\n","\n","def average_pool_downsample(R, resize: bool, new_size: tuple[int, int]):\n","    if not resize:\n","        return R  # do nothing\n","    else:\n","        is_numpy = isinstance(R, np.ndarray)\n","        R_torch = torch.from_numpy(R) if is_numpy else R\n","        R_torch = torch.nn.functional.adaptive_avg_pool2d(R_torch, new_size)\n","        return R_torch.numpy() if is_numpy else R_torch\n","\n","\n","def resize_wh_reps(R: npt.NDArray, Rp: npt.NDArray) -> Tuple[npt.NDArray, npt.NDArray]:\n","    \"\"\"\n","    Function for resizing spatial representations that are not the same size.\n","    Does through fourier transform and resizing.\n","\n","    Args:\n","        R: numpy array of shape  [batch_size, height, width, num_channels]\n","        RP: numpy array of shape [batch_size, height, width, num_channels]\n","\n","    Returns:\n","        fft_acts1: numpy array of shape [batch_size, (new) height, (new) width, num_channels]\n","        fft_acts2: numpy array of shape [batch_size, (new) height, (new) width, num_channels]\n","\n","    \"\"\"\n","    height1, width1 = R.shape[2], R.shape[3]\n","    height2, width2 = Rp.shape[2], Rp.shape[3]\n","    if height1 != height2 or width1 != width2:\n","        height = min(height1, height2)\n","        width = min(width1, width2)\n","        new_size = [height, width]\n","        resize = True\n","    else:\n","        height = height1\n","        width = width1\n","        new_size = None\n","        resize = False\n","\n","    # resize and preprocess with fft\n","    avg_ds1 = average_pool_downsample(R, resize=resize, new_size=new_size)\n","    avg_ds2 = average_pool_downsample(Rp, resize=resize, new_size=new_size)\n","    return avg_ds1, avg_ds2\n","\n","\n","def fft_resize(images, resize=False, new_size=None):\n","    \"\"\"Function for applying DFT and resizing.\n","\n","    This function takes in an array of images, applies the 2-d fourier transform\n","    and resizes them according to new_size, keeping the frequencies that overlap\n","    between the two sizes.\n","\n","    Args:\n","              images: a numpy array with shape\n","                      [batch_size, height, width, num_channels]\n","              resize: boolean, whether or not to resize\n","              new_size: a tuple (size, size), with height and width the same\n","\n","    Returns:\n","              im_fft_downsampled: a numpy array with shape\n","                           [batch_size, (new) height, (new) width, num_channels]\n","    \"\"\"\n","    assert len(images.shape) == 4, \"expecting images to be\" \"[batch_size, height, width, num_channels]\"\n","    if resize:\n","        # FFT --> remove high frequencies --> inverse FFT\n","        im_complex = images.astype(\"complex64\")\n","        im_fft = np.fft.fft2(im_complex, axes=(1, 2))\n","        im_shifted = np.fft.fftshift(im_fft, axes=(1, 2))\n","\n","        center_width = im_shifted.shape[2] // 2\n","        center_height = im_shifted.shape[1] // 2\n","        half_w = new_size[0] // 2\n","        half_h = new_size[1] // 2\n","        cropped_fft = im_shifted[\n","            :, center_height - half_h : center_height + half_h, center_width - half_w : center_width + half_w, :\n","        ]\n","        cropped_fft_shifted_back = np.fft.ifft2(cropped_fft, axes=(1, 2))\n","        return cropped_fft_shifted_back.real\n","    else:\n","        return images\n","\n","\n","class Pipeline:\n","    def __init__(\n","        self,\n","        preprocess_funcs: List[Callable[[npt.NDArray], npt.NDArray]],\n","        similarity_func: Callable[[npt.NDArray, npt.NDArray, SHAPE_TYPE], float],\n","    ) -> None:\n","        self.preprocess_funcs = preprocess_funcs\n","        self.similarity_func = similarity_func\n","\n","    def __call__(self, R: npt.NDArray, Rp: npt.NDArray, shape: SHAPE_TYPE) -> float:\n","        try:\n","            for preprocess_func in self.preprocess_funcs:\n","                R = preprocess_func(R)\n","                Rp = preprocess_func(Rp)\n","            return self.similarity_func(R, Rp, shape)\n","        except ValueError as e:\n","            log.info(f\"Pipeline failed: {e}\")\n","            return np.nan\n","\n","    def __str__(self) -> str:\n","        def func_name(func: Callable) -> str:\n","            return func.__name__ if not isinstance(func, functools.partial) else func.func.__name__\n","\n","        def partial_keywords(func: Callable) -> str:\n","            if not isinstance(func, functools.partial):\n","                return \"\"\n","            else:\n","                return str(func.keywords)\n","\n","        return (\n","            \"Pipeline(\"\n","            + (\n","                \"+\".join(map(func_name, self.preprocess_funcs))\n","                + \"+\"\n","                + func_name(self.similarity_func)\n","                + partial_keywords(self.similarity_func)\n","            )\n","            + \")\"\n","        )\n","\n","\n","def flatten(*args: Union[torch.Tensor, npt.NDArray], shape: SHAPE_TYPE) -> List[Union[torch.Tensor, npt.NDArray]]:\n","    if shape == \"ntd\":\n","        return list(map(flatten_nxtxd_to_ntxd, args))\n","    elif shape == \"nd\":\n","        return list(args)\n","    elif shape == \"nchw\":\n","        return list(map(flatten_nxcxhxw_to_nxchw, args))  # Flattening non-trivial for nchw\n","    else:\n","        raise ValueError(\"Unknown shape of representations. Must be one of 'ntd', 'nchw', 'nd'.\")\n","\n","\n","def flatten_nxtxd_to_ntxd(R: Union[torch.Tensor, npt.NDArray]) -> torch.Tensor:\n","    R = to_torch_if_needed(R)[0]\n","    log.debug(\"Shape before flattening: %s\", str(R.shape))\n","    R = torch.flatten(R, start_dim=0, end_dim=1)\n","    log.debug(\"Shape after flattening: %s\", str(R.shape))\n","    return R\n","\n","\n","def flatten_nxcxhxw_to_nxchw(R: Union[torch.Tensor, npt.NDArray]) -> torch.Tensor:\n","    R = to_torch_if_needed(R)[0]\n","    log.debug(\"Shape before flattening: %s\", str(R.shape))\n","    R = torch.reshape(R, (R.shape[0], -1))\n","    log.debug(\"Shape after flattening: %s\", str(R.shape))\n","    return R"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MU_QCO_UvFKl"},"outputs":[],"source":["from typing import Optional\n","from typing import Union\n","\n","import numpy as np\n","import numpy.typing as npt\n","import scipy.spatial.distance\n","import scipy.stats\n","import sklearn.metrics\n","import torch\n","# from repsim.measures.utils import flatten\n","# from repsim.measures.utils import RSMSimilarityMeasure\n","# from repsim.measures.utils import SHAPE_TYPE\n","# from repsim.measures.utils import to_numpy_if_needed\n","\n","\n","def representational_similarity_analysis(\n","    R: Union[torch.Tensor, npt.NDArray],\n","    Rp: Union[torch.Tensor, npt.NDArray],\n","    shape: SHAPE_TYPE,\n","    inner=\"correlation\",\n","    outer=\"spearman\",\n","    n_jobs: Optional[int] = None,\n",") -> float:\n","    \"\"\"Representational similarity analysis\n","\n","    Args:\n","        R (Union[torch.Tensor, npt.NDArray]): N x D representation\n","        Rp (Union[torch.Tensor, npt.NDArray]): N x D' representation\n","        inner (str, optional): inner similarity function for RSM. Must be one of\n","            scipy.spatial.distance.pdist identifiers . Defaults to \"correlation\".\n","        outer (str, optional): outer similarity function that compares RSMs. Defaults to\n","             \"spearman\". Must be one of \"spearman\", \"euclidean\"\n","\n","    Returns:\n","        float: _description_\n","    \"\"\"\n","    R, Rp = flatten(R, Rp, shape=shape)\n","    R, Rp = to_numpy_if_needed(R, Rp)\n","\n","    if inner == \"correlation\":\n","        # n_jobs only works if metric is in PAIRWISE_DISTANCES as defined in sklearn, i.e., not for correlation.\n","        # But correlation = 1 - cosine dist of row-centered data, so we use the faster cosine metric and center the data.\n","        R = R - R.mean(axis=1, keepdims=True)\n","        S = scipy.spatial.distance.squareform(  # take the lower triangle of RSM\n","            1 - sklearn.metrics.pairwise_distances(R, metric=\"cosine\", n_jobs=n_jobs),  # type:ignore\n","            checks=False,\n","        )\n","        Rp = Rp - Rp.mean(axis=1, keepdims=True)\n","        Sp = scipy.spatial.distance.squareform(\n","            1 - sklearn.metrics.pairwise_distances(Rp, metric=\"cosine\", n_jobs=n_jobs),  # type:ignore\n","            checks=False,\n","        )\n","    elif inner == \"euclidean\":\n","        # take the lower triangle of RSM\n","        S = scipy.spatial.distance.squareform(\n","            sklearn.metrics.pairwise_distances(R, metric=inner, n_jobs=n_jobs), checks=False\n","        )\n","        Sp = scipy.spatial.distance.squareform(\n","            sklearn.metrics.pairwise_distances(Rp, metric=inner, n_jobs=n_jobs), checks=False\n","        )\n","    else:\n","        raise NotImplementedError(f\"{inner=}\")\n","\n","    if outer == \"spearman\":\n","        return scipy.stats.spearmanr(S, Sp).statistic  # type:ignore\n","    elif outer == \"euclidean\":\n","        return float(np.linalg.norm(S - Sp, ord=2))\n","    else:\n","        raise ValueError(f\"Unknown outer similarity function: {outer}\")\n","\n","\n","class RSA(RSMSimilarityMeasure):\n","    def __init__(self):\n","        # choice of inner/outer in __call__ if fixed to default values, so these values are always the same\n","        super().__init__(\n","            sim_func=representational_similarity_analysis,\n","            larger_is_more_similar=True,\n","            is_metric=False,\n","            is_symmetric=True,\n","            invariant_to_affine=False,\n","            invariant_to_invertible_linear=False,\n","            invariant_to_ortho=False,\n","            invariant_to_permutation=True,\n","            invariant_to_isotropic_scaling=True,\n","            invariant_to_translation=True,\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LO8o8I5owA7p"},"outputs":[],"source":["##################################################################################\n","# Copied from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/cca_core.py\n","# Copyright 2018 Google Inc.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"\n","The core code for applying Canonical Correlation Analysis to deep networks.\n","\n","This module contains the core functions to apply canonical correlation analysis\n","to deep neural networks. The main function is get_cca_similarity, which takes in\n","two sets of activations, typically the neurons in two layers and their outputs\n","on all of the datapoints D = [d_1,...,d_m] that have been passed through.\n","\n","Inputs have shape (num_neurons1, m), (num_neurons2, m). This can be directly\n","applied used on fully connected networks. For convolutional layers, the 3d block\n","of neurons can either be flattened entirely, along channels, or alternatively,\n","the dft_ccas (Discrete Fourier Transform) module can be used.\n","\n","See:\n","https://arxiv.org/abs/1706.05806\n","https://arxiv.org/abs/1806.05759\n","for full details.\n","\n","\"\"\"\n","import numpy as np\n","# from repsim.measures.utils import align_spatial_dimensions\n","\n","num_cca_trials = 5\n","\n","\n","def positivedef_matrix_sqrt(array):\n","    \"\"\"Stable method for computing matrix square roots, supports complex matrices.\n","\n","    Args:\n","              array: A numpy 2d array, can be complex valued that is a positive\n","                     definite symmetric (or hermitian) matrix\n","\n","    Returns:\n","              sqrtarray: The matrix square root of array\n","    \"\"\"\n","    w, v = np.linalg.eigh(array)\n","    #  A - np.dot(v, np.dot(np.diag(w), v.T))\n","    wsqrt = np.sqrt(w)\n","    sqrtarray = np.dot(v, np.dot(np.diag(wsqrt), np.conj(v).T))\n","    return sqrtarray\n","\n","\n","def remove_small(sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon):\n","    \"\"\"Takes covariance between X, Y, and removes values of small magnitude.\n","\n","    Args:\n","              sigma_xx: 2d numpy array, variance matrix for x\n","              sigma_xy: 2d numpy array, crossvariance matrix for x,y\n","              sigma_yx: 2d numpy array, crossvariance matrixy for x,y,\n","                        (conjugate) transpose of sigma_xy\n","              sigma_yy: 2d numpy array, variance matrix for y\n","              epsilon : cutoff value for norm below which directions are thrown\n","                         away\n","\n","    Returns:\n","              sigma_xx_crop: 2d array with low x norm directions removed\n","              sigma_xy_crop: 2d array with low x and y norm directions removed\n","              sigma_yx_crop: 2d array with low x and y norm directiosn removed\n","              sigma_yy_crop: 2d array with low y norm directions removed\n","              x_idxs: indexes of sigma_xx that were removed\n","              y_idxs: indexes of sigma_yy that were removed\n","    \"\"\"\n","\n","    x_diag = np.abs(np.diagonal(sigma_xx))\n","    y_diag = np.abs(np.diagonal(sigma_yy))\n","    x_idxs = x_diag >= epsilon\n","    y_idxs = y_diag >= epsilon\n","\n","    sigma_xx_crop = sigma_xx[x_idxs][:, x_idxs]\n","    sigma_xy_crop = sigma_xy[x_idxs][:, y_idxs]\n","    sigma_yx_crop = sigma_yx[y_idxs][:, x_idxs]\n","    sigma_yy_crop = sigma_yy[y_idxs][:, y_idxs]\n","\n","    return (sigma_xx_crop, sigma_xy_crop, sigma_yx_crop, sigma_yy_crop, x_idxs, y_idxs)\n","\n","\n","def compute_ccas(sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon, verbose=True):\n","    \"\"\"Main cca computation function, takes in variances and crossvariances.\n","\n","    This function takes in the covariances and cross covariances of X, Y,\n","    preprocesses them (removing small magnitudes) and outputs the raw results of\n","    the cca computation, including cca directions in a rotated space, and the\n","    cca correlation coefficient values.\n","\n","    Args:\n","              sigma_xx: 2d numpy array, (num_neurons_x, num_neurons_x)\n","                        variance matrix for x\n","              sigma_xy: 2d numpy array, (num_neurons_x, num_neurons_y)\n","                        crossvariance matrix for x,y\n","              sigma_yx: 2d numpy array, (num_neurons_y, num_neurons_x)\n","                        crossvariance matrix for x,y (conj) transpose of sigma_xy\n","              sigma_yy: 2d numpy array, (num_neurons_y, num_neurons_y)\n","                        variance matrix for y\n","              epsilon:  small float to help with stabilizing computations\n","              verbose:  boolean on whether to print intermediate outputs\n","\n","    Returns:\n","              [ux, sx, vx]: [numpy 2d array, numpy 1d array, numpy 2d array]\n","                            ux and vx are (conj) transposes of each other, being\n","                            the canonical directions in the X subspace.\n","                            sx is the set of canonical correlation coefficients-\n","                            how well corresponding directions in vx, Vy correlate\n","                            with each other.\n","              [uy, sy, vy]: Same as above, but for Y space\n","              invsqrt_xx:   Inverse square root of sigma_xx to transform canonical\n","                            directions back to original space\n","              invsqrt_yy:   Same as above but for sigma_yy\n","              x_idxs:       The indexes of the input sigma_xx that were pruned\n","                            by remove_small\n","              y_idxs:       Same as above but for sigma_yy\n","    \"\"\"\n","\n","    (sigma_xx, sigma_xy, sigma_yx, sigma_yy, x_idxs, y_idxs) = remove_small(\n","        sigma_xx, sigma_xy, sigma_yx, sigma_yy, epsilon\n","    )\n","\n","    numx = sigma_xx.shape[0]\n","    numy = sigma_yy.shape[0]\n","\n","    if numx == 0 or numy == 0:\n","        return (\n","            [0, 0, 0],\n","            [0, 0, 0],\n","            np.zeros_like(sigma_xx),\n","            np.zeros_like(sigma_yy),\n","            x_idxs,\n","            y_idxs,\n","        )\n","\n","    if verbose:\n","        print(\"adding eps to diagonal and taking inverse\")\n","    sigma_xx += epsilon * np.eye(numx)\n","    sigma_yy += epsilon * np.eye(numy)\n","    inv_xx = np.linalg.pinv(sigma_xx)\n","    inv_yy = np.linalg.pinv(sigma_yy)\n","\n","    if verbose:\n","        print(\"taking square root\")\n","    invsqrt_xx = positivedef_matrix_sqrt(inv_xx)\n","    invsqrt_yy = positivedef_matrix_sqrt(inv_yy)\n","\n","    if verbose:\n","        print(\"dot products...\")\n","    arr = np.dot(invsqrt_xx, np.dot(sigma_xy, invsqrt_yy))\n","\n","    if verbose:\n","        print(\"trying to take final svd\")\n","    u, s, v = np.linalg.svd(arr)\n","\n","    if verbose:\n","        print(\"computed everything!\")\n","\n","    return [u, np.abs(s), v], invsqrt_xx, invsqrt_yy, x_idxs, y_idxs\n","\n","\n","def sum_threshold(array, threshold):\n","    \"\"\"Computes threshold index of decreasing nonnegative array by summing.\n","\n","    This function takes in a decreasing array nonnegative floats, and a\n","    threshold between 0 and 1. It returns the index i at which the sum of the\n","    array up to i is threshold*total mass of the array.\n","\n","    Args:\n","              array: a 1d numpy array of decreasing, nonnegative floats\n","              threshold: a number between 0 and 1\n","\n","    Returns:\n","              i: index at which np.sum(array[:i]) >= threshold\n","    \"\"\"\n","    assert (threshold >= 0) and (threshold <= 1), \"print incorrect threshold\"\n","\n","    for i in range(len(array)):\n","        if np.sum(array[:i]) / np.sum(array) >= threshold:\n","            return i\n","\n","\n","def create_zero_dict(compute_dirns, dimension):\n","    \"\"\"Outputs a zero dict when neuron activation norms too small.\n","\n","    This function creates a return_dict with appropriately shaped zero entries\n","    when all neuron activations are very small.\n","\n","    Args:\n","              compute_dirns: boolean, whether to have zero vectors for directions\n","              dimension: int, defines shape of directions\n","\n","    Returns:\n","              return_dict: a dict of appropriately shaped zero entries\n","    \"\"\"\n","    return_dict = {}\n","    return_dict[\"mean\"] = (np.asarray(0), np.asarray(0))\n","    return_dict[\"sum\"] = (np.asarray(0), np.asarray(0))\n","    return_dict[\"cca_coef1\"] = np.asarray(0)\n","    return_dict[\"cca_coef2\"] = np.asarray(0)\n","    return_dict[\"idx1\"] = 0\n","    return_dict[\"idx2\"] = 0\n","\n","    if compute_dirns:\n","        return_dict[\"cca_dirns1\"] = np.zeros((1, dimension))\n","        return_dict[\"cca_dirns2\"] = np.zeros((1, dimension))\n","\n","    return return_dict\n","\n","\n","def get_cca_similarity(\n","    acts1,\n","    acts2,\n","    epsilon=0.0,\n","    threshold=0.98,\n","    compute_coefs=True,\n","    compute_dirns=False,\n","    verbose=True,\n","):\n","    \"\"\"The main function for computing cca similarities.\n","\n","    This function computes the cca similarity between two sets of activations,\n","    returning a dict with the cca coefficients, a few statistics of the cca\n","    coefficients, and (optionally) the actual directions.\n","\n","    Args:\n","              acts1: (num_neurons1, data_points) a 2d numpy array of neurons by\n","                     datapoints where entry (i,j) is the output of neuron i on\n","                     datapoint j.\n","              acts2: (num_neurons2, data_points) same as above, but (potentially)\n","                     for a different set of neurons. Note that acts1 and acts2\n","                     can have different numbers of neurons, but must agree on the\n","                     number of datapoints\n","\n","              epsilon: small float to help stabilize computations\n","\n","              threshold: float between 0, 1 used to get rid of trailing zeros in\n","                         the cca correlation coefficients to output more accurate\n","                         summary statistics of correlations.\n","\n","\n","              compute_coefs: boolean value determining whether coefficients\n","                             over neurons are computed. Needed for computing\n","                             directions\n","\n","              compute_dirns: boolean value determining whether actual cca\n","                             directions are computed. (For very large neurons and\n","                             datasets, may be better to compute these on the fly\n","                             instead of store in memory.)\n","\n","              verbose: Boolean, whether intermediate outputs are printed\n","\n","    Returns:\n","              return_dict: A dictionary with outputs from the cca computations.\n","                           Contains neuron coefficients (combinations of neurons\n","                           that correspond to cca directions), the cca correlation\n","                           coefficients (how well aligned directions correlate),\n","                           x and y idxs (for computing cca directions on the fly\n","                           if compute_dirns=False), and summary statistics. If\n","                           compute_dirns=True, the cca directions are also\n","                           computed.\n","    \"\"\"\n","\n","    # assert dimensionality equal\n","    assert acts1.shape[1] == acts2.shape[1], \"dimensions don't match\"\n","    # check that acts1, acts2 are transposition\n","    assert acts1.shape[0] < acts1.shape[1], \"input must be number of neurons\" \"by datapoints\"\n","    return_dict = {}\n","\n","    # compute covariance with numpy function for extra stability\n","    numx = acts1.shape[0]\n","    numy = acts2.shape[0]\n","\n","    covariance = np.cov(acts1, acts2)\n","    sigmaxx = covariance[:numx, :numx]\n","    sigmaxy = covariance[:numx, numx:]\n","    sigmayx = covariance[numx:, :numx]\n","    sigmayy = covariance[numx:, numx:]\n","\n","    # rescale covariance to make cca computation more stable\n","    xmax = np.max(np.abs(sigmaxx))\n","    ymax = np.max(np.abs(sigmayy))\n","    sigmaxx /= xmax\n","    sigmayy /= ymax\n","    sigmaxy /= np.sqrt(xmax * ymax)\n","    sigmayx /= np.sqrt(xmax * ymax)\n","\n","    ([u, s, v], invsqrt_xx, invsqrt_yy, x_idxs, y_idxs) = compute_ccas(\n","        sigmaxx, sigmaxy, sigmayx, sigmayy, epsilon=epsilon, verbose=verbose\n","    )\n","\n","    # if x_idxs or y_idxs is all false, return_dict has zero entries\n","    if (not np.any(x_idxs)) or (not np.any(y_idxs)):\n","        return create_zero_dict(compute_dirns, acts1.shape[1])\n","\n","    if compute_coefs:\n","        # also compute full coefficients over all neurons\n","        x_mask = np.dot(x_idxs.reshape((-1, 1)), x_idxs.reshape((1, -1)))\n","        y_mask = np.dot(y_idxs.reshape((-1, 1)), y_idxs.reshape((1, -1)))\n","\n","        return_dict[\"coef_x\"] = u.T\n","        return_dict[\"invsqrt_xx\"] = invsqrt_xx\n","        return_dict[\"full_coef_x\"] = np.zeros((numx, numx))\n","        np.place(return_dict[\"full_coef_x\"], x_mask, return_dict[\"coef_x\"])\n","        return_dict[\"full_invsqrt_xx\"] = np.zeros((numx, numx))\n","        np.place(return_dict[\"full_invsqrt_xx\"], x_mask, return_dict[\"invsqrt_xx\"])\n","\n","        return_dict[\"coef_y\"] = v\n","        return_dict[\"invsqrt_yy\"] = invsqrt_yy\n","        return_dict[\"full_coef_y\"] = np.zeros((numy, numy))\n","        np.place(return_dict[\"full_coef_y\"], y_mask, return_dict[\"coef_y\"])\n","        return_dict[\"full_invsqrt_yy\"] = np.zeros((numy, numy))\n","        np.place(return_dict[\"full_invsqrt_yy\"], y_mask, return_dict[\"invsqrt_yy\"])\n","\n","        # compute means\n","        neuron_means1 = np.mean(acts1, axis=1, keepdims=True)\n","        neuron_means2 = np.mean(acts2, axis=1, keepdims=True)\n","        return_dict[\"neuron_means1\"] = neuron_means1\n","        return_dict[\"neuron_means2\"] = neuron_means2\n","\n","    if compute_dirns:\n","        # orthonormal directions that are CCA directions\n","        cca_dirns1 = (\n","            np.dot(\n","                np.dot(return_dict[\"full_coef_x\"], return_dict[\"full_invsqrt_xx\"]),\n","                (acts1 - neuron_means1),\n","            )\n","            + neuron_means1\n","        )\n","        cca_dirns2 = (\n","            np.dot(\n","                np.dot(return_dict[\"full_coef_y\"], return_dict[\"full_invsqrt_yy\"]),\n","                (acts2 - neuron_means2),\n","            )\n","            + neuron_means2\n","        )\n","\n","    # get rid of trailing zeros in the cca coefficients\n","    idx1 = sum_threshold(s, threshold)\n","    idx2 = sum_threshold(s, threshold)\n","\n","    return_dict[\"cca_coef1\"] = s\n","    return_dict[\"cca_coef2\"] = s\n","    return_dict[\"x_idxs\"] = x_idxs\n","    return_dict[\"y_idxs\"] = y_idxs\n","    # summary statistics\n","    return_dict[\"mean\"] = (np.mean(s[:idx1]), np.mean(s[:idx2]))\n","    return_dict[\"sum\"] = (np.sum(s), np.sum(s))\n","\n","    if compute_dirns:\n","        return_dict[\"cca_dirns1\"] = cca_dirns1\n","        return_dict[\"cca_dirns2\"] = cca_dirns2\n","\n","    return return_dict\n","\n","\n","def robust_cca_similarity(acts1, acts2, threshold=0.98, epsilon=1e-6, compute_dirns=True):\n","    \"\"\"Calls get_cca_similarity multiple times while adding noise.\n","\n","    This function is very similar to get_cca_similarity, and can be used if\n","    get_cca_similarity doesn't converge for some pair of inputs. This function\n","    adds some noise to the activations to help convergence.\n","\n","    Args:\n","              acts1: (num_neurons1, data_points) a 2d numpy array of neurons by\n","                     datapoints where entry (i,j) is the output of neuron i on\n","                     datapoint j.\n","              acts2: (num_neurons2, data_points) same as above, but (potentially)\n","                     for a different set of neurons. Note that acts1 and acts2\n","                     can have different numbers of neurons, but must agree on the\n","                     number of datapoints\n","\n","              threshold: float between 0, 1 used to get rid of trailing zeros in\n","                         the cca correlation coefficients to output more accurate\n","                         summary statistics of correlations.\n","\n","              epsilon: small float to help stabilize computations\n","\n","              compute_dirns: boolean value determining whether actual cca\n","                             directions are computed. (For very large neurons and\n","                             datasets, may be better to compute these on the fly\n","                             instead of store in memory.)\n","\n","    Returns:\n","              return_dict: A dictionary with outputs from the cca computations.\n","                           Contains neuron coefficients (combinations of neurons\n","                           that correspond to cca directions), the cca correlation\n","                           coefficients (how well aligned directions correlate),\n","                           x and y idxs (for computing cca directions on the fly\n","                           if compute_dirns=False), and summary statistics. If\n","                           compute_dirns=True, the cca directions are also\n","                           computed.\n","    \"\"\"\n","\n","    for trial in range(num_cca_trials):\n","        try:\n","            return_dict = get_cca_similarity(acts1, acts2, threshold, compute_dirns)\n","        except np.linalg.LinAlgError:\n","            acts1 = acts1 * 1e-1 + np.random.normal(size=acts1.shape) * epsilon\n","            acts2 = acts2 * 1e-1 + np.random.normal(size=acts1.shape) * epsilon\n","            if trial + 1 == num_cca_trials:\n","                raise\n","\n","    return return_dict\n","    # End of copy from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/cca_core.py\n","\n","\n","def top_k_pca_comps(singular_values, threshold=0.99):\n","    total_variance = np.sum(singular_values**2)\n","    explained_variance = (singular_values**2) / total_variance\n","    cumulative_variance = np.cumsum(explained_variance)\n","    return np.argmax(cumulative_variance >= threshold * total_variance) + 1\n","\n","\n","def _svcca_original(acts1, acts2):\n","    # Copy from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/tutorials/001_Introduction.ipynb\n","    # Modification: get_cca_similarity is in the same file.\n","    # Modification: top-k PCA component selection s.t. explained variance > 0.99 total variance\n","    # Mean subtract activations\n","    cacts1 = acts1 - np.mean(acts1, axis=1, keepdims=True)\n","    cacts2 = acts2 - np.mean(acts2, axis=1, keepdims=True)\n","\n","    # Perform SVD\n","    U1, s1, V1 = np.linalg.svd(cacts1, full_matrices=False)\n","    U2, s2, V2 = np.linalg.svd(cacts2, full_matrices=False)\n","\n","    # top-k PCA components only\n","    k1 = top_k_pca_comps(s1)\n","    k2 = top_k_pca_comps(s2)\n","\n","    svacts1 = np.dot(s1[:k1] * np.eye(k1), V1[:k1])\n","    # can also compute as svacts1 = np.dot(U1.T[:20], cacts1)\n","    svacts2 = np.dot(s2[:k2] * np.eye(k2), V2[:k2])\n","    # can also compute as svacts1 = np.dot(U2.T[:20], cacts2)\n","\n","    svcca_results = get_cca_similarity(svacts1, svacts2, epsilon=1e-10, verbose=False)\n","    # End of copy from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/tutorials/001_Introduction.ipynb\n","    return np.mean(svcca_results[\"cca_coef1\"])\n","\n","\n","# Copied from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/pwcca.py\n","# Modification: get_cca_similarity is in the same file.\n","def compute_pwcca(acts1, acts2, epsilon=0.0):\n","    \"\"\"Computes projection weighting for weighting CCA coefficients\n","\n","    Args:\n","         acts1: 2d numpy array, shaped (neurons, num_datapoints)\n","         acts2: 2d numpy array, shaped (neurons, num_datapoints)\n","\n","    Returns:\n","         Original cca coefficient mean and weighted mean\n","\n","    \"\"\"\n","    sresults = get_cca_similarity(\n","        acts1,\n","        acts2,\n","        epsilon=epsilon,\n","        compute_dirns=False,\n","        compute_coefs=True,\n","        verbose=False,\n","    )\n","    if np.sum(sresults[\"x_idxs\"]) <= np.sum(sresults[\"y_idxs\"]):\n","        dirns = (\n","            np.dot(\n","                sresults[\"coef_x\"],\n","                (acts1[sresults[\"x_idxs\"]] - sresults[\"neuron_means1\"][sresults[\"x_idxs\"]]),\n","            )\n","            + sresults[\"neuron_means1\"][sresults[\"x_idxs\"]]\n","        )\n","        coefs = sresults[\"cca_coef1\"]\n","        acts = acts1\n","        idxs = sresults[\"x_idxs\"]\n","    else:\n","        dirns = (\n","            np.dot(\n","                sresults[\"coef_y\"],\n","                (acts1[sresults[\"y_idxs\"]] - sresults[\"neuron_means2\"][sresults[\"y_idxs\"]]),\n","            )\n","            + sresults[\"neuron_means2\"][sresults[\"y_idxs\"]]\n","        )\n","        coefs = sresults[\"cca_coef2\"]\n","        acts = acts2\n","        idxs = sresults[\"y_idxs\"]\n","    P, _ = np.linalg.qr(dirns.T)\n","    weights = np.sum(np.abs(np.dot(P.T, acts[idxs].T)), axis=1)\n","    weights = weights / np.sum(weights)\n","\n","    return np.sum(weights * coefs), weights, coefs\n","    # End of copy from https://github.com/google/svcca/blob/1f3fbf19bd31bd9b76e728ef75842aa1d9a4cd2b/pwcca.py\n","\n","\n","##################################################################################\n","\n","from typing import Union  # noqa:e402\n","\n","import numpy.typing as npt  # noqa:e402\n","import torch  # noqa:e402\n","\n","# from repsim.measures.utils import (\n","#     SHAPE_TYPE,\n","#     flatten,\n","#     resize_wh_reps,\n","#     to_numpy_if_needed,\n","#     RepresentationalSimilarityMeasure,\n","# )  # noqa:e402\n","\n","\n","def svcca(\n","    R: Union[torch.Tensor, npt.NDArray],\n","    Rp: Union[torch.Tensor, npt.NDArray],\n","    shape: SHAPE_TYPE,\n",") -> float:\n","    R, Rp = flatten(R, Rp, shape=shape)\n","    R, Rp = to_numpy_if_needed(R, Rp)\n","    return _svcca_original(R.T, Rp.T)\n","\n","\n","def pwcca(\n","    R: Union[torch.Tensor, npt.NDArray],\n","    Rp: Union[torch.Tensor, npt.NDArray],\n","    shape: SHAPE_TYPE,\n",") -> float:\n","    R, Rp = flatten(R, Rp, shape=shape)\n","    R, Rp = to_numpy_if_needed(R, Rp)\n","    return compute_pwcca(R.T, Rp.T)[0]\n","\n","\n","class SVCCA(RepresentationalSimilarityMeasure):\n","    def __init__(self):\n","        super().__init__(\n","            sim_func=svcca,\n","            larger_is_more_similar=True,\n","            is_metric=False,\n","            is_symmetric=True,\n","            invariant_to_affine=False,\n","            invariant_to_invertible_linear=False,\n","            invariant_to_ortho=True,\n","            invariant_to_permutation=True,\n","            invariant_to_isotropic_scaling=True,\n","            invariant_to_translation=True,\n","        )\n","\n","    def __call__(self, R: torch.Tensor | npt.NDArray, Rp: torch.Tensor | npt.NDArray, shape: SHAPE_TYPE) -> float:\n","        if shape == \"nchw\":\n","            # Move spatial dimensions into the sample dimension\n","            # If not the same spatial dimension, resample via FFT.\n","            R, Rp = align_spatial_dimensions(R, Rp)\n","            shape = \"nd\"\n","\n","        return self.sim_func(R, Rp, shape)\n","\n","\n","class PWCCA(RepresentationalSimilarityMeasure):\n","    def __init__(self):\n","        super().__init__(\n","            sim_func=pwcca,\n","            larger_is_more_similar=True,\n","            is_metric=False,\n","            is_symmetric=False,\n","            invariant_to_affine=False,\n","            invariant_to_invertible_linear=False,\n","            invariant_to_ortho=False,\n","            invariant_to_permutation=False,\n","            invariant_to_isotropic_scaling=True,\n","            invariant_to_translation=True,\n","        )\n","\n","    def __call__(self, R: torch.Tensor | npt.NDArray, Rp: torch.Tensor | npt.NDArray, shape: SHAPE_TYPE) -> float:\n","        if shape == \"nchw\":\n","            # Move spatial dimensions into the sample dimension\n","            # If not the same spatial dimension, resample via FFT.\n","            R, Rp = align_spatial_dimensions(R, Rp)\n","            shape = \"nd\"\n","\n","        return self.sim_func(R, Rp, shape)"]},{"cell_type":"markdown","metadata":{"id":"aQ_3rxDtd3Mc"},"source":["## get rand"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BqIiTtkid4qA"},"outputs":[],"source":["def score_rand(num_feats, sim_fn, shapereq_bool=False):\n","    all_rand_scores = []\n","    # num_feats = len(uniq_corr_indices_AB_forA)\n","    for i in range(10):\n","        rand_modA_feats = np.random.randint(low=0, high=weight_matrix.shape[0], size=num_feats).tolist()\n","        rand_modB_feats = np.random.randint(low=0, high=weight_matrix.shape[0], size=num_feats).tolist()\n","\n","        if shapereq_bool:\n","            score = sim_fn(weight_matrix[rand_modA_feats], weight_matrix[rand_modB_feats], \"nd\")\n","        else:\n","            score = sim_fn(weight_matrix[rand_modA_feats], weight_matrix[rand_modB_feats])\n","        all_rand_scores.append(score)\n","    print(sum(all_rand_scores) / len(all_rand_scores))\n","    # plt.hist(all_rand_scores)\n","    # plt.show()\n","    return sum(all_rand_scores) / len(all_rand_scores)"]},{"cell_type":"markdown","source":["# load labels"],"metadata":{"id":"nZz2t_CbNKmK"}},{"cell_type":"code","source":["import json\n","with open('gemma-2-2b-20-gemmascope-res-16k-explanations.json', 'rb') as f:\n","    feat_labels_allData = json.load(f)"],"metadata":{"id":"6CvQQSVdQvq5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feat_labels_lst = [0 for i in range(feat_labels_allData['explanationsCount'])]\n","feat_labels_dict = {}\n","for f_dict in feat_labels_allData['explanations']:\n","    feat_labels_lst[int(f_dict['index'])] = f_dict['description']\n","    feat_labels_dict[int(f_dict['index'])] = f_dict['description']\n","    if int(f_dict['index']) == 0:\n","        print(f_dict['description'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qriFJT15P6MB","executionInfo":{"status":"ok","timestamp":1724847184596,"user_tz":-60,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"036f003f-e6dc-4475-e211-92a0532da48e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mentions of the letter 'Z' and variations of 'Z' in different contexts\n"]}]},{"cell_type":"code","source":["len(feat_labels_dict)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNE4LIP6RDJe","executionInfo":{"status":"ok","timestamp":1724847184596,"user_tz":-60,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"98b741f1-1ee8-42a3-e41a-f89d75a2d28b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["16383"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["# search for features"],"metadata":{"id":"CLquai4hOaWR"}},{"cell_type":"code","source":["def find_indices_with_keyword(f_dict, keyword):\n","    \"\"\"\n","    Find all indices of fList which contain the keyword in the string at those indices.\n","\n","    Args:\n","    fList (list of str): List of strings to search within.\n","    keyword (str): Keyword to search for within the strings of fList.\n","\n","    Returns:\n","    list of int: List of indices where the keyword is found within the strings of fList.\n","    \"\"\"\n","    filt_dict = {}\n","    for index, string in f_dict.items():\n","        # split_list = string.split(',')\n","        # no_space_list = [i.replace(' ', '').lower() for i in split_list]\n","        # if keyword in no_space_list:\n","        if keyword in string:\n","            filt_dict[index] = string\n","    return filt_dict"],"metadata":{"id":"FEbmjLd7c-Ju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword = \"number\"\n","number_feats = find_indices_with_keyword(feat_labels_dict, keyword)"],"metadata":{"id":"wKS0eFoCObvI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["keyword = \"month\"\n","month_feats = find_indices_with_keyword(feat_labels_dict, keyword)"],"metadata":{"id":"HdjRMBTOSpg0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set(number_feats).intersection(month_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J34BaToUZslj","executionInfo":{"status":"ok","timestamp":1724771683023,"user_tz":-60,"elapsed":490,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8862a4ef-e31d-48df-fd70-295bb59e4fd1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{5769, 12790, 20260, 21029}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["number_feats[5769]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"lUUY88y-Z73d","executionInfo":{"status":"ok","timestamp":1724771717493,"user_tz":-60,"elapsed":511,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"41df1caf-bb17-4473-dfaf-2ca2a4b6d3ef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'numbers with possible time expressions like years, months, and hours'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["for common_feat in set(number_feats).intersection(month_feats):\n","    del number_feats[common_feat]"],"metadata":{"id":"w-pYdKBKaBUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["set(number_feats).intersection(month_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3s8UwnDSaIKJ","executionInfo":{"status":"ok","timestamp":1724771764753,"user_tz":-60,"elapsed":522,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"86c32697-75f5-4324-93d0-48d2ada5e82c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["set()"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","source":["# load model"],"metadata":{"id":"l8UydG5Pc4GR"}},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n","import numpy as np\n","import torch"],"metadata":{"id":"nOBcV4om7mrT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import hf_hub_download, notebook_login\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158,"referenced_widgets":["b39e7b0ad7cc4a74aeb9557788828b6c","6424b878bd174f478f588a29f6381f8f","098ead6b513143cda629bcc71085ff53","1c2589c4d8c1456ea59e5b87b0083967","cca1f49505c64871bb9cf5d83f351df2","bc09a7bcff89496b804f9652dd2eae60","12a2089a8cc842288b04f21e6d07a8e6","300bc5ae3ed0408cb2c8bdfe52c9291b","8852677546c74feaa210b81adec34444","8d8c21d9a65541e2885a99bcb59b6d9e","3c071699dee64d43a07deecceed20379","68b18d0721ea4d16ab893aad0c240a7c","9f5d9d1efcb14e9fa9f9eee759fc28ef","93a1df240de04bc089437981160b2e10","92f34e8e49e24ebeb98d5572b84bca14","298a17f8a9e0401092342ce60a7ac3a0","a52d20ee2c834c86804f61980cb4f0ac","657ac7bd409a4a02be205626922514b5","3b77ff9d59ab4a95b17331491acf82a1","73ea9e05f0674116a9b00abcb0ffa637","3661aebbb3ed42499e2d573df667d8e5","1e2184565afa4dcb9a01a0bbdfaeace2","84c4a32b1203414a9af4acc7f43bbb7a","6983d842bd754d9c82056c40ed3348ea","b1950423f3204c5c977279b8c3dcd70c","4525c2b50ace4eb3a664370352cf3095","4b9c37cac9514bcfb7f159af08bff9fd","1a0e37492b014abf8c551a7d6a8fc71d","d19b3f4818b549cfb1b3025e8ac172e0","6472f5b1188a42f6b1ff7c4ac32586d7","d060504927c34f0c812762cd20f7c32e","b721bc9f7c984c03ad0283a5114aa53f"]},"id":"UKRmnN7jbxbd","executionInfo":{"status":"ok","timestamp":1724846400918,"user_tz":-60,"elapsed":446,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4b3227fe-3299-430d-aebf-775a8f0eb79c"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b39e7b0ad7cc4a74aeb9557788828b6c"}},"metadata":{}}]},{"cell_type":"code","source":["torch.set_grad_enabled(False) # avoid blowing up mem"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-z3Bg4Ui26l2","executionInfo":{"status":"ok","timestamp":1724846419821,"user_tz":-60,"elapsed":244,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a35d41c9-f320-4a93-9485-5730ae9c0294"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7ea025f63070>"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(\n","    \"google/gemma-2-2b\",\n","    device_map='auto',\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["09a7931afe4b44f09ef3980ae7056e29","d523b0bc010b4c899763d89816a3cb5c","c7bde1ec4ebc4fd98d88fabcab554c65","342c5972ba9f47c5939b8ac474936580","f684342c6c034177a6e1645f301b9fac","8ce0eaf3b9394576b669819fa5175d4e","3ea58ec0c57d4127baa394b7cfab8fb4","d8c3f02062d74e378d6e669724abd974","7ad32527a0eb46a590b93f9c4004e515","b343821de3434e6d87ddd027131cd7d3","89abc024fe844c8883ef7d8b87b5d6b9"]},"id":"12wF3f7o1Ni7","executionInfo":{"status":"ok","timestamp":1724847133604,"user_tz":-60,"elapsed":4481,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"84cf5b64-9699-4887-ac94-5b85a0f09f07"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09a7931afe4b44f09ef3980ae7056e29"}},"metadata":{}}]},{"cell_type":"code","source":["tokenizer =  AutoTokenizer.from_pretrained(\"google/gemma-2-2b\")"],"metadata":{"id":"RV_HTT-v7ggl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# load sae"],"metadata":{"id":"N8ZMNbBKMuKA"}},{"cell_type":"code","source":["path_to_params = hf_hub_download(\n","    repo_id=\"google/gemma-scope-2b-pt-res\",\n","    filename=\"layer_20/width_16k/average_l0_71/params.npz\",\n","    force_download=False,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["dcfe563067884ac291d69de1364b0b9a","77d445127c1546829d415e538319b0de","2dbda754987646179586d669faaacaee","04105033ba4f4a22914f17c62b3864eb","38b6fed3397b40aa9ad434cb9ddca8e4","2179988cd919431195698e3bbd852377","fc4055b9799b44a4bd0891265d8e27fb","b5a5b78f34eb4453a596859bd20543e3","7ef3e7ab029f4e488407923180ef44bc","fdf9df74a3db4f88ad8cfe774cfaec48","56b059d0797e4bdeb729fab88f402208"]},"id":"havaBGy6b97D","executionInfo":{"status":"ok","timestamp":1724847110322,"user_tz":-60,"elapsed":1824,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5fb5424c-7437-4caf-a4e0-90cf38d1952b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["params.npz:   0%|          | 0.00/302M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcfe563067884ac291d69de1364b0b9a"}},"metadata":{}}]},{"cell_type":"code","source":["params = np.load(path_to_params)\n","pt_params = {k: torch.from_numpy(v).cuda() for k, v in params.items()}"],"metadata":{"id":"HB0GoeZccJD7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","class JumpReLUSAE(nn.Module):\n","  def __init__(self, d_model, d_sae):\n","    # Note that we initialise these to zeros because we're loading in pre-trained weights.\n","    # If you want to train your own SAEs then we recommend using blah\n","    super().__init__()\n","    self.W_enc = nn.Parameter(torch.zeros(d_model, d_sae))\n","    self.W_dec = nn.Parameter(torch.zeros(d_sae, d_model))\n","    self.threshold = nn.Parameter(torch.zeros(d_sae))\n","    self.b_enc = nn.Parameter(torch.zeros(d_sae))\n","    self.b_dec = nn.Parameter(torch.zeros(d_model))\n","\n","  def encode(self, input_acts):\n","    pre_acts = input_acts @ self.W_enc + self.b_enc\n","    mask = (pre_acts > self.threshold)\n","    acts = mask * torch.nn.functional.relu(pre_acts)\n","    return acts\n","\n","  def decode(self, acts):\n","    return acts @ self.W_dec + self.b_dec\n","\n","  def forward(self, acts):\n","    acts = self.encode(acts)\n","    recon = self.decode(acts)\n","    return recon\n"],"metadata":{"id":"WYfvS97fAFzq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sae = JumpReLUSAE(params['W_enc'].shape[0], params['W_enc'].shape[1])\n","sae.load_state_dict(pt_params)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X91UGkU1cSrC","executionInfo":{"status":"ok","timestamp":1724847117865,"user_tz":-60,"elapsed":764,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"99d8c3e1-fb57-42b2-ebf0-a5009eef9a24"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["sae.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sjgbfaPCjSWv","executionInfo":{"status":"ok","timestamp":1724847118079,"user_tz":-60,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ff454a3c-8cd4-425b-b9c6-ce76ed7ed33c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["JumpReLUSAE()"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sNSfL80Uv611"},"outputs":[],"source":["# layer_name = \"blocks.17.hook_resid_post\"\n","\n","# sae, cfg_dict, sparsity = SAE.from_pretrained(\n","#     release = \"google/gemma-scope-2b-pt-res\",\n","#     sae_id = layer_name,\n","#     device = device\n","# )"]},{"cell_type":"markdown","source":["# test prompts"],"metadata":{"id":"XA_Y_AfNSra3"}},{"cell_type":"code","source":["prompt = \"one two three four\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"id":"v6ZlLW2J5LA8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724845688568,"user_tz":-60,"elapsed":2613,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"06e21790-f62a-4e8d-8783-3c91e1a8341f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" five\n"]}]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CdE23DXMc04C","executionInfo":{"status":"ok","timestamp":1724789636052,"user_tz":-60,"elapsed":314,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"aab347c6-a846-41df-f18c-61fefd7af28b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" May\n"]}]},{"cell_type":"code","source":["prompt = \"January February March April May\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SUbbaqZ7Aqz1","executionInfo":{"status":"ok","timestamp":1724848981489,"user_tz":-60,"elapsed":417,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f4de4dcd-df8d-4000-b328-5f7593e31186"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" June\n"]}]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=5)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V7iS7ISHANIC","executionInfo":{"status":"ok","timestamp":1724848863025,"user_tz":-60,"elapsed":538,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"84f02ed7-8d24-4e9e-d3b2-a49fd0af4058"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" September\n"]}]},{"cell_type":"code","source":["prompt = \"uno dos tres cuatro\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724790595008,"user_tz":-60,"elapsed":371,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ee74c4cb-597c-4c9b-a069-ef9c3ec392d3","id":"c4aECQ2nh770"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" cinco\n"]}]},{"cell_type":"code","source":["prompt = \"My favorite animal is a\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724791055357,"user_tz":-60,"elapsed":107,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"37c36c1c-e47d-43a5-e14a-f0ffe8d32700","id":"6zqJa8cHjrpf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" dog\n"]}]},{"cell_type":"code","source":["prompt = \"My least favorite animal is a\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tF3lUpZ3ju3C","executionInfo":{"status":"ok","timestamp":1724791209913,"user_tz":-60,"elapsed":271,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"69dcc53c-715f-4661-8cd7-cce6fa65b883"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" snake\n"]}]},{"cell_type":"code","source":["prompt = \"enero febrero marzo abril\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9i30SjukWri","executionInfo":{"status":"ok","timestamp":1724791289421,"user_tz":-60,"elapsed":253,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b4fa0895-0c53-4f0b-965d-72093b55176d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" mayo\n"]}]},{"cell_type":"markdown","source":["# get actv fns"],"metadata":{"id":"pzHjkBaA3RfR"}},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","# outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","# print(tokenizer.decode(outputs[0, -1]))"],"metadata":{"id":"313MaH5JepiA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def gather_residual_activations(model, target_layer, inputs):\n","  target_act = None\n","  def gather_target_act_hook(mod, inputs, outputs):\n","    nonlocal target_act # make sure we can modify the target_act from the outer scope\n","    target_act = outputs[0]\n","    return outputs\n","  handle = model.model.layers[target_layer].register_forward_hook(gather_target_act_hook)\n","  _ = model.forward(inputs)\n","  handle.remove()\n","  return target_act"],"metadata":{"id":"aSvKs581WU7j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_act = gather_residual_activations(model, 20, inputs)"],"metadata":{"id":"pXoyLeWeXQF2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, we can run our SAE on the saved activations."],"metadata":{"id":"iS4Re5VTQti5"}},{"cell_type":"code","source":["sae_acts = sae.encode(target_act.to(torch.float32))\n","# recon = sae.decode(sae_acts)"],"metadata":{"id":"H2ax0KLtZfcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sae_acts.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vXYwKLQe42S","executionInfo":{"status":"ok","timestamp":1724847159126,"user_tz":-60,"elapsed":345,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3b4853a5-aca8-42e9-fd86-9ca00a11cc86"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16384])"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["# load sae weights\n"],"metadata":{"id":"bXTxV071e8IN"}},{"cell_type":"code","source":["weight_matrix = sae.W_dec.detach().cpu().numpy()\n","weight_matrix.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QI45LyeKe-K8","executionInfo":{"status":"ok","timestamp":1724847161460,"user_tz":-60,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4a00a504-f8b4-4b3c-a6e0-7d4d9efb977a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(16384, 2304)"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["# find common features"],"metadata":{"id":"5PbewO_DSURc"}},{"cell_type":"markdown","source":["## nums vs months"],"metadata":{"id":"SvGMBNH9h5FW"}},{"cell_type":"code","source":["prompt = \"one two three four\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_1 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"hqk8nzl6SURm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"-oV5IB0mSURm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sae_acts_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xpXut7t5foG6","executionInfo":{"status":"ok","timestamp":1724847190876,"user_tz":-60,"elapsed":14,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7a8d2460-add0-4da7-b3a3-1a61af7353d5"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 16384])"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":["feat_k = 15\n","one_top_acts_values, one_top_acts_indices = sae_acts_1[0, -1, :].topk(feat_k, dim=-1)\n","one_top_acts_indices.sort().values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724847190876,"user_tz":-60,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9888283a-b013-4029-b85b-25e48deca044","id":"5b3ACtlNSURp"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 2269,  3019,  3109,  5441,  6131,  6300,  6631,  8640,  9768, 10230,\n","        10802, 10841, 11864, 14441, 15451], device='cuda:0')"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["two_top_acts_values, two_top_acts_indices = sae_acts_2[0, -1, :].topk(feat_k, dim=-1)\n","two_top_acts_indices.sort().values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ffSERTGdUWv1","executionInfo":{"status":"ok","timestamp":1724847190876,"user_tz":-60,"elapsed":13,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bcd835af-0cd9-44a3-9129-b4934188f336"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  522,   609,  1530,  2269,  3109,  3162,  5710,  5968,  6240,  7159,\n","         9768, 11546, 12748, 13767, 15451], device='cuda:0')"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","source":["common_feats = set((one_top_acts_indices).tolist()).intersection(set((two_top_acts_indices).tolist()))\n","common_feats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724847191345,"user_tz":-60,"elapsed":482,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ed6be5da-bf15-4ce3-f14c-58a342a383cd","id":"GVIT4bPUSURq"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{2269, 3109, 9768, 15451}"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["for f_ind in common_feats:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":34,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fb61beea-8989-4891-94f6-50c34e5b22a1","id":"yZCyqPxHXPzG"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["9768 terms related to control and authority, particularly in political or systemic contexts\n","2269  structured lists or numbered steps in procedural texts\n","15451  structured data or attributes in a coded format\n","3109  technical terms and symbols related to data structures\n"]}]},{"cell_type":"markdown","source":["### top 100"],"metadata":{"id":"HTR8vZYzruqB"}},{"cell_type":"code","source":["feat_k = 50\n","one_top_acts_values, one_top_acts_indices = sae_acts_1[0, -1, :].topk(feat_k, dim=-1)\n","# one_top_acts_indices.sort().values"],"metadata":{"id":"VOvPHMPgruqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["two_top_acts_values, two_top_acts_indices = sae_acts_2[0, -1, :].topk(feat_k, dim=-1)\n","# two_top_acts_indices.sort().values"],"metadata":{"id":"OsYpz9o5ruqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for val, ind in zip(two_top_acts_values, two_top_acts_indices):\n","    print(round(val.item(), 2), ind.item(), feat_labels_lst[ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":32,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9a5abf5b-f412-4083-e093-e441ee8245b3","id":"-ANxallFruqH"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["65.1 15451  structured data or attributes in a coded format\n","41.55 5710 references to seasonal festivals and their associated cultural significance\n","38.81 13767 terms related to approval and assessment in a regulatory or medical context\n","34.97 11546 dates and months\n","33.7 6240  words related to time indicators, specifically months or dates\n","33.65 9768 terms related to control and authority, particularly in political or systemic contexts\n","32.96 3109  technical terms and symbols related to data structures\n","28.4 2269  structured lists or numbered steps in procedural texts\n","27.97 7159  references to time frames and chronological events\n","27.64 3162  markers indicating the beginning of a document\n","24.91 609 terms related to research articles and their attributes, specifically focusing on methodologies and data contributions\n","24.85 12748  structured data representations and their attributes\n","23.26 5968 references to specific dates, particularly in October and April\n","22.92 522  numerical data related to gene activity or expression\n","21.45 1530 technical terms related to inventions and scientific descriptions\n","21.06 1094 special characters and mathematical symbols\n","19.84 10636 specific dates or references to time\n","16.79 14017 conjunctions and relationships between elements, particularly in logical or mathematical contexts\n","15.79 11004 categories and statistics related to demographic groups\n","14.89 15194 specific terms related to legal and governmental contexts\n","14.44 12658 mathematical terminology and symbols, particularly in equations and expressions\n","14.34 8684  technical jargon and programming-related terms\n","14.21 5052 the beginning of a document or section, likely signaling the start of significant content\n","14.2 15763  sequences of numbers and their occurrences within the document\n","13.8 1692 legal and technical terminology related to statutes and inventions\n","13.55 10103 dates, specifically in the month of July\n","12.89 11210  expressions related to logical comparisons\n","12.7 1046  dates and events related to the calendar\n","12.58 11938 specific medical histories and conditions related to participant eligibility in clinical trials\n","11.66 5405  numerical data and statistics\n","11.4 6082  segments related to instructions or guidelines\n","11.33 12161 terms related to programming and technical constructs\n","10.87 12958 words and themes associated with seasonal transitions and celebrations\n","10.86 4295 connections and relationships among elements in technical or procedural contexts\n","10.71 10254 references to days of the week and their associated schedules\n","10.57 3032 numerical data or references to values and measurements\n","10.19 13817 dates and times\n","10.06 8640  numerical data and references to scientific publications\n","9.62 3019  elements related to operational or procedural contexts in a structured format\n","9.54 14249 links to privacy and cookie policies\n","9.41 14388 terms and phrases related to technology, particularly in the context of apps, security, and online privacy\n","9.36 13254  terms related to experimental control and intervention settings in research contexts\n","9.15 14782  references to data structures and types in programming\n","9.13 1233 scientific terminology related to statistics and experimental methods\n","9.06 8820 references to ethical concerns and issues related to accountability\n","8.83 10174  patterns or codes related to data structures or object representations\n","8.57 10997  references to specific health conditions or treatments\n","8.5 9203 the start of sections or paragraphs in the text\n","8.47 1039 technical terms and elements within programming and coding contexts\n","8.31 6612 instances of numbers and counting, indicating calculations or measurements\n"]}]},{"cell_type":"code","source":["common_feats = set((one_top_acts_indices).tolist()).intersection(set((two_top_acts_indices).tolist()))\n","# common_feats"],"metadata":{"id":"3XHGkye9ruqH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(common_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":30,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"197c7557-88b0-4928-f7c6-d8ded9633c31","id":"__cqBXibruqH"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["12"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["for f_ind in common_feats:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QBdmgXgSr-lh","executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":29,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"038df700-f616-49b6-d450-ee8b78b8e5f8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8640  numerical data and references to scientific publications\n","609 terms related to research articles and their attributes, specifically focusing on methodologies and data contributions\n","3109  technical terms and symbols related to data structures\n","1094 special characters and mathematical symbols\n","9768 terms related to control and authority, particularly in political or systemic contexts\n","522  numerical data related to gene activity or expression\n","3019  elements related to operational or procedural contexts in a structured format\n","12658 mathematical terminology and symbols, particularly in equations and expressions\n","8820 references to ethical concerns and issues related to accountability\n","2269  structured lists or numbered steps in procedural texts\n","15451  structured data or attributes in a coded format\n","5405  numerical data and statistics\n"]}]},{"cell_type":"code","source":["nums_only = set((one_top_acts_indices).tolist()) - set((two_top_acts_indices).tolist())\n","for f_ind in nums_only:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5h3lBTZ6slPz","executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":28,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"581709bd-e6d5-44d5-8a08-ebd34521616b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["6793 elements that resemble structured data or identifiers, likely in a list or JSON format\n","7180 references to various forms of art and cultural commentary\n","16150 details about physical movements or actions related to exercises and yoga poses\n","6300 information related to LGBTQ+ families and their unique needs in health care contexts\n","6305 mathematical expressions and notations used in equations and proofs\n","290 references to songs and musical works\n","11171 references to titles or distinctions within a lineage or notable families\n","13860 statements related to election integrity and voter registration\n","7717 expressions of gratitude and friendly communication\n","12329  statements about violation of rules or regulations\n","6954 commands and calls to action, especially those invoking collective effort or encouragement\n","8750  themes related to social awareness and community issues\n","1583 words related to administrative or technical tasks and issues\n","1198  instructions and steps in a process\n","10802 dialogue and quotes within the text\n","5811  sequences of numerical data and references to tables or figures\n","12853 specific identifiers, metrics, or references related to dimensional analysis or graphical data representation\n","13112 specific numerical values and quantities in the text\n","6779 greetings and conversational prompts\n","5440 special formatting or structural elements within the text\n","5441  content related to legal processes and outcomes\n","7881 references to iron and related biochemical processes\n","14030 references to unisex bathrooms and related signage\n","10584 proper nouns.\n","11864 technical terms and phrases related to legal and procedural contexts\n","10841 references to people and relationships in a narrative context\n","11993  expressions of frustration or disappointment\n","4835 elements related to childhood and play experiences\n","2916 references to technology and its implications\n","6631 the beginning of a text or important markers in a document\n","14441  punctuations and special characters\n","15469 mathematical expressions and symbols, indicating calculations or operations\n","5614 specific geometric shapes and their properties in mathematical contexts\n","6131  numerical values and their relationships to age and time\n","9844 positive sentiment in customer feedback about service and products\n","10230 numerical values and quantities in the text\n","11003  words indicating causation or reasoning\n","6143 phrases related to medical conditions and treatments\n"]}]},{"cell_type":"code","source":["months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in months_only:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847191346,"user_tz":-60,"elapsed":27,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"1994b660-dbd6-4cf1-bd6e-d71f3aec104d","id":"ByLRrW1UruqH"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12161 terms related to programming and technical constructs\n","10636 specific dates or references to time\n","10254 references to days of the week and their associated schedules\n","1039 technical terms and elements within programming and coding contexts\n","15763  sequences of numbers and their occurrences within the document\n","1046  dates and events related to the calendar\n","11546 dates and months\n","1692 legal and technical terminology related to statutes and inventions\n","12958 words and themes associated with seasonal transitions and celebrations\n","11938 specific medical histories and conditions related to participant eligibility in clinical trials\n","14249 links to privacy and cookie policies\n","7159  references to time frames and chronological events\n","14388 terms and phrases related to technology, particularly in the context of apps, security, and online privacy\n","5052 the beginning of a document or section, likely signaling the start of significant content\n","14782  references to data structures and types in programming\n","10174  patterns or codes related to data structures or object representations\n","14017 conjunctions and relationships between elements, particularly in logical or mathematical contexts\n","6082  segments related to instructions or guidelines\n","13254  terms related to experimental control and intervention settings in research contexts\n","13767 terms related to approval and assessment in a regulatory or medical context\n","4295 connections and relationships among elements in technical or procedural contexts\n","11210  expressions related to logical comparisons\n","12748  structured data representations and their attributes\n","5710 references to seasonal festivals and their associated cultural significance\n","5968 references to specific dates, particularly in October and April\n","1233 scientific terminology related to statistics and experimental methods\n","6612 instances of numbers and counting, indicating calculations or measurements\n","3032 numerical data or references to values and measurements\n","3162  markers indicating the beginning of a document\n","15194 specific terms related to legal and governmental contexts\n","6240  words related to time indicators, specifically months or dates\n","8684  technical jargon and programming-related terms\n","9203 the start of sections or paragraphs in the text\n","10997  references to specific health conditions or treatments\n","10103 dates, specifically in the month of July\n","13817 dates and times\n","1530 technical terms related to inventions and scientific descriptions\n","11004 categories and statistics related to demographic groups\n"]}]},{"cell_type":"markdown","source":["## english vs spanish numbers"],"metadata":{"id":"jJVvd_L6jLVq"}},{"cell_type":"code","source":["prompt = \"one two three four\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_1 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"2oL2DD4HjLVr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"uno dos tres cuatro\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"yjTPU2OKjLVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feat_k = 15\n","one_top_acts_values, one_top_acts_indices = sae_acts_1[0, -1, :].topk(feat_k, dim=-1)\n","one_top_acts_indices.sort().values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724792714391,"user_tz":-60,"elapsed":303,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4f1f121b-81f8-46b9-b783-0636419d7912","id":"4fdbkWiPjLVx"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([ 2269,  3019,  3109,  5441,  6131,  6300,  6631,  8640,  9768, 10230,\n","        10802, 10841, 11864, 14441, 15451], device='cuda:0')"]},"metadata":{},"execution_count":90}]},{"cell_type":"code","source":["for val, ind in zip(one_top_acts_values, one_top_acts_indices):\n","    print(round(val.item(), 2), ind.item(), feat_labels_lst[ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0_auc6f8kyDh","executionInfo":{"status":"ok","timestamp":1724791400709,"user_tz":-60,"elapsed":248,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5ff858e0-57e3-4ba5-8b86-6af838f097a7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64.49 15451  structured data or attributes in a coded format\n","45.17 9768 terms related to control and authority, particularly in political or systemic contexts\n","40.59 10230 numerical values and quantities in the text\n","35.36 10841 references to people and relationships in a narrative context\n","34.37 3109  technical terms and symbols related to data structures\n","30.33 8640  numerical data and references to scientific publications\n","30.23 2269  structured lists or numbered steps in procedural texts\n","21.96 14441  punctuations and special characters\n","21.9 11864 technical terms and phrases related to legal and procedural contexts\n","21.72 6631 the beginning of a text or important markers in a document\n","20.7 3019  elements related to operational or procedural contexts in a structured format\n","20.29 10802 dialogue and quotes within the text\n","19.69 6300 information related to LGBTQ+ families and their unique needs in health care contexts\n","19.03 5441  content related to legal processes and outcomes\n","18.07 6131  numerical values and their relationships to age and time\n"]}]},{"cell_type":"code","source":["two_top_acts_values, two_top_acts_indices = sae_acts_2[0, -1, :].topk(feat_k, dim=-1)\n","two_top_acts_indices.sort().values"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724792717735,"user_tz":-60,"elapsed":1169,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1ed0add1-1ee0-4470-9038-1d41f7fd8a5f","id":"KUq5FnHIjLV3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([  522,  2269,  3019,  3109,  5405,  5441,  6131,  8590,  8640,  8684,\n","         9768, 10230, 10549, 15451, 15469], device='cuda:0')"]},"metadata":{},"execution_count":91}]},{"cell_type":"code","source":["common_feats = set((one_top_acts_indices).tolist()).intersection(set((two_top_acts_indices).tolist()))\n","common_feats"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724790948307,"user_tz":-60,"elapsed":277,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e0c0e41a-d1bf-4caa-af3d-c866e73fc095","id":"aHdnFhirjLV4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{2269, 3019, 3109, 5441, 6131, 8640, 9768, 10230, 15451}"]},"metadata":{},"execution_count":56}]},{"cell_type":"code","source":["len(common_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vio8o-zYku9F","executionInfo":{"status":"ok","timestamp":1724791324032,"user_tz":-60,"elapsed":245,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0d5ce015-ac42-4c89-c643-9e462a6f954e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9"]},"metadata":{},"execution_count":63}]},{"cell_type":"code","source":["for f_ind in common_feats:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724790949689,"user_tz":-60,"elapsed":251,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e13bd5c6-23fa-4186-d4ed-3f3a83fabd82","id":"c326U4xAjLV4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["8640  numerical data and references to scientific publications\n","5441  content related to legal processes and outcomes\n","3109  technical terms and symbols related to data structures\n","9768 terms related to control and authority, particularly in political or systemic contexts\n","3019  elements related to operational or procedural contexts in a structured format\n","6131  numerical values and their relationships to age and time\n","10230 numerical values and quantities in the text\n","15451  structured data or attributes in a coded format\n","2269  structured lists or numbered steps in procedural texts\n"]}]},{"cell_type":"code","source":["spa_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())"],"metadata":{"id":"143RvDi0qHBS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for f_ind in spa_only:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724792761777,"user_tz":-60,"elapsed":7,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"78067d76-40bb-4bde-b673-d30fceb4f084","id":"OnipbcoJqE5p"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["522  numerical data related to gene activity or expression\n","8684  technical jargon and programming-related terms\n","15469 mathematical expressions and symbols, indicating calculations or operations\n","8590 instances and definitions related to the 'delete' function in programming, particularly in JavaScript\n","10549 specific formatting or markup characters and sections in the text\n","5405  numerical data and statistics\n"]}]},{"cell_type":"markdown","source":["### top 100"],"metadata":{"id":"eKeY5W75qZeZ"}},{"cell_type":"code","source":["feat_k = 100\n","one_top_acts_values, one_top_acts_indices = sae_acts_1[0, -1, :].topk(feat_k, dim=-1)\n","# one_top_acts_indices.sort().values"],"metadata":{"id":"JFPLoNI-qaJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for val, ind in zip(one_top_acts_values, one_top_acts_indices):\n","    print(round(val.item(), 2), ind.item(), feat_labels_lst[ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724792821055,"user_tz":-60,"elapsed":333,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b261825a-215c-489c-d2cc-c6bcbe0c7ac0","id":"DvZKvS4ZqaKA"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["64.49 15451  structured data or attributes in a coded format\n","45.17 9768 terms related to control and authority, particularly in political or systemic contexts\n","40.59 10230 numerical values and quantities in the text\n","35.36 10841 references to people and relationships in a narrative context\n","34.37 3109  technical terms and symbols related to data structures\n","30.33 8640  numerical data and references to scientific publications\n","30.23 2269  structured lists or numbered steps in procedural texts\n","21.96 14441  punctuations and special characters\n","21.9 11864 technical terms and phrases related to legal and procedural contexts\n","21.72 6631 the beginning of a text or important markers in a document\n","20.7 3019  elements related to operational or procedural contexts in a structured format\n","20.29 10802 dialogue and quotes within the text\n","19.69 6300 information related to LGBTQ+ families and their unique needs in health care contexts\n","19.03 5441  content related to legal processes and outcomes\n","18.07 6131  numerical values and their relationships to age and time\n","18.02 5405  numerical data and statistics\n","16.91 2916 references to technology and its implications\n","16.76 15469 mathematical expressions and symbols, indicating calculations or operations\n","15.07 10584 proper nouns.\n","14.36 8750  themes related to social awareness and community issues\n","14.15 12658 mathematical terminology and symbols, particularly in equations and expressions\n","13.97 14030 references to unisex bathrooms and related signage\n","13.85 12853 specific identifiers, metrics, or references related to dimensional analysis or graphical data representation\n","13.56 6143 phrases related to medical conditions and treatments\n","13.21 609 terms related to research articles and their attributes, specifically focusing on methodologies and data contributions\n","13.06 6779 greetings and conversational prompts\n","13.06 13860 statements related to election integrity and voter registration\n","12.93 11003  words indicating causation or reasoning\n","12.83 13112 specific numerical values and quantities in the text\n","12.46 6954 commands and calls to action, especially those invoking collective effort or encouragement\n","12.37 5811  sequences of numerical data and references to tables or figures\n","12.24 7180 references to various forms of art and cultural commentary\n","11.64 1583 words related to administrative or technical tasks and issues\n","11.55 16150 details about physical movements or actions related to exercises and yoga poses\n","11.55 9844 positive sentiment in customer feedback about service and products\n","11.51 11993  expressions of frustration or disappointment\n","11.49 6305 mathematical expressions and notations used in equations and proofs\n","11.48 7717 expressions of gratitude and friendly communication\n","11.45 522  numerical data related to gene activity or expression\n","11.32 8820 references to ethical concerns and issues related to accountability\n","11.03 6793 elements that resemble structured data or identifiers, likely in a list or JSON format\n","10.76 290 references to songs and musical works\n","10.72 7881 references to iron and related biochemical processes\n","10.7 11171 references to titles or distinctions within a lineage or notable families\n","10.56 5440 special formatting or structural elements within the text\n","10.54 4835 elements related to childhood and play experiences\n","9.93 1094 special characters and mathematical symbols\n","9.88 5614 specific geometric shapes and their properties in mathematical contexts\n","9.73 1198  instructions and steps in a process\n","9.51 12329  statements about violation of rules or regulations\n","9.19 11671 phrases indicating the desires or preferences of individuals\n","8.88 15763  sequences of numbers and their occurrences within the document\n","8.81 15782  mathematical expressions and technical specifications\n","8.76 15509 words likely to be near the end of sentences\n","8.58 1692 legal and technical terminology related to statutes and inventions\n","8.57 3748  specific keywords and phrases related to formal or legal scenarios\n","8.54 319 numerical values, particularly for structured data or mathematical results\n","8.46 9618 expressions related to events, seasons, and community engagement\n","8.34 13571 sections or elements related to statistical notation and algebra\n","8.34 10517  mentions of individuals' names and titles\n","8.29 13622 terms related to implementation strategies in mental health interventions\n","8.01 7868 keywords related to software functionality and user actions\n","7.92 3848 keywords and terms related to functionality and usability in various contexts\n","7.88 4565 terms related to scientific measurement and analysis\n","7.87 12161 terms related to programming and technical constructs\n","7.82 6996 character names and titles associated with specific series or media\n","7.77 9678 terms related to health and safety, particularly in medical and consumer contexts\n","7.66 11576 the presence of numerical values or references to numbered steps\n","7.62 9557 temperature-related data and numerical statistics\n","7.57 3032 numerical data or references to values and measurements\n","7.49 5623 structured data and references to phases in research studies\n","7.41 11099 questions and interactions related to discussions or dialogues\n","7.32 6917 elements related to data formatting and organization, including lists and numerical representations\n","7.23 12153 references to specific numerical identifiers or titles associated with cultural works\n","7.18 11938 specific medical histories and conditions related to participant eligibility in clinical trials\n","7.02 8872  musical and lyrical references in songs\n","7.0 11925 exclamatory expressions and emotional responses\n","0.0 0 mentions of the letter 'Z' and variations of 'Z' in different contexts\n","0.0 1 phrases emphasizing continuity or progression in narratives\n","0.0 2 elements related to interviews and interrogation processes\n","0.0 3 instances of the verb \"to be\" in various forms\n","0.0 4  temporal references such as days, weeks, months, or years\n","0.0 5 words related to trends and patterns in various contexts\n","0.0 6 references to television talent shows and their judges\n","0.0 7 conjunctions and transitional phrases that signify contrast or condition\n","0.0 8  programming-related constructs and structures\n","0.0 9 words and phrases associated with conflict and physical confrontations\n","0.0 10 terms related to confirmation and verification processes\n","0.0 11 terms related to applications and industries, especially in the context of materials and products\n","0.0 12  import statements in programming code\n","0.0 13  phrases related to missed opportunities or regrets\n","0.0 14  mentions of logarithmic or related mathematical concepts\n","0.0 15  mathematical expressions and operations\n","0.0 16 dates and times related to a specific event.\n","0.0 17  structured data or programming code elements\n","0.0 18  numerical data and associated values\n","0.0 19  terminology related to academic research and analysis methods\n","0.0 20  instances of possessive pronouns or possessive constructions\n","0.0 21 terms related to agricultural practices and research\n","0.0 22  references to the React library and its components\n"]}]},{"cell_type":"code","source":["two_top_acts_values, two_top_acts_indices = sae_acts_2[0, -1, :].topk(feat_k, dim=-1)\n","# two_top_acts_indices.sort().values"],"metadata":{"id":"j6WnJq5YqaKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for val, ind in zip(two_top_acts_values, two_top_acts_indices):\n","    print(round(val.item(), 2), ind.item(), feat_labels_lst[ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CMkVp5ngqjHS","executionInfo":{"status":"ok","timestamp":1724792853795,"user_tz":-60,"elapsed":249,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"60518885-38dc-42db-be05-1df1a91bb557"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["62.01 15451  structured data or attributes in a coded format\n","45.38 8590 instances and definitions related to the 'delete' function in programming, particularly in JavaScript\n","40.29 9768 terms related to control and authority, particularly in political or systemic contexts\n","37.59 6131  numerical values and their relationships to age and time\n","34.23 3109  technical terms and symbols related to data structures\n","27.93 2269  structured lists or numbered steps in procedural texts\n","25.1 10230 numerical values and quantities in the text\n","23.56 5441  content related to legal processes and outcomes\n","22.16 522  numerical data related to gene activity or expression\n","22.16 8640  numerical data and references to scientific publications\n","21.08 5405  numerical data and statistics\n","16.91 8684  technical jargon and programming-related terms\n","16.77 3019  elements related to operational or procedural contexts in a structured format\n","16.72 10549 specific formatting or markup characters and sections in the text\n","16.24 15469 mathematical expressions and symbols, indicating calculations or operations\n","16.22 6300 information related to LGBTQ+ families and their unique needs in health care contexts\n","15.83 3032 numerical data or references to values and measurements\n","15.5 609 terms related to research articles and their attributes, specifically focusing on methodologies and data contributions\n","15.35 14441  punctuations and special characters\n","14.84 6917 elements related to data formatting and organization, including lists and numerical representations\n","14.09 8820 references to ethical concerns and issues related to accountability\n","14.02 14689 elements related to technical instructions or commands in a computing context\n","13.86 12161 terms related to programming and technical constructs\n","13.53 11864 technical terms and phrases related to legal and procedural contexts\n","13.4 117 repeated instances of specific numerical or categorical markers in data analysis\n","13.3 14030 references to unisex bathrooms and related signage\n","12.27 11993  expressions of frustration or disappointment\n","12.15 12658 mathematical terminology and symbols, particularly in equations and expressions\n","11.79 5811  sequences of numerical data and references to tables or figures\n","11.21 10802 dialogue and quotes within the text\n","10.83 15763  sequences of numbers and their occurrences within the document\n","10.65 1583 words related to administrative or technical tasks and issues\n","10.25 1094 special characters and mathematical symbols\n","10.23 1692 legal and technical terminology related to statutes and inventions\n","10.01 5614 specific geometric shapes and their properties in mathematical contexts\n","9.86 8750  themes related to social awareness and community issues\n","9.81 6779 greetings and conversational prompts\n","9.54 2916 references to technology and its implications\n","9.47 5440 special formatting or structural elements within the text\n","9.38 12853 specific identifiers, metrics, or references related to dimensional analysis or graphical data representation\n","9.36 6143 phrases related to medical conditions and treatments\n","9.24 4295 connections and relationships among elements in technical or procedural contexts\n","8.92 8366 verbs and their related forms, often related to medical or technical contexts\n","8.34 9844 positive sentiment in customer feedback about service and products\n","8.24 15587 mathematical calculations and expressions\n","8.11 13142  phrases and terminology related to data management and programming tasks\n","8.02 6080 recommendations or suggestions related to various topics\n","7.8 9473  semicolons or statement terminators in programming code\n","7.79 11576 the presence of numerical values or references to numbered steps\n","7.65 11099 questions and interactions related to discussions or dialogues\n","7.33 9557 temperature-related data and numerical statistics\n","7.17 13808 references to medical information and conditions\n","7.11 15541 elements related to formatting and presentation in text or programming contexts\n","7.11 14781 terms and definitions associated with names and their meanings\n","7.1 11938 specific medical histories and conditions related to participant eligibility in clinical trials\n","7.07 10841 references to people and relationships in a narrative context\n","7.01 13581 products related to portable electronics and appliances\n","0.0 0 mentions of the letter 'Z' and variations of 'Z' in different contexts\n","0.0 1 phrases emphasizing continuity or progression in narratives\n","0.0 2 elements related to interviews and interrogation processes\n","0.0 3 instances of the verb \"to be\" in various forms\n","0.0 4  temporal references such as days, weeks, months, or years\n","0.0 5 words related to trends and patterns in various contexts\n","0.0 6 references to television talent shows and their judges\n","0.0 7 conjunctions and transitional phrases that signify contrast or condition\n","0.0 8  programming-related constructs and structures\n","0.0 9 words and phrases associated with conflict and physical confrontations\n","0.0 10 terms related to confirmation and verification processes\n","0.0 11 terms related to applications and industries, especially in the context of materials and products\n","0.0 12  import statements in programming code\n","0.0 13  phrases related to missed opportunities or regrets\n","0.0 14  mentions of logarithmic or related mathematical concepts\n","0.0 15  mathematical expressions and operations\n","0.0 16 dates and times related to a specific event.\n","0.0 17  structured data or programming code elements\n","0.0 18  numerical data and associated values\n","0.0 19  terminology related to academic research and analysis methods\n","0.0 20  instances of possessive pronouns or possessive constructions\n","0.0 21 terms related to agricultural practices and research\n","0.0 22  references to the React library and its components\n","0.0 23 references to the name \"Jonathan\" and variations of \"Jon.\"\n","0.0 24  terms related to \"coroner\" and associated medical investigations\n","0.0 25 references to issued equipment or supplies\n","0.0 26  terms related to leadership styles and their effects on workplace dynamics\n","0.0 27 mathematical symbols and constructs\n","0.0 28 location and administrative division references\n","0.0 29  phrases indicating various purposes and uses of information or actions\n","0.0 30 terms and references associated with political and social organizations\n","0.0 31 punctuation marks and special symbols often found in written texts\n","0.0 32  mathematical symbols and notation typically used in equations or algorithms\n","0.0 33 terms related to research methodologies and data analysis in scientific studies\n","0.0 34 words and phrases associated with conjunctions and connections between ideas\n","0.0 35 references to the verb \"be\" in various contexts\n","0.0 36 words or roots related to specific dependencies or components in a software or programming context\n","0.0 37  punctuated phrases and statements\n","0.0 38 modal verbs and their usage in sentences\n","0.0 39 details about product features and specifications\n","0.0 40 phrases related to hybrid combinations of concepts or elements in various contexts\n","0.0 41 phrases related to family relationships and marital status\n","0.0 42 terms related to medical treatment options and their effectiveness\n"]}]},{"cell_type":"code","source":["common_feats = set((one_top_acts_indices).tolist()).intersection(set((two_top_acts_indices).tolist()))\n","# common_feats"],"metadata":{"id":"etWKK9yPqaKA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(common_feats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724792989027,"user_tz":-60,"elapsed":365,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2d61bf4b-2505-43f7-bd28-992a459cab58","id":"XrAyAp7jqaKA"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["65"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["spa_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in spa_only:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724793012760,"user_tz":-60,"elapsed":275,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"Gsaunv7fqaKA","outputId":"fac8c4d6-6463-4d80-f952-cfd94150396b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["9473  semicolons or statement terminators in programming code\n","13581 products related to portable electronics and appliances\n","8590 instances and definitions related to the 'delete' function in programming, particularly in JavaScript\n","23 references to the name \"Jonathan\" and variations of \"Jon.\"\n","24  terms related to \"coroner\" and associated medical investigations\n","25 references to issued equipment or supplies\n","26  terms related to leadership styles and their effects on workplace dynamics\n","27 mathematical symbols and constructs\n","28 location and administrative division references\n","29  phrases indicating various purposes and uses of information or actions\n","30 terms and references associated with political and social organizations\n","31 punctuation marks and special symbols often found in written texts\n","32  mathematical symbols and notation typically used in equations or algorithms\n","33 terms related to research methodologies and data analysis in scientific studies\n","34 words and phrases associated with conjunctions and connections between ideas\n","35 references to the verb \"be\" in various contexts\n","36 words or roots related to specific dependencies or components in a software or programming context\n","37  punctuated phrases and statements\n","38 modal verbs and their usage in sentences\n","39 details about product features and specifications\n","40 phrases related to hybrid combinations of concepts or elements in various contexts\n","41 phrases related to family relationships and marital status\n","42 terms related to medical treatment options and their effectiveness\n","8366 verbs and their related forms, often related to medical or technical contexts\n","15541 elements related to formatting and presentation in text or programming contexts\n","10549 specific formatting or markup characters and sections in the text\n","14781 terms and definitions associated with names and their meanings\n","6080 recommendations or suggestions related to various topics\n","4295 connections and relationships among elements in technical or procedural contexts\n","13142  phrases and terminology related to data management and programming tasks\n","14689 elements related to technical instructions or commands in a computing context\n","15587 mathematical calculations and expressions\n","8684  technical jargon and programming-related terms\n","13808 references to medical information and conditions\n","117 repeated instances of specific numerical or categorical markers in data analysis\n"]}]},{"cell_type":"markdown","source":["# umap on feature subset"],"metadata":{"id":"Z8Mf98oXwJQV"}},{"cell_type":"code","source":["import umap"],"metadata":{"id":"_FpS7c3mwmG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["feature_subset = two_top_acts_indices.tolist() + one_top_acts_indices.tolist()\n","feature_subset = list(set(feature_subset))\n","feature_subset.sort()\n","len(feature_subset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVYqUgBPwHYr","executionInfo":{"status":"ok","timestamp":1724797367080,"user_tz":-60,"elapsed":347,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b83b89c9-8a04-4433-910d-318df1cc586f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["88"]},"metadata":{},"execution_count":212}]},{"cell_type":"code","source":["f_labels = [ \"F\" + str(f_ind) + \" : \" + feat_labels_lst[f_ind] for f_ind in feature_subset]"],"metadata":{"id":"VifJah73xU-n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now labels and weight matrix may share the same new indices (rather than feature numbers of orig matrix) due to slicing them in same way"],"metadata":{"id":"ZgS2tMwGx4gK"}},{"cell_type":"code","source":["weights_feature_subset = weight_matrix[feature_subset]\n","weights_feature_subset.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F_EieYpPw4Sn","executionInfo":{"status":"ok","timestamp":1724794820915,"user_tz":-60,"elapsed":307,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"255bda11-6f83-4d07-eadd-9fe9007aac69"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(88, 2304)"]},"metadata":{},"execution_count":144}]},{"cell_type":"code","source":["reducer = umap.UMAP(n_neighbors=15, min_dist=0.01, metric='euclidean')\n","embedding1 = reducer.fit_transform(weights_feature_subset)"],"metadata":{"id":"Cf5LZ_dHFtTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_feats = set((one_top_acts_indices).tolist()).intersection(set((two_top_acts_indices).tolist()))\n","months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())"],"metadata":{"id":"kIxQJkLQy-9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["common_feats = list(common_feats)\n","months_only = list(months_only )"],"metadata":{"id":"NEOX3HfI0rWH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import plotly.express as px\n","import numpy as np\n","import pandas as pd\n","\n","df = pd.DataFrame(embedding1, columns=['UMAP Component 1', 'UMAP Component 2'])\n","df['Feature ID'] = feature_subset\n","# df['Feature Description']\n","df['feature'] = [i[:75] for i in f_labels] # [str(i) for i in feature_subset] # feature_subset\n","df['Color'] = df['Feature ID'].apply(lambda x: 'green' if x in common_feats else ('red' if x in months_only else 'blue'))\n","\n","fig = px.scatter(df, x='UMAP Component 1', y='UMAP Component 2', # text='Feature ID',\n","                 color='Color',  # Use the Color column for coloring points\n","                 color_discrete_map={'red': 'red', 'green': 'green', 'blue': 'blue'},\n","                #  hover_data='Feature Description')\n","                hover_data={\n","                     'UMAP Component 1': False,\n","                     'UMAP Component 2': False,\n","                     'Feature ID': False,\n","                     'Color': False,\n","                     'feature': True  # Only display the feature description\n","                 } ) #,\n","                #  size=[0.1] * len(df))\n","\n","fig.update_layout(\n","    title='UMAP of Decoder Weights',\n","    xaxis_title='UMAP Component 1',\n","    yaxis_title='UMAP Component 2'\n",")\n","\n","# Customize legend labels\n","fig.for_each_trace(lambda t: t.update(name = {\n","    'red': 'Months Only',\n","    'green': 'Both',\n","    'blue': 'Numbers Only'\n","}[t.name]))\n","\n","fig.update_traces(marker=dict(size= 8))\n","\n","fig.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"8KHLgwkiw9-6","executionInfo":{"status":"ok","timestamp":1724798463588,"user_tz":-60,"elapsed":554,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7b35ee77-bcfe-41df-8808-9746895a5de8"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<html>\n","<head><meta charset=\"utf-8\" /></head>\n","<body>\n","    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n","        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.34.0.min.js\"></script>                <div id=\"c061a563-a62d-4d63-94c0-12a63f950123\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c061a563-a62d-4d63-94c0-12a63f950123\")) {                    Plotly.newPlot(                        \"c061a563-a62d-4d63-94c0-12a63f950123\",                        [{\"customdata\":[[290,\"blue\",\"F290 : references to songs and musical works\"],[1198,\"blue\",\"F1198 :  instructions and steps in a process\"],[1583,\"blue\",\"F1583 : words related to administrative or technical tasks and issues\"],[2916,\"blue\",\"F2916 : references to technology and its implications\"],[4835,\"blue\",\"F4835 : elements related to childhood and play experiences\"],[5440,\"blue\",\"F5440 : special formatting or structural elements within the text\"],[5441,\"blue\",\"F5441 :  content related to legal processes and outcomes\"],[5614,\"blue\",\"F5614 : specific geometric shapes and their properties in mathematical cont\"],[5811,\"blue\",\"F5811 :  sequences of numerical data and references to tables or figures\"],[6131,\"blue\",\"F6131 :  numerical values and their relationships to age and time\"],[6143,\"blue\",\"F6143 : phrases related to medical conditions and treatments\"],[6300,\"blue\",\"F6300 : information related to LGBTQ+ families and their unique needs in he\"],[6305,\"blue\",\"F6305 : mathematical expressions and notations used in equations and proofs\"],[6631,\"blue\",\"F6631 : the beginning of a text or important markers in a document\"],[6779,\"blue\",\"F6779 : greetings and conversational prompts\"],[6793,\"blue\",\"F6793 : elements that resemble structured data or identifiers, likely in a \"],[6954,\"blue\",\"F6954 : commands and calls to action, especially those invoking collective \"],[7180,\"blue\",\"F7180 : references to various forms of art and cultural commentary\"],[7717,\"blue\",\"F7717 : expressions of gratitude and friendly communication\"],[7881,\"blue\",\"F7881 : references to iron and related biochemical processes\"],[8750,\"blue\",\"F8750 :  themes related to social awareness and community issues\"],[9844,\"blue\",\"F9844 : positive sentiment in customer feedback about service and products\"],[10230,\"blue\",\"F10230 : numerical values and quantities in the text\"],[10584,\"blue\",\"F10584 : proper nouns.\"],[10802,\"blue\",\"F10802 : dialogue and quotes within the text\"],[10841,\"blue\",\"F10841 : references to people and relationships in a narrative context\"],[11003,\"blue\",\"F11003 :  words indicating causation or reasoning\"],[11171,\"blue\",\"F11171 : references to titles or distinctions within a lineage or notable f\"],[11864,\"blue\",\"F11864 : technical terms and phrases related to legal and procedural contex\"],[11993,\"blue\",\"F11993 :  expressions of frustration or disappointment\"],[12329,\"blue\",\"F12329 :  statements about violation of rules or regulations\"],[12853,\"blue\",\"F12853 : specific identifiers, metrics, or references related to dimensiona\"],[13112,\"blue\",\"F13112 : specific numerical values and quantities in the text\"],[13860,\"blue\",\"F13860 : statements related to election integrity and voter registration\"],[14030,\"blue\",\"F14030 : references to unisex bathrooms and related signage\"],[14441,\"blue\",\"F14441 :  punctuations and special characters\"],[15469,\"blue\",\"F15469 : mathematical expressions and symbols, indicating calculations or o\"],[16150,\"blue\",\"F16150 : details about physical movements or actions related to exercises a\"]],\"hovertemplate\":\"feature=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"blue\",\"marker\":{\"color\":\"blue\",\"symbol\":\"circle\",\"size\":8},\"mode\":\"markers\",\"name\":\"Numbers Only\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[13.75345,12.923385,14.683951,14.153081,13.209967,14.637615,16.271503,15.106781,13.976579,16.60797,15.726294,13.610335,14.90986,15.41549,13.139114,15.526065,13.027321,12.9784,12.868438,16.456486,15.139067,14.252894,16.62183,13.0497875,13.009216,13.672823,14.0938015,15.393509,14.884364,13.110721,14.556663,16.074429,13.563986,13.573089,14.070298,15.528915,13.921982,12.868771],\"xaxis\":\"x\",\"y\":[3.2718353,2.2558737,4.312455,3.2371387,2.7951953,2.268766,0.78272724,0.7654897,0.84340346,0.9200726,4.3489556,2.3800192,3.4748175,3.373634,1.8864483,1.4486879,1.9599953,3.188582,2.0601265,2.6276977,4.240248,3.7819118,1.3326746,2.6053455,2.2941034,3.4681032,3.4141262,3.8873718,4.0680733,3.0685444,3.8231852,2.6910808,3.1227295,3.4316366,3.028871,2.9508908,0.7505008,2.7415657],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[522,\"green\",\"F522 :  numerical data related to gene activity or expression\"],[609,\"green\",\"F609 : terms related to research articles and their attributes, specificall\"],[1094,\"green\",\"F1094 : special characters and mathematical symbols\"],[2269,\"green\",\"F2269 :  structured lists or numbered steps in procedural texts\"],[3019,\"green\",\"F3019 :  elements related to operational or procedural contexts in a struct\"],[3109,\"green\",\"F3109 :  technical terms and symbols related to data structures\"],[5405,\"green\",\"F5405 :  numerical data and statistics\"],[8640,\"green\",\"F8640 :  numerical data and references to scientific publications\"],[8820,\"green\",\"F8820 : references to ethical concerns and issues related to accountability\"],[9768,\"green\",\"F9768 : terms related to control and authority, particularly in political o\"],[12658,\"green\",\"F12658 : mathematical terminology and symbols, particularly in equations an\"],[15451,\"green\",\"F15451 :  structured data or attributes in a coded format\"]],\"hovertemplate\":\"feature=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"green\",\"marker\":{\"color\":\"green\",\"symbol\":\"circle\",\"size\":8},\"mode\":\"markers\",\"name\":\"Both\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[14.334786,15.419083,15.039111,14.324929,14.834765,14.2934,14.171412,14.543412,15.646115,15.004181,16.64897,14.367578],\"xaxis\":\"x\",\"y\":[0.47255465,2.8100424,2.3211017,2.0489278,3.6385207,0.6735643,0.29399788,1.6140038,3.8949175,3.1911063,2.3033044,0.37589502],\"yaxis\":\"y\",\"type\":\"scatter\"},{\"customdata\":[[1039,\"red\",\"F1039 : technical terms and elements within programming and coding contexts\"],[1046,\"red\",\"F1046 :  dates and events related to the calendar\"],[1233,\"red\",\"F1233 : scientific terminology related to statistics and experimental metho\"],[1530,\"red\",\"F1530 : technical terms related to inventions and scientific descriptions\"],[1692,\"red\",\"F1692 : legal and technical terminology related to statutes and inventions\"],[3032,\"red\",\"F3032 : numerical data or references to values and measurements\"],[3162,\"red\",\"F3162 :  markers indicating the beginning of a document\"],[4295,\"red\",\"F4295 : connections and relationships among elements in technical or proced\"],[5052,\"red\",\"F5052 : the beginning of a document or section, likely signaling the start \"],[5710,\"red\",\"F5710 : references to seasonal festivals and their associated cultural sign\"],[5968,\"red\",\"F5968 : references to specific dates, particularly in October and April\"],[6082,\"red\",\"F6082 :  segments related to instructions or guidelines\"],[6240,\"red\",\"F6240 :  words related to time indicators, specifically months or dates\"],[6612,\"red\",\"F6612 : instances of numbers and counting, indicating calculations or measu\"],[7159,\"red\",\"F7159 :  references to time frames and chronological events\"],[8684,\"red\",\"F8684 :  technical jargon and programming-related terms\"],[9203,\"red\",\"F9203 : the start of sections or paragraphs in the text\"],[10103,\"red\",\"F10103 : dates, specifically in the month of July\"],[10174,\"red\",\"F10174 :  patterns or codes related to data structures or object representa\"],[10254,\"red\",\"F10254 : references to days of the week and their associated schedules\"],[10636,\"red\",\"F10636 : specific dates or references to time\"],[10997,\"red\",\"F10997 :  references to specific health conditions or treatments\"],[11004,\"red\",\"F11004 : categories and statistics related to demographic groups\"],[11210,\"red\",\"F11210 :  expressions related to logical comparisons\"],[11546,\"red\",\"F11546 : dates and months\"],[11938,\"red\",\"F11938 : specific medical histories and conditions related to participant e\"],[12161,\"red\",\"F12161 : terms related to programming and technical constructs\"],[12748,\"red\",\"F12748 :  structured data representations and their attributes\"],[12958,\"red\",\"F12958 : words and themes associated with seasonal transitions and celebrat\"],[13254,\"red\",\"F13254 :  terms related to experimental control and intervention settings i\"],[13767,\"red\",\"F13767 : terms related to approval and assessment in a regulatory or medica\"],[13817,\"red\",\"F13817 : dates and times\"],[14017,\"red\",\"F14017 : conjunctions and relationships between elements, particularly in l\"],[14249,\"red\",\"F14249 : links to privacy and cookie policies\"],[14388,\"red\",\"F14388 : terms and phrases related to technology, particularly in the conte\"],[14782,\"red\",\"F14782 :  references to data structures and types in programming\"],[15194,\"red\",\"F15194 : specific terms related to legal and governmental contexts\"],[15763,\"red\",\"F15763 :  sequences of numbers and their occurrences within the document\"]],\"hovertemplate\":\"feature=%{customdata[2]}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"legendgroup\":\"red\",\"marker\":{\"color\":\"red\",\"symbol\":\"circle\",\"size\":8},\"mode\":\"markers\",\"name\":\"Months Only\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[14.766274,15.413507,15.319165,15.665925,15.363124,14.1375475,16.026485,15.028143,15.853862,15.543636,13.548557,15.7951145,15.964493,14.846476,16.157116,15.108818,15.18494,15.95307,16.228987,16.38658,16.284819,13.737755,16.21147,15.43679,16.378376,15.997549,15.429755,15.330973,15.764529,16.541828,14.007136,15.679228,14.521381,15.972781,15.067742,14.9906645,16.130919,14.620452],\"xaxis\":\"x\",\"y\":[1.1822549,0.04343798,2.2507572,1.9099731,3.4639769,0.30521384,0.91358995,0.59582734,1.8085282,0.12744753,1.319784,3.4053738,0.33482474,2.5951414,0.2936501,1.6905583,2.628421,1.2993389,0.7279279,0.6025398,2.247296,1.4559394,3.3392386,0.73133296,1.7807057,3.7644215,4.0574656,1.4284968,0.19099459,2.800239,1.5860244,4.2257366,0.25425693,4.060485,4.118892,2.090782,3.7169633,1.0261694],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UMAP Component 1\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"UMAP Component 2\"}},\"legend\":{\"title\":{\"text\":\"Color\"},\"tracegroupgap\":0},\"margin\":{\"t\":60},\"title\":{\"text\":\"UMAP of Decoder Weights\"}},                        {\"responsive\": true}                    ).then(function(){\n","                            \n","var gd = document.getElementById('c061a563-a62d-4d63-94c0-12a63f950123');\n","var x = new MutationObserver(function (mutations, observer) {{\n","        var display = window.getComputedStyle(gd).display;\n","        if (!display || display === 'none') {{\n","            console.log([gd, 'removed!']);\n","            Plotly.purge(gd);\n","            observer.disconnect();\n","        }}\n","}});\n","\n","// Listen for the removal of the full notebook cells\n","var notebookContainer = gd.closest('#notebook-container');\n","if (notebookContainer) {{\n","    x.observe(notebookContainer, {childList: true});\n","}}\n","\n","// Listen for the clearing of the current output cell\n","var outputEl = gd.closest('.output');\n","if (outputEl) {{\n","    x.observe(outputEl, {childList: true});\n","}}\n","\n","                        })                };                            </script>        </div>\n","</body>\n","</html>"]},"metadata":{}}]},{"cell_type":"code","source":["from google.colab import files\n","output_filename = 'umap.html'\n","fig.write_html(output_filename)\n","files.download(output_filename)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"OFI3yP_t_EsK","executionInfo":{"status":"ok","timestamp":1724798471149,"user_tz":-60,"elapsed":304,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5af54462-9565-43a9-d6cd-bf9c9efe2323"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_dd646692-9945-49e3-be1d-6dc0c2004b16\", \"umap.html\", 3661855)"]},"metadata":{}}]},{"cell_type":"markdown","source":["# ablate then generate"],"metadata":{"id":"NINCeYomqek1"}},{"cell_type":"code","source":["# prompt = \"January February March April\"\n","# inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","\n","# outputs = model.generate(input_ids=inputs, max_new_tokens=1)\n","# print(tokenizer.decode(outputs[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QnaFOwmPz-KM","executionInfo":{"status":"ok","timestamp":1724845841266,"user_tz":-60,"elapsed":2132,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"13bedb77-98b5-41fa-a61f-8182c4e08ed3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bos>January February March April May\n"]}]},{"cell_type":"code","source":["# model_2 = AutoModelForCausalLM.from_pretrained(\n","#     \"google/gemma-2-2b\",\n","#     device_map='auto',\n","# )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["d66a6dcabf584889ac001907219fc8d4","2510e86c8b99435d854c66289e7c0232","34a4a3b75a3048bcbbd41d9a406aa8f0","4914437272c94aa7a7c4776f232ae728","a6bee4f0563748eabd4c89d7a9211c5b","15a9fb4a0fe24b3e8958a1054f3f85bf","a46252fc50a744a8a5abc5e00a43e7cb","3362157ac077431c80dd5421951f5c89","19131713e19a4624b2ceeccd8cb7867c","f8b2f493a6f64b3997cd6eab97e2cc38","f7c74b817ac44a668b8ce7c19999d752"]},"id":"rUSsFugn0VTy","executionInfo":{"status":"ok","timestamp":1724845852964,"user_tz":-60,"elapsed":4658,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"37795f3a-4171-4ee9-ba58-dde696f06d06"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d66a6dcabf584889ac001907219fc8d4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:accelerate.big_modeling:Some parameters are on the meta device device because they were offloaded to the cpu.\n"]}]},{"cell_type":"code","source":["# # from transformers import AutoModelForCausalLM, AutoTokenizer\n","# # import torch\n","# # from torch import nn\n","\n","# def patch_mlp_vectors(module, input, output, layer_to_patch, LLM_patch):\n","#     if isinstance(module, nn.Linear) and module.in_features == module.out_features:\n","#         if module.__class__.__name__ == f\"GemmaDecoderLayer{layer_to_patch}\":\n","#             return LLM_patch\n","#     return output\n","\n","# def patch_model_layer(model, layer_to_patch, LLM_patch):\n","#     for name, module in model.named_modules():\n","#         if isinstance(module, nn.Linear) and module.in_features == module.out_features:\n","#             if module.__class__.__name__ == f\"GemmaDecoderLayer{layer_to_patch}\":\n","#                 module.register_forward_hook(lambda mod, inp, out: patch_mlp_vectors(mod, inp, out, layer_to_patch, LLM_patch))\n","\n","# prompt = \"January February March April\"\n","# inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","\n","# LLM_patch = torch.randn_like(model_2.get_input_embeddings()(inputs))\n","\n","# # Patch the model\n","# layer_to_patch = 0  # Adjust this to patch different layers\n","# patch_model_layer(model_2, layer_to_patch, LLM_patch)\n","\n","# # Generate output with the patched model\n","# with torch.no_grad():\n","#     outputs = model_2.generate(input_ids=inputs, max_new_tokens=1)\n","#     print(tokenizer.decode(outputs[0]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dmOcjO4Hzg_h","executionInfo":{"status":"ok","timestamp":1724845934241,"user_tz":-60,"elapsed":657,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"018cf27d-77e4-41e1-9e7c-4a512cf6cb67"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<bos>January February March April May\n"]}]},{"cell_type":"markdown","source":["## patch using transformerlens"],"metadata":{"id":"NU3TRser6AdO"}},{"cell_type":"code","source":["%%capture\n","!pip install transformer_lens"],"metadata":{"id":"44hx2_jY1SaC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformer_lens import HookedTransformer\n","\n","# uses a lot of memory, use A100\n","model_2 = HookedTransformer.from_pretrained(\n","    \"gemma-2-2b\"\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":473,"referenced_widgets":["df10fd7618d94e4d82b550327262027e","be89c1f746474a36886966b347d7da2d","b3bbf8781c034182a5d10cf7e021508b","3d7f272623624f86b638df8c2173def8","349e77c234754accb437b462e89dfec8","50cc32250e6b4c5d899baf92ba7d48f9","0193bc6a2ff7403ea9c2beb75ac183dc","06c7d590f3734392b062b6c661c93161","98b1af11da384a90a3bc03becb944080","891f778d7da44ce79af59068975e9c48","0bbe1506db6446cea742a33965a5b21e","6ad1bb51252a4e38be6056e61a601cb3","fed3d9711b9f4cf5ac8199a9275f1dcf","b1d87fae274d4142920b07a398bde156","2d6cf6154b174ca0aaaf2801fc198616","cc29ef7677da4ad98407c1769d348b0b","8dfe4278092b4c24bf1d52c039c3a6df","ea8dc13d679a4709b349352cbb4f8ab4","fa28ff5c942441b2af3c51801422ae3d","f1ea01866dae4d40b6a5539ff8f17dcf","9f5d16a0fbd1424c8a30a8f6bcf0eea9","75c1db6ec810438ca1aad337942ede1c","bf64fb4c744441249afa9c12dab787ee","f6d2291378b54eabbd90ab63ba29269a","d8cb9fe352554550b22c75cbbdeec93f","7042f4c5bdee489a9f20c71a75b81592","9ad0348892d04d588c02ecd1e383ba48","df989a6d89ee42b1a63a98c08cbe4faa","fac86c64d8f349c39b26c4b6c084c1e8","170c943f64294423a4a44af5ffde8970","c1fea719e4e842e8b6f890836d41caf7","900a876482e9421897e1d8b1037f1ade","5c116d5a2e984a7e870d5929f20ce229","f0974b18fde94e2aaded81eb0fff2b02","a733fd1a465f4284806232465e1c7016","9253944d601743bfa4519e0f4fe521d6","cff7a2ee929b4c59872566928fc91ca0","5bc27551edea41f7b5d358685f06a7ef","12fb0a0406444ba089a9c7cfef9326b4","916ee5a51f7a4fd3aa6d98185dc40ae2","f72b4a5d01a847bda0c1111f62c3710c","ac0f6771f111495b962ebd9fcb0a9e66","8df2798e6a0a4df287c0cbb4d421a5ae","fe53c338a5cb403ba0285983ce593afc","6fba7b00b60145c78ed88ab4457c2d33","0ea40470ff04466a824d4cd01d120161","386ad95352b24782b7f77cf0a0daa064","6ea70caa9bf243c7b961af94f99ca5ee","c6e852b868d54c74ba79d2beacb0ee4a","65b071bf21e54779a9e7a8ceb17adf8b","2604e449cd754ecb814452f390207660","20892b64e194499389b2748dcdae433a","7e957f74cf444cbe8ac40b7517580cb5","aab3bc7260f44c37b04e262c2b0319e2","bfdc9cfaf5ba4fc29b69244ebdd2ccc4","959850ea258d4dc6a81cc8238e53ce33","10e3065a87154d22be886c26c0640d2e","152631b0a7a146b88089c2f0201249d1","e1707dd2527747dca53a28ed35c2206b","ed60dc3946fc405a800f76ba4af0a402","10e369c1e338411f82766c23fd727729","a8cb9e9e71a54fc2aabd23304e525d6e","9601e519fac0401ea9e06a3c78fbb74d","ab920d0dd6744949b5e0f91a076a7280","0a91d9d4d5734d348139c825c38cfe2e","d1a18238dc62457e84b5f3f9d19eb0c1","8dbc77b0202849139641db05c3e9379b","306c7d6d143f43bd86ec119371a4521d","6b7b99faefc44387be4e5ad64a2aa68f","316d2d30c18b48b68f47e21b3a02b88e","223a3752aae6429983bb7009e3fcafb2","8611a5c7a2c04ccf95cbaae523a4f807","91867d4503b24922b08bc4180722cae1","d06aa763742c4b0d9aee2bc95f1a2ab8","f4f9a9c3910c467fb413112fa9272f86","eaadf2f6f6ad4ed2aea7c1bfde84a8d6","b58354992ab84816b1268b0570091bee","ea5fd0e9e9b64b4cbb5163908a726ab2","45e735be694a49e78b8aae6e18f1f1c1","a56d6922eafa42f2af7a2927526257f4","1d2e10f83e97467c96669b2e8d9d854c","22a3b323cf354fb5aa5964b96ebaa327","a3bde62c83d0420c8b82a99f2e96bf8f","6b322f0f469a42d0817490af4c150b99","16eb25dbb16643e28de4bf0123ff66ee","35fb0928ffa6432a84514335cff59b8c","633321cdcf8c441ab761e7df86790e4f","38f2feb48fac48128f65a98d1be0c650","1897a553232043f4b0d87a15a4986f03","49bc08a6fd95497bb431952b1af7af7d","268e7cc20f0e488cacd6823368e38581","aff281c01f99455b9f9d8776faff137f","b8259e39942c444d8a0692839ac92377","68551e7604b14f2881271441d2e567dc","5c9367e260b5439abcae842782dfee5f","163f1ff368f942c4aeb80277b9e473e1","f2121e280f2b4643a444932b174cabaf","d787a3873e264d47b178ccb28b5604ea","941a6e6eeda148898ff376196f963f83","9c4c77bb05124888a4499a4ced1f9014","e0cc94fb85ee4cbf856dd82f8d118a58","4fb59904c5974b0bb4c7b2a84e35b54a","a067c5ab6a644b3a9a4dee3e231e728d","08fc7e8c16e345ccb9aa451063f28241","d04a3dabc625428cbb185b2d32c01d15","abceafcbb1c04ec88ffbaf7931951652","9502e7ce48424bdab3350925a9cd2986","cd10d3777ccb4c6f8493f81bacbfda20","c546dfc37d5d4fb6b196d399580c01d9","3acf0139cba94ba580c7fa8b91438829","1be8f31d191943198e128a64321f32dc","6e6b45f4e33148a3a9468098bce438ac","88a1164c9fe84227b5b6d5eebde3138e","56deb9b37b9c468180dbf90248690409","55bc2fa34a4045d4b52229d56e246f0a","5a7792f2f19b4d30baab0634e150f2c5","ab9ecb65f9fa41749530aec5860bb962","b36c3779ba484b88b04c0b211eacdae0","859085765d574de09940085226dd2b46","28079e919820475b94ad495808852b7d","59bf11ad72774cd58cd39fa069572baf","3295b4bacffb47bab18a3f09f0e66a0e","2a4750440e9d4b3d914086496a10ec40","0e14135fb69c48c4afbf9ba1772f9f78","8e39d00c88bc469c9e1a681b83132810","d6727a0c4825402d99a75d196de3c95c","c33e1b0ea72f4925bd41d36526f8138a","fbd37e25a53d4840905ba30e817fa52c","51ffccab09844ec39942f2176f45c441","903b270fef4848569a217f88f88ecb36","20db55e3f48e4f52b4f56fbd0f8c73fe","23389ae0071f4da28fc6457aaf447a2d"]},"id":"VbJhCygk1Vbq","executionInfo":{"status":"ok","timestamp":1724846569259,"user_tz":-60,"elapsed":113029,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2c654ed4-edf2-497c-82c9-ea3ec2ea1f01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:You tried to specify center_unembed=True for a model using logit softcap, but this can't be done! Softcapping is not invariant upon adding a constantSetting center_unembed=False instead.\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/818 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df10fd7618d94e4d82b550327262027e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad1bb51252a4e38be6056e61a601cb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf64fb4c744441249afa9c12dab787ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0974b18fde94e2aaded81eb0fff2b02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00002-of-00003.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fba7b00b60145c78ed88ab4457c2d33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model-00003-of-00003.safetensors:   0%|          | 0.00/481M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959850ea258d4dc6a81cc8238e53ce33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dbc77b0202849139641db05c3e9379b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea5fd0e9e9b64b4cbb5163908a726ab2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/46.4k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1897a553232043f4b0d87a15a4986f03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c4c77bb05124888a4499a4ced1f9014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1be8f31d191943198e128a64321f32dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3295b4bacffb47bab18a3f09f0e66a0e"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gemma-2-2b into HookedTransformer\n"]}]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","logits = model_2(tokens)\n","\n","next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_A1usnkP2UXD","executionInfo":{"status":"ok","timestamp":1724846983720,"user_tz":-60,"elapsed":213,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2c542c75-76fc-45f3-c072-2d251961e602"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' May'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":28}]},{"cell_type":"code","source":["model_2.blocks[20].hook_resid_post"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"15djqV_J3tsI","executionInfo":{"status":"ok","timestamp":1724846657702,"user_tz":-60,"elapsed":244,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6e6a51cc-72c8-497a-98c9-f05d1d21910c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["HookPoint()"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["LLM_patch = torch.zeros(1, 5, 2304)"],"metadata":{"id":"nJireqyV4p7f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch=LLM_patch,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )"],"metadata":{"id":"k027lZ_myiTS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"312awXdL4-Q3","executionInfo":{"status":"ok","timestamp":1724846977811,"user_tz":-60,"elapsed":227,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6b56095d-d0d1-4e4a-f427-0b17b494f100"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'<pad>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","source":["## patch one feature"],"metadata":{"id":"i02oozto5QYC"}},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"xIUdrmcEybbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in list(months_only)[:5]:\n","    print(f_ind, feat_labels_lst[f_ind])"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847199244,"user_tz":-60,"elapsed":246,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"770f3758-cd81-4c99-c3f0-85225f40918c","id":"KVqRq0J9yuKb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["12161 terms related to programming and technical constructs\n","10636 specific dates or references to time\n","10254 references to days of the week and their associated schedules\n","1039 technical terms and elements within programming and coding contexts\n","15763  sequences of numbers and their occurrences within the document\n"]}]},{"cell_type":"code","source":["sae_acts_2[0, -1, 12161]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"miPa9NFRyxwT","executionInfo":{"status":"ok","timestamp":1724847206333,"user_tz":-60,"elapsed":354,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2d481244-45a6-49c9-e34d-6c569ac9e058"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(11.3316, device='cuda:0')"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":["# ablate a feature (idx = 12161) by setting it to 0\n","sae_acts_2[0, -1, 12161] = 0\n","sae_acts_2[0, -1, 12161]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zfj7Y1WMqgDp","executionInfo":{"status":"ok","timestamp":1724847212347,"user_tz":-60,"elapsed":370,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"19dc9286-6a77-44c8-a3f7-a137f80a494b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(0., device='cuda:0')"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["recon = sae.decode(sae_acts_2)\n","recon.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sVxmmBnUzHAS","executionInfo":{"status":"ok","timestamp":1724847214286,"user_tz":-60,"elapsed":249,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"dee5e10d-6a4c-4d24-f962-e8a8d38d8535"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 2304])"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )"],"metadata":{"id":"QHVApqmK5NsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"t2wuSfKR6OJq","executionInfo":{"status":"ok","timestamp":1724847286531,"user_tz":-60,"elapsed":277,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"8d010f52-4ea7-4282-b6b9-c16de29436aa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' May'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","source":["## patch all month only features"],"metadata":{"id":"ir-IFX5H6TcH"}},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"vSIvhDPx6Yok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in list(months_only):\n","    sae_acts_2[0, -1, f_ind] = 0\n","recon = sae.decode(sae_acts_2)\n","recon.shape"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847368597,"user_tz":-60,"elapsed":237,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"013d1ed2-d49a-4380-f2cb-0970d6528deb","id":"2UQqGT6R6Yos"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 2304])"]},"metadata":{},"execution_count":76}]},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847384551,"user_tz":-60,"elapsed":329,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"voSl2yCq6kqr","outputId":"1e4eeb12-aec8-499c-bde8-e5cbe05759ee"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":77}]},{"cell_type":"markdown","source":["## patch half month only features\n","\n","\n"],"metadata":{"id":"zIQuvEXC6pjq"}},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"bAayE84x6pjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in list(months_only)[:25]:\n","    sae_acts_2[0, -1, f_ind] = 0\n","recon = sae.decode(sae_acts_2)\n","recon.shape"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847471540,"user_tz":-60,"elapsed":266,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"68080563-98e8-409e-f1c9-747dccf84278","id":"ax2pnheZ6pjw"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 2304])"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724847476048,"user_tz":-60,"elapsed":316,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b2f1ece2-d99d-4d9c-f495-c6d5d41adce5","id":"zZkRidv36pjx"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":83}]},{"cell_type":"markdown","source":["## gen more than next token"],"metadata":{"id":"VosRDkTn9HzJ"}},{"cell_type":"code","source":["prompt = \"January February March April\"\n","clean_text = prompt\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","for i in range(3):\n","    if next_char == '':\n","        next_char = ' '\n","\n","    clean_text = clean_text + next_char\n","\n","    # tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","    tokens = model_2.to_tokens(clean_text).to(device)\n","\n","    #########\n","    inputs = tokenizer.encode(clean_text, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","    target_act = gather_residual_activations(model, 20, inputs)\n","    sae_acts_2 = sae.encode(target_act.to(torch.float32))\n","\n","    for f_ind in list(months_only)[:25]:\n","        sae_acts_2[0, -1, f_ind] = 0\n","    recon = sae.decode(sae_acts_2)\n","\n","    #########\n","    def patch_layer(\n","        orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","        hook: HookPoint,\n","        LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","        layer_to_patch: int,\n","    ) -> Float[Tensor, \"batch pos d_model\"]:\n","        if layer_to_patch == hook.layer():\n","            orig_actvs[:, :, :] = LLM_patch\n","        return orig_actvs\n","\n","    hook_fn = partial(\n","            patch_layer,\n","            LLM_patch= recon,\n","            layer_to_patch = 20\n","        )\n","\n","    #########\n","    ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","    next_token = ablated_logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","    # next_char = model.to_string(next_token)\n","\n","print(model_2.to_string(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6-5J-Jhw7TjX","executionInfo":{"status":"ok","timestamp":1724848034235,"user_tz":-60,"elapsed":676,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3652a611-f201-452a-f295-a1f04e1a07cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<bos>January February March April\\n\\n\\n']\n"]}]},{"cell_type":"markdown","source":["## patch top features\n","\n","\n"],"metadata":{"id":"dBUHbxud9mws"}},{"cell_type":"markdown","source":["around 23 to destroy"],"metadata":{"id":"XncFbAeT-JbK"}},{"cell_type":"code","source":["# for val, ind in zip(two_top_acts_values, two_top_acts_indices):\n","#     print(round(val.item(), 2), ind.item(), feat_labels_lst[ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724848326973,"user_tz":-60,"elapsed":236,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b05b0958-b914-49ef-c0b4-fcf9be30c15b","id":"ipdIwSrJ-LS3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["65.1 15451  structured data or attributes in a coded format\n","41.55 5710 references to seasonal festivals and their associated cultural significance\n","38.81 13767 terms related to approval and assessment in a regulatory or medical context\n","34.97 11546 dates and months\n","33.7 6240  words related to time indicators, specifically months or dates\n","33.65 9768 terms related to control and authority, particularly in political or systemic contexts\n","32.96 3109  technical terms and symbols related to data structures\n","28.4 2269  structured lists or numbered steps in procedural texts\n","27.97 7159  references to time frames and chronological events\n","27.64 3162  markers indicating the beginning of a document\n","24.91 609 terms related to research articles and their attributes, specifically focusing on methodologies and data contributions\n","24.85 12748  structured data representations and their attributes\n","23.26 5968 references to specific dates, particularly in October and April\n","22.92 522  numerical data related to gene activity or expression\n","21.45 1530 technical terms related to inventions and scientific descriptions\n","21.06 1094 special characters and mathematical symbols\n","19.84 10636 specific dates or references to time\n","16.79 14017 conjunctions and relationships between elements, particularly in logical or mathematical contexts\n","15.79 11004 categories and statistics related to demographic groups\n","14.89 15194 specific terms related to legal and governmental contexts\n","14.44 12658 mathematical terminology and symbols, particularly in equations and expressions\n","14.34 8684  technical jargon and programming-related terms\n","14.21 5052 the beginning of a document or section, likely signaling the start of significant content\n","14.2 15763  sequences of numbers and their occurrences within the document\n","13.8 1692 legal and technical terminology related to statutes and inventions\n","13.55 10103 dates, specifically in the month of July\n","12.89 11210  expressions related to logical comparisons\n","12.7 1046  dates and events related to the calendar\n","12.58 11938 specific medical histories and conditions related to participant eligibility in clinical trials\n","11.66 5405  numerical data and statistics\n","11.4 6082  segments related to instructions or guidelines\n","11.33 12161 terms related to programming and technical constructs\n","10.87 12958 words and themes associated with seasonal transitions and celebrations\n","10.86 4295 connections and relationships among elements in technical or procedural contexts\n","10.71 10254 references to days of the week and their associated schedules\n","10.57 3032 numerical data or references to values and measurements\n","10.19 13817 dates and times\n","10.06 8640  numerical data and references to scientific publications\n","9.62 3019  elements related to operational or procedural contexts in a structured format\n","9.54 14249 links to privacy and cookie policies\n","9.41 14388 terms and phrases related to technology, particularly in the context of apps, security, and online privacy\n","9.36 13254  terms related to experimental control and intervention settings in research contexts\n","9.15 14782  references to data structures and types in programming\n","9.13 1233 scientific terminology related to statistics and experimental methods\n","9.06 8820 references to ethical concerns and issues related to accountability\n","8.83 10174  patterns or codes related to data structures or object representations\n","8.57 10997  references to specific health conditions or treatments\n","8.5 9203 the start of sections or paragraphs in the text\n","8.47 1039 technical terms and elements within programming and coding contexts\n","8.31 6612 instances of numbers and counting, indicating calculations or measurements\n"]}]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"-MfYyFx79mw1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokens = model_2.to_tokens(prompt).to(device)"],"metadata":{"id":"nJx1pUTk9r1C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_months_only_f = [feat for feat in list(months_only) if feat in two_top_acts_indices[:9] ]"],"metadata":{"id":"U6VO-qv0-SdN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["two_top_acts_indices[:9]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JLY755F9CJGE","executionInfo":{"status":"ok","timestamp":1724849363010,"user_tz":-60,"elapsed":206,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2fb8536c-8170-4063-b10e-0154eed4cdae"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([15451,  5710, 13767, 11546,  6240,  9768,  3109,  2269,  7159],\n","       device='cuda:0')"]},"metadata":{},"execution_count":179}]},{"cell_type":"code","source":["len(months_only)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rObdYLuV98JC","executionInfo":{"status":"ok","timestamp":1724848269693,"user_tz":-60,"elapsed":346,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bacc6118-4f07-4bbe-e357-91a1cf163eb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["38"]},"metadata":{},"execution_count":114}]},{"cell_type":"code","source":["# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only)[:23]:\n","for f_ind in top_months_only_f:\n","    sae_acts_2[0, -1, f_ind] = 0\n","recon = sae.decode(sae_acts_2)\n","recon.shape"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724849518501,"user_tz":-60,"elapsed":264,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78556208-f186-468a-dcc0-0be89d8d574f","id":"bcVooX9-9mw2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 2304])"]},"metadata":{},"execution_count":189}]},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724849520318,"user_tz":-60,"elapsed":473,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b8d52242-a1ee-42a7-999f-645a6a9bdac0","id":"HvuvS_NQ9mw2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":190}]},{"cell_type":"code","source":["ablated_logits[0, -1].argmax(dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724849522701,"user_tz":-60,"elapsed":230,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f82cefed-bca9-4eb2-a565-b7dab5b7cb37","id":"m2zdURKOCsxc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(108, device='cuda:0')"]},"metadata":{},"execution_count":191}]},{"cell_type":"code","source":["ablated_logits[0, -1, 2782]"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724849523786,"user_tz":-60,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d9ee4e76-12cb-482b-84ba-709e97c9b819","colab":{"base_uri":"https://localhost:8080/"},"id":"0CC2pNOVCsxi"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(14.3767, device='cuda:0')"]},"metadata":{},"execution_count":192}]},{"cell_type":"code","source":["ablated_logits[0, -1, 108]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jnb6OZfgCx8N","executionInfo":{"status":"ok","timestamp":1724849533259,"user_tz":-60,"elapsed":388,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ecda6a56-0ba4-472b-b039-ba3e87ee694d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(15.3885, device='cuda:0')"]},"metadata":{},"execution_count":193}]},{"cell_type":"code","source":["sae_acts_2 = sae.encode(target_act.to(torch.float32))\n","# top_months_only_f = [feat for feat in list(months_only) if feat in two_top_acts_indices[:9] ]\n","# for f_ind in top_months_only_f:\n","sae_acts_2[0, -1, 7159] = 0\n","recon = sae.decode(sae_acts_2)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"executionInfo":{"status":"ok","timestamp":1724849447719,"user_tz":-60,"elapsed":253,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"2b55a76e-511a-471b-d848-6dc5bc5b3cb5","id":"3sILesHDCN47"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' May'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":182}]},{"cell_type":"code","source":["ablated_logits[0, -1].argmax(dim=-1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LbY66jirCh3X","executionInfo":{"status":"ok","timestamp":1724849481171,"user_tz":-60,"elapsed":326,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"d5f1a4ad-216a-40b0-fd52-24960d3ef563"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(2782, device='cuda:0')"]},"metadata":{},"execution_count":184}]},{"cell_type":"code","source":["ablated_logits[0, -1, 2782]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IvUjMwhTCn5f","executionInfo":{"status":"ok","timestamp":1724849497230,"user_tz":-60,"elapsed":357,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9d69ed3f-a1c1-4b93-dd39-761b2fc8b85f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(17.3316, device='cuda:0')"]},"metadata":{},"execution_count":185}]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","clean_text = prompt\n","tokens = model_2.to_tokens(prompt).to(device)\n","next_char = 0\n","\n","top_months_only_f = [feat for feat in list(months_only) if feat in two_top_acts_indices[:1] ]\n","\n","for i in range(10):\n","\n","    #########\n","    inputs = tokenizer.encode(clean_text, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","    target_act = gather_residual_activations(model, 20, inputs)\n","    sae_acts_2 = sae.encode(target_act.to(torch.float32))\n","\n","    # for f_ind in top_months_only_f:\n","    #     sae_acts_2[0, -1, f_ind] = 0\n","    recon = sae.decode(sae_acts_2)\n","\n","    #########\n","    def patch_layer(\n","        orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","        hook: HookPoint,\n","        LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","        layer_to_patch: int,\n","    ) -> Float[Tensor, \"batch pos d_model\"]:\n","        if layer_to_patch == hook.layer():\n","            orig_actvs[:, :, :] = LLM_patch\n","        return orig_actvs\n","\n","    hook_fn = partial(\n","            patch_layer,\n","            LLM_patch= recon,\n","            layer_to_patch = 20\n","        )\n","\n","    #########\n","    ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","    next_token = ablated_logits[0, -1].argmax(dim=-1) # Get the predicted token at the end of our sequence\n","    next_char = model_2.to_string(next_token)\n","    print(\"Next char: \", next_char)\n","    print(model_2.to_string(tokens))\n","\n","    if next_char == '':\n","        next_char = ' '\n","\n","    clean_text = clean_text + next_char\n","\n","    print(clean_text)\n","\n","    # tokens = torch.cat([tokens, next_token[None, None]], dim=-1)\n","    tokens = model_2.to_tokens(clean_text).to(device)\n","\n","print(model_2.to_string(tokens))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1724849081271,"user_tz":-60,"elapsed":1960,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c8e6d7a2-aacd-4b85-c1e3-39e3361e77bb","id":"zr_pBSWq-74_"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Next char:   May\n","['<bos>January February March April']\n","January February March April May\n","Next char:  \n","\n","['<bos>January February March April May']\n","January February March April May\n","\n","Next char:   \n","['<bos>January February March April May\\n']\n","January February March April May\n"," \n","Next char:  \n","\n","['<bos>January February March April May\\n ']\n","January February March April May\n"," \n","\n","Next char:   \n","['<bos>January February March April May\\n \\n']\n","January February March April May\n"," \n"," \n","Next char:  \n","\n","['<bos>January February March April May\\n \\n ']\n","January February March April May\n"," \n"," \n","\n","Next char:   \n","['<bos>January February March April May\\n \\n \\n']\n","January February March April May\n"," \n"," \n"," \n","Next char:  \n","\n","['<bos>January February March April May\\n \\n \\n ']\n","January February March April May\n"," \n"," \n"," \n","\n","Next char:   \n","['<bos>January February March April May\\n \\n \\n \\n']\n","January February March April May\n"," \n"," \n"," \n"," \n","Next char:  \n","\n","['<bos>January February March April May\\n \\n \\n \\n ']\n","January February March April May\n"," \n"," \n"," \n"," \n","\n","['<bos>January February March April May\\n \\n \\n \\n \\n']\n"]}]},{"cell_type":"markdown","source":["# steer nums to months"],"metadata":{"id":"Qple37K-DDJ2"}},{"cell_type":"code","source":["prompt = \"one two three four\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_1 = sae.encode(target_act.to(torch.float32))\n","recon = sae.decode(sae_acts_1)\n","\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"tUahQcb_EHSO","executionInfo":{"status":"ok","timestamp":1724849919178,"user_tz":-60,"elapsed":420,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b5cb995e-8f0c-4844-b93c-42ca9ef4ab18"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' five'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":199}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_1 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"O-GknpvtDY2y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"January February March April\"\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_2 = sae.encode(target_act.to(torch.float32))"],"metadata":{"id":"As-XHEP2Dpvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","for f_ind in list(months_only):\n","    sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","recon = sae.decode(sae_acts_1)\n","recon.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P_0ixDjwDaBN","executionInfo":{"status":"ok","timestamp":1724849764927,"user_tz":-60,"elapsed":238,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"90da380e-fec7-4533-ff55-3767131f0f8e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 5, 2304])"]},"metadata":{},"execution_count":196}]},{"cell_type":"code","source":["for f_ind in list(months_only):\n","    print(sae_acts_1[0, -1, f_ind])\n","    print(sae_acts_1[0, -1, f_ind] - sae_acts_2[0, -1, f_ind])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJO2-UbtEVrP","executionInfo":{"status":"ok","timestamp":1724849970215,"user_tz":-60,"elapsed":218,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b207c820-9574-477d-f21c-26124d9b9645"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7.8716, device='cuda:0')\n","tensor(-3.4600, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-19.8431, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-10.7117, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-8.4733, device='cuda:0')\n","tensor(8.8761, device='cuda:0')\n","tensor(-5.3283, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-12.6979, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-34.9663, device='cuda:0')\n","tensor(8.5766, device='cuda:0')\n","tensor(-5.2230, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-10.8700, device='cuda:0')\n","tensor(7.1779, device='cuda:0')\n","tensor(-5.3980, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-9.5370, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-27.9718, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-9.4093, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-14.2105, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-9.1506, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-8.8343, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-16.7889, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-11.4039, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-9.3615, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-38.8069, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-10.8596, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-12.8891, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-24.8540, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-41.5456, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-23.2618, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-9.1318, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-8.3146, device='cuda:0')\n","tensor(7.5703, device='cuda:0')\n","tensor(-2.9991, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-27.6400, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-14.8893, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-33.6973, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-14.3373, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-8.5014, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-8.5739, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-13.5548, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-10.1922, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-21.4519, device='cuda:0')\n","tensor(0., device='cuda:0')\n","tensor(-15.7933, device='cuda:0')\n"]}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","recon = sae.decode(sae_acts_2)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"1QdXnRCpDsGP","executionInfo":{"status":"ok","timestamp":1724850079915,"user_tz":-60,"elapsed":236,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f19b952f-6c34-4b76-e583-a3f7aee4da05"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' May'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":204}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","recon = sae.decode(sae_acts_1 - sae_acts_2)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"GYai7fVNE6Q-","executionInfo":{"status":"ok","timestamp":1724850097677,"user_tz":-60,"elapsed":455,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"47f1a9c4-9a9e-4ddb-edba-c063a83e6701"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["','"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":205}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","recon = sae.decode(sae_acts_2 - sae_acts_1)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"CVE3HEKQE9WV","executionInfo":{"status":"ok","timestamp":1724850106890,"user_tz":-60,"elapsed":231,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3f2383cd-c461-4a5d-db68-926aa9d8e2be"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":206}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","recon = sae.decode(3*sae_acts_2 - sae_acts_1)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"q75KQ5NmFKMh","executionInfo":{"status":"ok","timestamp":1724850161570,"user_tz":-60,"elapsed":428,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"4198c8a6-2cb8-4ee1-c549-62d30053a907"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' May'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":207}]},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","recon = sae.decode(10*sae_acts_1 - sae_acts_2)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ODwBxnRbFN8R","executionInfo":{"status":"ok","timestamp":1724850183508,"user_tz":-60,"elapsed":447,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2238e1cc-f098-4ea5-8cc5-f1d5fc5ce190"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":209}]},{"cell_type":"markdown","source":["## steering vec"],"metadata":{"id":"pjCWyi0PFn7P"}},{"cell_type":"code","source":["prompt = \"three four five six\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3)\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"2SOk9sdfFpMw","executionInfo":{"status":"ok","timestamp":1724850285803,"user_tz":-60,"elapsed":337,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"fb8b0e46-3133-4c31-9248-f01ec8693144"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' seven'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":211}]},{"cell_type":"code","source":["prompt = \"three four five six\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3 + 3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"EABPfVwZFUHP","executionInfo":{"status":"ok","timestamp":1724850301506,"user_tz":-60,"elapsed":427,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"50fa7af5-fb78-4752-829b-349cf104fc85"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":213}]},{"cell_type":"code","source":["prompt = \"two three four five\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3 + 3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"RxC-D69uF4hf","executionInfo":{"status":"ok","timestamp":1724850409903,"user_tz":-60,"elapsed":397,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"65d72524-202e-4d80-b709-a737f6118c0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' June'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":222}]},{"cell_type":"code","source":["prompt = \"four five six seven\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3 + 3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Sjr88jeRGKjC","executionInfo":{"status":"ok","timestamp":1724850426411,"user_tz":-60,"elapsed":253,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2050c8cc-b74e-4d68-bf7e-e2cd66365695"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":223}]},{"cell_type":"code","source":["prompt = \"one two three one\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3 + 3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"FKxZSAawGQI_","executionInfo":{"status":"ok","timestamp":1724850460594,"user_tz":-60,"elapsed":354,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"9c6b8581-86ac-4f58-dec3-27e9f1c908ce"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":225}]},{"cell_type":"code","source":["prompt = \"a a a a\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(sae_acts_3 + 3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"P2OT5HDqGWxw","executionInfo":{"status":"ok","timestamp":1724850478230,"user_tz":-60,"elapsed":716,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"28ef0aca-5f5e-4335-b4d7-e6d961a4c6b4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":226}]},{"cell_type":"code","source":[],"metadata":{"id":"GvZ3jbzwGjo9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["prompt = \"a a a a\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# months_only = set((two_top_acts_indices).tolist()) - set((one_top_acts_indices).tolist())\n","# for f_ind in list(months_only):\n","# for f_ind in range(sae_acts_2.shape[-1]):\n","#     sae_acts_1[0, -1, f_ind] = sae_acts_2[0, -1, f_ind]\n","# recon = sae.decode(sae_acts_1)\n","\n","inputs = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=True).to(\"cuda\")\n","target_act = gather_residual_activations(model, 20, inputs)\n","sae_acts_3 = sae.encode(target_act.to(torch.float32))\n","\n","recon = sae.decode(3*(sae_acts_2 - sae_acts_1))\n","recon.shape\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"id":"2QGufsIpGjui","executionInfo":{"status":"ok","timestamp":1724850525306,"user_tz":-60,"elapsed":294,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f3a6d3a0-9559-4f38-c3d7-ff58c1f01cb5","colab":{"base_uri":"https://localhost:8080/","height":35}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' April'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":227}]},{"cell_type":"markdown","source":["## replace sae_1 with sae_2"],"metadata":{"id":"9QJGoe5EElDG"}},{"cell_type":"code","source":["prompt = \"one two three four\"\n","tokens = model_2.to_tokens(prompt).to(device)\n","\n","# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","from jaxtyping import Float, Int\n","\n","layer_name = 'blocks.20.hook_resid_post'\n","\n","def patch_layer(\n","    orig_actvs: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_actvs[:, :, :] = LLM_patch\n","    return orig_actvs\n","\n","hook_fn = partial(\n","        patch_layer,\n","        LLM_patch= recon,\n","        layer_to_patch = 20\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","# no need to reset hoooks after since run_with_hooks isn't permanent like add_hook with perm arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model_2.run_with_hooks(tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )\n","\n","next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model_2.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1724849851886,"user_tz":-60,"elapsed":232,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"73eddb53-9855-4a60-e8d6-bc0f62f4a669","id":"m4aeLhSlEnOq"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":197}]},{"cell_type":"markdown","source":["# retain only subset of features for each domain"],"metadata":{"id":"oixTYiMfcVuE"}},{"cell_type":"code","source":["feat_k = 100\n","\n","_, topInds = sae_acts_1[0, -1, :].topk(feat_k, dim=-1)\n","eng_num_feats = (topInds).tolist()\n","\n","_, topInds = sae_acts_2[0, -1, :].topk(feat_k, dim=-1)\n","spa_num_feats = (topInds).tolist()"],"metadata":{"id":"m--BUSrGcYhP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["weight_matrix[eng_num_feats].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L8WE9eWUeupe","executionInfo":{"status":"ok","timestamp":1724791874755,"user_tz":-60,"elapsed":10,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"6b64e6df-3db0-4b5c-b0f3-97aa362eacf7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(100, 2304)"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["eng_num_feats.sort()"],"metadata":{"id":"Y-_m618znWO6"},"execution_count":null,"outputs":[]}]}