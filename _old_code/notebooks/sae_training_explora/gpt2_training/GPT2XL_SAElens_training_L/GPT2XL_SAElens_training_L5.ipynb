{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","collapsed_sections":["shAFb9-lOVHu","oe2nlqf-OVHv"],"authorship_tag":"ABX9TyMRb3JywO4lAT2waCIciuQN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0e5cf77cc041424da6b798f1dd4b9e94":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_38f44cc374924298922ca17d08e8848c","IPY_MODEL_c1e055b126e34e36b0daa959f0216e84","IPY_MODEL_72aa246caf46408a8dbeace5ec91079f"],"layout":"IPY_MODEL_f322564a32ca414bbdd5a7b1ff54944c"}},"38f44cc374924298922ca17d08e8848c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7b62c22b2a346819ff6b3471eaa77da","placeholder":"​","style":"IPY_MODEL_5b6983725337479fbc47fbc1b56c4496","value":"config.json: 100%"}},"c1e055b126e34e36b0daa959f0216e84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_07548b3580204787a01138d9ba8d15cb","max":689,"min":0,"orientation":"horizontal","style":"IPY_MODEL_837c8f33d1114a49b8d6ba7432815cc4","value":689}},"72aa246caf46408a8dbeace5ec91079f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71078575ddbe4be498295fbbe64414fa","placeholder":"​","style":"IPY_MODEL_cc62a48085cf4943adc10838deca54ba","value":" 689/689 [00:00&lt;00:00, 60.9kB/s]"}},"f322564a32ca414bbdd5a7b1ff54944c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c7b62c22b2a346819ff6b3471eaa77da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b6983725337479fbc47fbc1b56c4496":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07548b3580204787a01138d9ba8d15cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"837c8f33d1114a49b8d6ba7432815cc4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71078575ddbe4be498295fbbe64414fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cc62a48085cf4943adc10838deca54ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2e077bf71f354724965ab024751beb35":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_45af8fe0378049d7860c06e99cf16f15","IPY_MODEL_741afdffc32f4b33835a1f7cd774eca0","IPY_MODEL_d20a7a74590549ddbada87bf24bfd94b"],"layout":"IPY_MODEL_c00b9c57865b4a5b949340ae98cd9c12"}},"45af8fe0378049d7860c06e99cf16f15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe48467ecb594115a03e279ad1f3507c","placeholder":"​","style":"IPY_MODEL_d66e9b96639542af997571e56aae3832","value":"model.safetensors: 100%"}},"741afdffc32f4b33835a1f7cd774eca0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8db4394996f04a788c7df58c9edb5bae","max":6431829964,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ebcb98e472ae4e4da46584570cc66d11","value":6431829964}},"d20a7a74590549ddbada87bf24bfd94b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_122664b9635c4c828052e83cf7f05a50","placeholder":"​","style":"IPY_MODEL_143abc8a924e494b881628f8e03b789d","value":" 6.43G/6.43G [00:18&lt;00:00, 423MB/s]"}},"c00b9c57865b4a5b949340ae98cd9c12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe48467ecb594115a03e279ad1f3507c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d66e9b96639542af997571e56aae3832":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8db4394996f04a788c7df58c9edb5bae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ebcb98e472ae4e4da46584570cc66d11":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"122664b9635c4c828052e83cf7f05a50":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"143abc8a924e494b881628f8e03b789d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0bb98bd50f1b4c599650e7d457061920":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8111cc34fcc14d2f95760782518a4b1f","IPY_MODEL_2ad7774bca894bc994011899b0f2e650","IPY_MODEL_c51ccdba7e594db0b4a8776384e2082e"],"layout":"IPY_MODEL_f141c408e04949c1ab6dbfdfa913e4ed"}},"8111cc34fcc14d2f95760782518a4b1f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a4496b8ab7f4c3cbc19cee290c6923f","placeholder":"​","style":"IPY_MODEL_9cf3f4de83094fb0ac05d966e7b8af8e","value":"generation_config.json: 100%"}},"2ad7774bca894bc994011899b0f2e650":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7197c6d8ee04b90b412eb901b2fea80","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_80b878f6f8354105982b8420f236d4e0","value":124}},"c51ccdba7e594db0b4a8776384e2082e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d1ace4837dd47418461c7d80075c922","placeholder":"​","style":"IPY_MODEL_ef9f8708dea4468cb2bda89be1f521ed","value":" 124/124 [00:00&lt;00:00, 9.55kB/s]"}},"f141c408e04949c1ab6dbfdfa913e4ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a4496b8ab7f4c3cbc19cee290c6923f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9cf3f4de83094fb0ac05d966e7b8af8e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7197c6d8ee04b90b412eb901b2fea80":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"80b878f6f8354105982b8420f236d4e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3d1ace4837dd47418461c7d80075c922":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef9f8708dea4468cb2bda89be1f521ed":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acc1393a1fb04cc4b8c91ce97569912d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1c5f461c45ce41fab10d54c5440e217f","IPY_MODEL_fefc1d62640640fbb3a2cc56c8c11cab","IPY_MODEL_067a41a4bc194864b6c15d38de7b9972"],"layout":"IPY_MODEL_4e87adb8560e4c76aa756d41f5b82dfd"}},"1c5f461c45ce41fab10d54c5440e217f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c160205ac34b4986a491304b1f556668","placeholder":"​","style":"IPY_MODEL_8dae719728a64fbfb1f5042eaa206e23","value":"tokenizer_config.json: 100%"}},"fefc1d62640640fbb3a2cc56c8c11cab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_469553f75d9f426ea73de111262b2f59","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_77f74526125f4c4e8b39e318fa580678","value":26}},"067a41a4bc194864b6c15d38de7b9972":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75ae4630cbc74401bc3f4e38f56b9149","placeholder":"​","style":"IPY_MODEL_a595faffd31e44a8a9f3c10d3e091df9","value":" 26.0/26.0 [00:00&lt;00:00, 2.21kB/s]"}},"4e87adb8560e4c76aa756d41f5b82dfd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c160205ac34b4986a491304b1f556668":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8dae719728a64fbfb1f5042eaa206e23":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"469553f75d9f426ea73de111262b2f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77f74526125f4c4e8b39e318fa580678":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"75ae4630cbc74401bc3f4e38f56b9149":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a595faffd31e44a8a9f3c10d3e091df9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9c77c42c46da4e39b87db6804f534ba1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e6020d98c64749be967c39ed830bdccf","IPY_MODEL_f520a6267744473eb4dd2c332b6d9b66","IPY_MODEL_dc9361c47c8646a8a6015f6ffe220f56"],"layout":"IPY_MODEL_66e85ceb2f1d42ffb33f0a172dcc5d13"}},"e6020d98c64749be967c39ed830bdccf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_70906709f0b946ac92a2a801a0aa3510","placeholder":"​","style":"IPY_MODEL_87bc9333d027454189ab7e5e09a2a587","value":"vocab.json: 100%"}},"f520a6267744473eb4dd2c332b6d9b66":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_539111bf9a33420fab45e0237b0b9323","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1c4fa1341cda4f59ae7a7719a5f84eca","value":1042301}},"dc9361c47c8646a8a6015f6ffe220f56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_646c371e686041ceac9a3eae3297ddc1","placeholder":"​","style":"IPY_MODEL_7d10579d0aa64c56bf64462faadf2973","value":" 1.04M/1.04M [00:00&lt;00:00, 35.4MB/s]"}},"66e85ceb2f1d42ffb33f0a172dcc5d13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70906709f0b946ac92a2a801a0aa3510":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87bc9333d027454189ab7e5e09a2a587":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"539111bf9a33420fab45e0237b0b9323":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1c4fa1341cda4f59ae7a7719a5f84eca":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"646c371e686041ceac9a3eae3297ddc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7d10579d0aa64c56bf64462faadf2973":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"32d39934bb884208ae622d1ac573014c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_060e70488f9c43f69add4401238ace89","IPY_MODEL_b87cc450e8c64477bc39fae1c32e8a89","IPY_MODEL_c789e0901ce749a1a90a2424f50c076b"],"layout":"IPY_MODEL_523309ee674a4021947febd17f1075e0"}},"060e70488f9c43f69add4401238ace89":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fe649d03a7754c8b8ec0dc91b4129407","placeholder":"​","style":"IPY_MODEL_20ba3a106d5f44dd973a9a5f6d56c583","value":"merges.txt: 100%"}},"b87cc450e8c64477bc39fae1c32e8a89":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_24eb7387da7f414c8e92d809628f0c5b","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8a125da509bf4be2bc3c439660bfb53e","value":456318}},"c789e0901ce749a1a90a2424f50c076b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9837523832ef4b55af31c647a15fb56c","placeholder":"​","style":"IPY_MODEL_d288a37f26dd45fc9f17399e72452bdb","value":" 456k/456k [00:00&lt;00:00, 1.93MB/s]"}},"523309ee674a4021947febd17f1075e0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe649d03a7754c8b8ec0dc91b4129407":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20ba3a106d5f44dd973a9a5f6d56c583":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"24eb7387da7f414c8e92d809628f0c5b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8a125da509bf4be2bc3c439660bfb53e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9837523832ef4b55af31c647a15fb56c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d288a37f26dd45fc9f17399e72452bdb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"11ebe95ccd1349babe23c83a81846014":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1cde02b55dd343ecb3eedce18ab312c2","IPY_MODEL_f84813dd79824c5ea92b90d983e1e248","IPY_MODEL_fd289a3177534efab7308f7090e4c628"],"layout":"IPY_MODEL_fed034892e1c405883a54328e3629123"}},"1cde02b55dd343ecb3eedce18ab312c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1e4b26162cdb4a1cb5dac2b5d61b7e4a","placeholder":"​","style":"IPY_MODEL_3c2ac14be9fa4748af4bea278fe9ac6d","value":"tokenizer.json: 100%"}},"f84813dd79824c5ea92b90d983e1e248":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d3abe31639f4a298ed88ae5e526c6fa","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_83290d56c4494e0c94983d9cae4070f6","value":1355256}},"fd289a3177534efab7308f7090e4c628":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6872c6333a2c430aae54e10f4af0354f","placeholder":"​","style":"IPY_MODEL_47f9b66ebbcd405082ae6ce1773a1701","value":" 1.36M/1.36M [00:00&lt;00:00, 42.3MB/s]"}},"fed034892e1c405883a54328e3629123":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e4b26162cdb4a1cb5dac2b5d61b7e4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c2ac14be9fa4748af4bea278fe9ac6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d3abe31639f4a298ed88ae5e526c6fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"83290d56c4494e0c94983d9cae4070f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6872c6333a2c430aae54e10f4af0354f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47f9b66ebbcd405082ae6ce1773a1701":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c96f238d4c541feaa1a1b4e43782381":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a9cb3767aa2a4ad48aa34f938dab6dd3","IPY_MODEL_0c43112bde344226a35227bfbafb6187","IPY_MODEL_8b67eca996de4647bb634f93d1150f2b"],"layout":"IPY_MODEL_7f2e6bb00ea14f9ea294838c71de2472"}},"a9cb3767aa2a4ad48aa34f938dab6dd3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c23a78d4229e4566b0e97d9c5d2a9d9a","placeholder":"​","style":"IPY_MODEL_dc3ccfad233247a38da289022d79fc36","value":"100%"}},"0c43112bde344226a35227bfbafb6187":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_90a2d1699e9e4bd19d17a756dca0d6ea","max":50,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0c6f0ada39d430fb9cf4dadb070abc6","value":50}},"8b67eca996de4647bb634f93d1150f2b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_03ccc94c24c54c8ba6d383600e97f96a","placeholder":"​","style":"IPY_MODEL_93f2169c7b4942cebfb4ee6b5c93ed9c","value":" 50/50 [00:05&lt;00:00,  9.86it/s]"}},"7f2e6bb00ea14f9ea294838c71de2472":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23a78d4229e4566b0e97d9c5d2a9d9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dc3ccfad233247a38da289022d79fc36":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90a2d1699e9e4bd19d17a756dca0d6ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0c6f0ada39d430fb9cf4dadb070abc6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"03ccc94c24c54c8ba6d383600e97f96a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93f2169c7b4942cebfb4ee6b5c93ed9c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bf50c3760f084f118d5fcd3dd80afee1":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_69124339cb954064806f38e26b59f662","IPY_MODEL_40c13ac7d323443d9bae241f8d44a0fa"],"layout":"IPY_MODEL_e8d1fad89c5f4ee0b5851a57006e59ad"}},"69124339cb954064806f38e26b59f662":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_245bd1eba4bf4848a41fa7e0fef91628","placeholder":"​","style":"IPY_MODEL_7e7c366b76534965a2094cab426359ae","value":"312.819 MB of 312.819 MB uploaded\r"}},"40c13ac7d323443d9bae241f8d44a0fa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_551c46f2c4ed468696db157a1d167b9c","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fb5a9ff6061a45a1bb3597b8c3a718b6","value":1}},"e8d1fad89c5f4ee0b5851a57006e59ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"245bd1eba4bf4848a41fa7e0fef91628":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e7c366b76534965a2094cab426359ae":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"551c46f2c4ed468696db157a1d167b9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fb5a9ff6061a45a1bb3597b8c3a718b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"shAFb9-lOVHu"},"source":["## Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"LeRi_tw2dhae","executionInfo":{"status":"ok","timestamp":1715860959209,"user_tz":240,"elapsed":239574,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"outputs":[],"source":["%%capture\n","try:\n","    import google.colab # type: ignore\n","    from google.colab import output\n","    %pip install sae-lens==1.3.0 transformer-lens==1.17.0 circuitsvis==1.43.2\n","except:\n","    from IPython import get_ipython # type: ignore\n","    ipython = get_ipython(); assert ipython is not None\n","    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n","    ipython.run_line_magic(\"autoreload\", \"2\")"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"uy-b3CcSOVHu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715860964283,"user_tz":240,"elapsed":5080,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e395c5ab-ee9f-4f0e-82bd-57ac0dbe1983"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n"]}],"source":["import torch\n","import os\n","\n","from sae_lens.training.config import LanguageModelSAERunnerConfig\n","from sae_lens.training.lm_runner import language_model_sae_runner\n","\n","if torch.cuda.is_available():\n","    device = \"cuda\"\n","elif torch.backends.mps.is_available():\n","    device = \"mps\"\n","else:\n","    device = \"cpu\"\n","\n","print(\"Using device:\", device)\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""]},{"cell_type":"markdown","metadata":{"id":"oe2nlqf-OVHv"},"source":["# Model Selection and Evaluation\n"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"hFz6JUMuOVHv","colab":{"base_uri":"https://localhost:8080/","height":0,"referenced_widgets":["0e5cf77cc041424da6b798f1dd4b9e94","38f44cc374924298922ca17d08e8848c","c1e055b126e34e36b0daa959f0216e84","72aa246caf46408a8dbeace5ec91079f","f322564a32ca414bbdd5a7b1ff54944c","c7b62c22b2a346819ff6b3471eaa77da","5b6983725337479fbc47fbc1b56c4496","07548b3580204787a01138d9ba8d15cb","837c8f33d1114a49b8d6ba7432815cc4","71078575ddbe4be498295fbbe64414fa","cc62a48085cf4943adc10838deca54ba","2e077bf71f354724965ab024751beb35","45af8fe0378049d7860c06e99cf16f15","741afdffc32f4b33835a1f7cd774eca0","d20a7a74590549ddbada87bf24bfd94b","c00b9c57865b4a5b949340ae98cd9c12","fe48467ecb594115a03e279ad1f3507c","d66e9b96639542af997571e56aae3832","8db4394996f04a788c7df58c9edb5bae","ebcb98e472ae4e4da46584570cc66d11","122664b9635c4c828052e83cf7f05a50","143abc8a924e494b881628f8e03b789d","0bb98bd50f1b4c599650e7d457061920","8111cc34fcc14d2f95760782518a4b1f","2ad7774bca894bc994011899b0f2e650","c51ccdba7e594db0b4a8776384e2082e","f141c408e04949c1ab6dbfdfa913e4ed","5a4496b8ab7f4c3cbc19cee290c6923f","9cf3f4de83094fb0ac05d966e7b8af8e","c7197c6d8ee04b90b412eb901b2fea80","80b878f6f8354105982b8420f236d4e0","3d1ace4837dd47418461c7d80075c922","ef9f8708dea4468cb2bda89be1f521ed","acc1393a1fb04cc4b8c91ce97569912d","1c5f461c45ce41fab10d54c5440e217f","fefc1d62640640fbb3a2cc56c8c11cab","067a41a4bc194864b6c15d38de7b9972","4e87adb8560e4c76aa756d41f5b82dfd","c160205ac34b4986a491304b1f556668","8dae719728a64fbfb1f5042eaa206e23","469553f75d9f426ea73de111262b2f59","77f74526125f4c4e8b39e318fa580678","75ae4630cbc74401bc3f4e38f56b9149","a595faffd31e44a8a9f3c10d3e091df9","9c77c42c46da4e39b87db6804f534ba1","e6020d98c64749be967c39ed830bdccf","f520a6267744473eb4dd2c332b6d9b66","dc9361c47c8646a8a6015f6ffe220f56","66e85ceb2f1d42ffb33f0a172dcc5d13","70906709f0b946ac92a2a801a0aa3510","87bc9333d027454189ab7e5e09a2a587","539111bf9a33420fab45e0237b0b9323","1c4fa1341cda4f59ae7a7719a5f84eca","646c371e686041ceac9a3eae3297ddc1","7d10579d0aa64c56bf64462faadf2973","32d39934bb884208ae622d1ac573014c","060e70488f9c43f69add4401238ace89","b87cc450e8c64477bc39fae1c32e8a89","c789e0901ce749a1a90a2424f50c076b","523309ee674a4021947febd17f1075e0","fe649d03a7754c8b8ec0dc91b4129407","20ba3a106d5f44dd973a9a5f6d56c583","24eb7387da7f414c8e92d809628f0c5b","8a125da509bf4be2bc3c439660bfb53e","9837523832ef4b55af31c647a15fb56c","d288a37f26dd45fc9f17399e72452bdb","11ebe95ccd1349babe23c83a81846014","1cde02b55dd343ecb3eedce18ab312c2","f84813dd79824c5ea92b90d983e1e248","fd289a3177534efab7308f7090e4c628","fed034892e1c405883a54328e3629123","1e4b26162cdb4a1cb5dac2b5d61b7e4a","3c2ac14be9fa4748af4bea278fe9ac6d","4d3abe31639f4a298ed88ae5e526c6fa","83290d56c4494e0c94983d9cae4070f6","6872c6333a2c430aae54e10f4af0354f","47f9b66ebbcd405082ae6ce1773a1701"]},"executionInfo":{"status":"ok","timestamp":1715860995651,"user_tz":240,"elapsed":31438,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"05675f85-15ce-4f92-f4fb-fdcc531a827d"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e5cf77cc041424da6b798f1dd4b9e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/6.43G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e077bf71f354724965ab024751beb35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bb98bd50f1b4c599650e7d457061920"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acc1393a1fb04cc4b8c91ce97569912d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c77c42c46da4e39b87db6804f534ba1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d39934bb884208ae622d1ac573014c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ebe95ccd1349babe23c83a81846014"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-xl into HookedTransformer\n"]}],"source":["from transformer_lens import HookedTransformer\n","\n","model = HookedTransformer.from_pretrained(\n","    \"gpt2-xl\"\n",")  # This will wrap huggingface models and has lots of nice utilities."]},{"cell_type":"markdown","metadata":{"id":"aUiXrjdUOVHv"},"source":["### Getting a vibe for a model using `model.generate`"]},{"cell_type":"markdown","metadata":{"id":"ZZfKT5aDOVHv"},"source":["Let's start by generating some stories using the model."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G4ad4Zz1OVHv","colab":{"base_uri":"https://localhost:8080/","height":108},"executionInfo":{"status":"ok","timestamp":1715856750035,"user_tz":240,"elapsed":10747,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"11ec73f9-1f84-4cfc-ed9f-8d88fbed8341"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\"You messed up because you're dishonest; honest people will accept that I know what I'm doing. If you don't, you can try again next week, next month, or next year. Or do us all a favor and fuck yourself\\n\\nI can't tell you to\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\"You messed up because you're not good enough. At some time you have to unlearn the ways you've learned. At some point you have to learn new ways.\\n\\n\\nThe treatment of Walter White's epiphany was truly brilliant. It was unusual, yes. He certainly\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{}}],"source":["# # here we use generate to get 10 completeions with temperature 1. Feel free to play with the prompt to make it more interesting.\n","# for i in range(2):\n","#     display(\n","#         model.generate(\n","#             # \"I think you're\",\n","#             \"You messed up because you're\",\n","#             stop_at_eos=False,  # avoids a bug on MPS\n","#             temperature=1,\n","#             verbose=False,\n","#             max_new_tokens=50,\n","#         )\n","#     )"]},{"cell_type":"markdown","metadata":{"id":"RDKr8o1xOVHv"},"source":["One thing we notice is that the model seems to be able to repeat [X] consistently. To better understand the models ability to remember [X], let's extract a prompt where the next character is determined and use the \"test_prompt\" utility from TransformerLens to check the ranking of the token for [X]."]},{"cell_type":"markdown","metadata":{"id":"KsfJX-YpOVHv"},"source":["### Spot checking model abilities with `transformer_lens.utils.test_prompt`"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"TpmPoj7uOVHv","colab":{"base_uri":"https://localhost:8080/","height":280},"executionInfo":{"status":"ok","timestamp":1715856750036,"user_tz":240,"elapsed":77,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"74d1dedb-231b-43d9-b4f1-c2b1360ab76a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['<|endoftext|>', 'I', ' think', ' you', \"'re\"]\n","Tokenized answer: [' angry']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m248\u001b[0m\u001b[1m      Logit:  \u001b[0m\u001b[1;36m9.03\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.03\u001b[0m\u001b[1m% Token: | angry|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span><span style=\"font-weight: bold\">      Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.03</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.03</span><span style=\"font-weight: bold\">% Token: | angry|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 15.02 Prob: 10.41% Token: | right|\n","Top 1th token. Logit: 14.38 Prob:  5.48% Token: | missing|\n","Top 2th token. Logit: 14.27 Prob:  4.89% Token: | being|\n","Top 3th token. Logit: 13.88 Prob:  3.32% Token: | going|\n","Top 4th token. Logit: 13.76 Prob:  2.94% Token: | a|\n","Top 5th token. Logit: 13.47 Prob:  2.20% Token: | making|\n","Top 6th token. Logit: 13.46 Prob:  2.18% Token: | getting|\n","Top 7th token. Logit: 13.42 Prob:  2.09% Token: | talking|\n","Top 8th token. Logit: 13.41 Prob:  2.07% Token: | looking|\n","Top 9th token. Logit: 13.39 Prob:  2.03% Token: | on|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' angry'\u001b[0m, \u001b[1;36m248\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' angry'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">248</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}],"source":["# from transformer_lens.utils import test_prompt\n","\n","# # Test the model with a prompt\n","# test_prompt(\n","#     \"I think you're\",\n","#     \" angry\",\n","#     model,\n","#     prepend_space_to_answer=False,\n","# )"]},{"cell_type":"markdown","metadata":{"id":"jGzOvReDOVHv"},"source":["In the output above, we see that the model assigns ~ % probability to [X] being the next token."]},{"cell_type":"markdown","metadata":{"id":"QH8YOZOzOVHv"},"source":["### Exploring Model Capabilities with Log Probs"]},{"cell_type":"markdown","metadata":{"id":"50mqTBihOVHw"},"source":["Look at token log probs for ALL tokens in a prompt. Hover to get the top5 tokens by log probability. Darker tokens are tokens where the model assigned a higher probability to the actual next token.\n","\n","Given prompt \"A B C D\", this predicts the rank of predicting \"C\" given \"A B\". The actual prompt has \"A B C\", but if only \"A B\" was given, how \"much\" does the model expect C? [improve this explanation]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"Tic0RCUpOVHw","colab":{"base_uri":"https://localhost:8080/","height":419},"executionInfo":{"status":"ok","timestamp":1715856750987,"user_tz":240,"elapsed":1013,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"948d2b9f-98ca-47a5-f4ff-75600c457dcb"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<circuitsvis.utils.render.RenderedHTML at 0x7ab8248b21d0>"],"text/html":["<div id=\"circuits-vis-bf84f846-b76a\" style=\"margin: 15px 0;\"/>\n","    <script crossorigin type=\"module\">\n","    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n","    render(\n","      \"circuits-vis-bf84f846-b76a\",\n","      TokenLogProbs,\n","      {\"prompt\": [\"<|endoftext|>\", \"Hi\", \",\", \" how\", \" are\", \" you\", \" doing\", \" this\", \"?\", \" I\", \"'m\", \" really\", \" enjoying\", \" your\", \" posts\"], \"topKLogProbs\": [[-2.6872425079345703, -3.715317726135254, -3.7391061782836914, -4.057321548461914, -4.083163261413574, -4.277179718017578, -4.56345272064209, -4.786035537719727, -4.840024948120117, -4.921682357788086], [-1.5797330141067505, -2.027867317199707, -2.518280029296875, -2.590961456298828, -2.6814584732055664, -2.930814743041992, -3.703841209411621, -3.7082290649414062, -3.964371681213379, -4.156004905700684], [-1.2318705320358276, -1.7359265089035034, -2.5717782974243164, -2.994002342224121, -3.4413251876831055, -3.494873046875, -3.7443418502807617, -4.275408744812012, -4.586957931518555, -4.770859718322754], [-0.24571247398853302, -2.6420347690582275, -3.656780481338501, -4.192373275756836, -4.237569808959961, -4.753711700439453, -4.913618087768555, -4.970931053161621, -4.990654945373535, -5.3073530197143555], [-0.03641727939248085, -4.4173665046691895, -4.958733081817627, -5.668175220489502, -6.122833728790283, -6.753963947296143, -6.75415563583374, -6.940831661224365, -7.264197826385498, -7.270103931427002], [-0.6300495266914368, -1.803013563156128, -2.566391706466675, -2.714130163192749, -3.3134095668792725, -3.869537115097046, -4.360833168029785, -4.72672176361084, -5.252769470214844, -5.295635223388672], [-0.36235496401786804, -2.0277154445648193, -3.162118673324585, -3.9841878414154053, -4.014175891876221, -4.03933572769165, -4.709605693817139, -5.5270676612854, -5.783412456512451, -5.950422763824463], [-1.6068084239959717, -1.6255261898040771, -2.246575117111206, -2.420941114425659, -2.4848086833953857, -2.5086209774017334, -3.891991376876831, -4.138782501220703, -4.2538299560546875, -4.264670372009277], [-1.6876351833343506, -1.691267728805542, -3.0004794597625732, -3.5912444591522217, -3.6452834606170654, -3.8094260692596436, -3.9082038402557373, -4.138927459716797, -4.2039079666137695, -4.211038589477539], [-0.9844256639480591, -1.781178593635559, -2.76583194732666, -3.050565719604492, -3.126866340637207, -3.3353052139282227, -3.823701858520508, -3.837080955505371, -4.730781555175781, -4.876978874206543], [-3.284498453140259, -3.4799654483795166, -3.5058062076568604, -3.5515472888946533, -3.736788034439087, -3.7611420154571533, -3.951920747756958, -4.018680572509766, -4.040225982666016, -4.1057281494140625], [-1.4299159049987793, -1.8895297050476074, -1.982043743133545, -2.109611988067627, -3.2732672691345215, -3.4283337593078613, -3.7002005577087402, -4.0049262046813965, -4.085874080657959, -4.503758907318115], [-1.6405433416366577, -1.6505874395370483, -2.1066107749938965, -2.702155590057373, -3.4549689292907715, -3.4871678352355957, -3.685403347015381, -3.914625644683838, -4.134899616241455, -4.439239978790283], [-2.2108352184295654, -2.5059049129486084, -2.595972776412964, -3.2893331050872803, -3.7323920726776123, -3.972808599472046, -4.045782089233398, -4.075493812561035, -4.11903190612793, -4.294276237487793]], \"topKTokens\": [[\"The\", \"A\", \"\\\"\", \"I\", \"In\", \"This\", \"It\", \"As\", \"We\", \"If\"], [\",\", \" everyone\", \" there\", \" all\", \"!\", \" guys\", \".\", \" Everyone\", \" All\", \" everybody\"], [\" I\", \"\\n\", \" my\", \"\\n\\n\", \" everyone\", \" this\", \" we\", \" and\", \"I\", \" i\"], [\" are\", \"'s\", \" is\", \" you\", \" can\", \" about\", \" may\", \"'re\", \" do\", \" ya\"], [\" you\", \" ya\", \" things\", \" the\", \" we\", \" your\", \" u\", \" all\", \" ye\", \" y\"], [\"?\", \" doing\", \" all\", \" today\", \" guys\", \",\", \" this\", \"!\", \" ?\", \".\"], [\"?\", \" today\", \",\", \".\", \"!\", \" this\", \" ?\", \" guys\", \" now\", \" tonight\"], [\" morning\", \" evening\", \" week\", \" afternoon\", \" fine\", \" weekend\", \" summer\", \" year\", \" month\", \" time\"], [\"\\n\", \" I\", \" It\", \" We\", \" This\", \" My\", \" If\", \" Well\", \" Welcome\", \" Today\"], [\"'m\", \" am\", \" hope\", \"'ve\", \" have\", \" know\", \" was\", \" just\", \" see\", \" thought\"], [\" a\", \" here\", \" glad\", \" so\", \" writing\", \" back\", \" doing\", \" very\", \" trying\", \" not\"], [\" excited\", \" sorry\", \" happy\", \" glad\", \" looking\", \" enjoying\", \" pleased\", \" busy\", \",\", \" good\"], [\" this\", \" the\", \" your\", \" my\", \" it\", \" reading\", \" working\", \" writing\", \" watching\", \" our\"], [\" blog\", \" work\", \" site\", \" website\", \" new\", \" book\", \" podcast\", \" story\", \" show\", \" article\"]], \"correctTokenRank\": [147, 0, 39, 0, 0, 1, 5, 68, 1, 0, 15, 5, 2, 21], \"correctTokenLogProb\": [-6.979203224182129, -1.5797330141067505, -6.518105506896973, -0.24571247398853302, -0.03641727939248085, -1.803013563156128, -4.03933572769165, -7.823346138000488, -1.691267728805542, -0.9844256639480591, -4.393807411193848, -3.4283337593078613, -2.1066107749938965, -4.952796936035156]}\n","    )\n","    </script>"]},"metadata":{},"execution_count":6}],"source":["# import circuitsvis as cv  # optional dep, install with pip install circuitsvis\n","\n","# # Let's make a longer prompt and see the log probabilities of the tokens\n","# example_prompt = \"\"\"Hi, how are you doing this? I'm really enjoying your posts\"\"\"\n","# logits, cache = model.run_with_cache(example_prompt)\n","# cv.logits.token_log_probs(\n","#     model.to_tokens(example_prompt),\n","#     model(example_prompt)[0].log_softmax(dim=-1),\n","#     model.to_string,\n","# )\n","# # hover on the output to see the result."]},{"cell_type":"markdown","metadata":{"id":"lhGIl3YbOVHw"},"source":["Let's combine `model.generate` and the token log probs visualization to see the log probs on text generated by the model. Note that we can play with the temperature and this should sample less likely trajectories according to the model.\n","\n","Some things to explore:\n","- Which tokens does the model assign high probability to? Can you see how the model should know which word comes next?\n","- What happens if you increase / decrease the temperature?\n","- Do the rankings of tokens seem sensible to you? What about where the model doesn't assign a high probability to the token which came next?"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"Nikp2ASlOVHw","colab":{"base_uri":"https://localhost:8080/","height":517,"referenced_widgets":["3c96f238d4c541feaa1a1b4e43782381","a9cb3767aa2a4ad48aa34f938dab6dd3","0c43112bde344226a35227bfbafb6187","8b67eca996de4647bb634f93d1150f2b","7f2e6bb00ea14f9ea294838c71de2472","c23a78d4229e4566b0e97d9c5d2a9d9a","dc3ccfad233247a38da289022d79fc36","90a2d1699e9e4bd19d17a756dca0d6ea","a0c6f0ada39d430fb9cf4dadb070abc6","03ccc94c24c54c8ba6d383600e97f96a","93f2169c7b4942cebfb4ee6b5c93ed9c"]},"executionInfo":{"status":"ok","timestamp":1715856757299,"user_tz":240,"elapsed":6342,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3d073b91-363f-4423-b289-cc9394db01f2"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/50 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c96f238d4c541feaa1a1b4e43782381"}},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["<circuitsvis.utils.render.RenderedHTML at 0x7ab8248b03a0>"],"text/html":["<div id=\"circuits-vis-8307fc9b-2974\" style=\"margin: 15px 0;\"/>\n","    <script crossorigin type=\"module\">\n","    import { render, TokenLogProbs } from \"https://unpkg.com/circuitsvis@1.43.2/dist/cdn/esm.js\";\n","    render(\n","      \"circuits-vis-8307fc9b-2974\",\n","      TokenLogProbs,\n","      {\"prompt\": [\"<|endoftext|>\", \"You\", \" messed\", \" up\", \" because\", \" you\", \"'re\", \" not\", \" the\", \" one\", \" who\", \" can\", \" please\", \" her\", \".\", \" You\", \"'re\", \" not\", \" the\", \" one\", \" who\", \" can\", \" please\", \" her\", \" in\", \" ways\", \" that\", \" won\", \"'t\", \" mess\", \" up\", \" the\", \" relationship\", \".\", \"\\n\", \"\\n\", \"You\", \" win\", \" the\", \" day\", \" by\", \" pursuing\", \" the\", \" things\", \" that\", \" bring\", \" you\", \" pleasure\", \" and\", \" hard\", \" work\", \" and\", \" the\", \" magic\", \" of\", \" optional\", \" assignments\"], \"topKLogProbs\": [[-2.6872377395629883, -3.7153148651123047, -3.739107131958008, -4.057324409484863, -4.083160400390625, -4.277179718017578, -4.563450813293457, -4.786036491394043, -4.840023040771484, -4.921683311462402], [-1.7873629331588745, -2.5576038360595703, -2.62857723236084, -2.747570037841797, -2.8047752380371094, -2.813754081726074, -2.89150333404541, -3.041299819946289, -3.2347869873046875, -3.2431373596191406], [-0.3385504186153412, -1.4747122526168823, -4.356453895568848, -4.5300397872924805, -5.285601615905762, -5.693315505981445, -5.95765495300293, -6.163137435913086, -6.167057037353516, -6.388240814208984], [-1.2483701705932617, -1.8593559265136719, -2.7399988174438477, -3.0622034072875977, -3.0980682373046875, -3.120511054992676, -3.4289941787719727, -3.972315788269043, -4.008922576904297, -4.034462928771973], [-0.3124949634075165, -2.8272788524627686, -3.258756399154663, -3.9420039653778076, -4.086424350738525, -4.121204853057861, -4.995398998260498, -4.998521327972412, -5.29218053817749, -5.350015163421631], [-1.9465878009796143, -2.0086777210235596, -2.2450506687164307, -2.5195229053497314, -3.013073205947876, -3.7174274921417236, -3.804032564163208, -3.8170406818389893, -3.981431245803833, -4.274041175842285], [-1.6439428329467773, -2.0617876052856445, -3.06442928314209, -3.274247169494629, -3.7173662185668945, -3.9257287979125977, -3.9751367568969727, -4.105877876281738, -4.119577407836914, -4.472037315368652], [-2.158535957336426, -3.285696029663086, -3.5178604125976562, -3.650984764099121, -3.698563575744629, -3.891572952270508, -3.906336784362793, -3.9508323669433594, -3.951833724975586, -3.958584785461426], [-2.3490042686462402, -2.349325656890869, -2.6970009803771973, -2.8562169075012207, -2.962996006011963, -3.059354305267334, -3.2140746116638184, -3.2613930702209473, -4.120418071746826, -4.242217540740967], [-1.2856601476669312, -2.201756000518799, -2.264711856842041, -2.8258252143859863, -3.592711925506592, -3.689370632171631, -3.7678465843200684, -3.7719874382019043, -4.126728534698486, -4.1998820304870605], [-2.225620985031128, -2.7286431789398193, -2.920698881149292, -2.983276128768921, -3.1232669353485107, -3.209876775741577, -3.7424771785736084, -3.8624417781829834, -3.948195219039917, -4.013924598693848], [-2.1403377056121826, -2.2541778087615967, -2.5149648189544678, -3.01517653465271, -3.4313037395477295, -3.4901435375213623, -3.498321294784546, -3.770372152328491, -3.7853124141693115, -4.200722694396973], [-2.0391693115234375, -2.106265068054199, -2.1847076416015625, -2.3002328872680664, -3.08074951171875, -3.3596744537353516, -3.4352474212646484, -3.45139217376709, -3.8670339584350586, -3.9320068359375], [-0.5466644763946533, -2.175818681716919, -2.75890851020813, -3.6444170475006104, -4.399212837219238, -4.542452812194824, -4.569319725036621, -4.674748420715332, -4.865621566772461, -4.941126823425293], [-0.9569246768951416, -1.864258050918579, -2.8214075565338135, -3.797034502029419, -3.885302782058716, -3.9175217151641846, -3.945772409439087, -4.088380813598633, -4.1745405197143555, -4.533755302429199], [-1.2675819396972656, -2.59829044342041, -2.791231155395508, -3.2918033599853516, -3.3372573852539062, -3.5855798721313477, -3.6936159133911133, -3.7745561599731445, -3.807828903198242, -3.8222389221191406], [-1.1716477870941162, -1.5762383937835693, -2.3363492488861084, -2.682267904281616, -4.107259750366211, -4.296236038208008, -4.493527412414551, -4.566300392150879, -4.6352949142456055, -4.690702438354492], [-0.5044971704483032, -3.0355305671691895, -3.1550755500793457, -3.7215914726257324, -3.7363219261169434, -4.001378536224365, -4.5745673179626465, -4.7657694816589355, -4.7784037590026855, -5.236582279205322], [-0.23151250183582306, -3.92024564743042, -3.991908550262451, -4.233270168304443, -4.310323238372803, -4.9706501960754395, -5.442297458648682, -5.6174702644348145, -5.617517948150635, -5.645626544952393], [-0.35710379481315613, -2.3727993965148926, -2.934431552886963, -3.417224407196045, -3.8068032264709473, -4.6649298667907715, -4.945765018463135, -5.087083339691162, -5.163236141204834, -5.237595081329346], [-0.5801388025283813, -2.756748676300049, -3.051485538482666, -3.3054614067077637, -3.457672595977783, -3.86216402053833, -3.965482234954834, -4.024339199066162, -4.660750865936279, -4.753879070281982], [-1.8077993392944336, -3.1121301651000977, -3.1463937759399414, -3.222201347351074, -3.4992170333862305, -3.6470556259155273, -3.8813858032226562, -3.903590202331543, -4.133543968200684, -4.183468818664551], [-0.505101203918457, -3.2657365798950195, -3.3704967498779297, -3.5322961807250977, -3.5583410263061523, -3.6221799850463867, -3.646574020385742, -4.031893730163574, -4.200410842895508, -4.240102767944336], [-0.9837397933006287, -1.2604866027832031, -2.452450752258301, -3.849262237548828, -3.882645606994629, -3.933089256286621, -4.391762733459473, -4.456111907958984, -4.757844924926758, -4.842827796936035], [-1.0843923091888428, -1.4779431819915771, -2.8142030239105225, -2.873859167098999, -3.0928094387054443, -3.5565106868743896, -3.748281240463257, -4.0552778244018555, -4.1524200439453125, -4.165536880493164], [-0.44388991594314575, -1.9418470859527588, -2.369917154312134, -3.649724245071411, -4.226032733917236, -4.806936740875244, -4.860236644744873, -5.267614841461182, -5.614292621612549, -5.805591106414795], [-1.9802968502044678, -2.008923292160034, -2.0183560848236084, -2.4498403072357178, -2.877018690109253, -2.893666982650757, -3.3648669719696045, -3.9442927837371826, -4.069876670837402, -4.297624588012695], [-0.0006195771275088191, -8.982114791870117, -9.688482284545898, -9.8783597946167, -11.044065475463867, -11.146950721740723, -11.28772258758545, -11.412152290344238, -11.707179069519043, -11.778841972351074], [-1.874646782875061, -1.8779655694961548, -3.2086148262023926, -3.261226177215576, -3.3928980827331543, -3.6343884468078613, -3.6903767585754395, -3.9340033531188965, -4.172500133514404, -4.188342571258545], [-0.5356545448303223, -1.4683175086975098, -3.2303643226623535, -3.262878894805908, -3.3340306282043457, -4.283947467803955, -4.677598476409912, -4.760753154754639, -5.367326259613037, -5.464405536651611], [-0.9349475502967834, -1.5315890312194824, -1.6549811363220215, -3.2426342964172363, -4.305487155914307, -4.420846462249756, -4.426055431365967, -4.671708583831787, -4.860382556915283, -5.037599086761475], [-1.050565242767334, -2.235999584197998, -3.707014560699463, -3.8282923698425293, -3.8647780418395996, -4.07201623916626, -4.237024784088135, -4.326439380645752, -4.410292148590088, -4.464146137237549], [-0.442756712436676, -2.5967273712158203, -3.3058815002441406, -3.607089042663574, -3.7316579818725586, -3.806386947631836, -3.9135189056396484, -4.334621429443359, -4.474876403808594, -4.7014970779418945], [-0.7462474703788757, -1.700709581375122, -2.7781927585601807, -3.6124870777130127, -4.1340837478637695, -4.202277183532715, -4.287289619445801, -4.381991386413574, -4.433050155639648, -4.447575569152832], [-0.01150108128786087, -6.731751441955566, -7.202230453491211, -7.8137617111206055, -8.038844108581543, -8.077698707580566, -8.079440116882324, -8.203414916992188, -8.411263465881348, -8.489569664001465], [-1.0488871335983276, -2.9650325775146484, -2.9956436157226562, -3.4723806381225586, -3.620415687561035, -3.667515754699707, -3.8526830673217773, -3.865774154663086, -4.12070369720459, -4.127379417419434], [-0.8902984857559204, -2.786618709564209, -2.867593288421631, -3.1838040351867676, -3.4414267539978027, -3.7208304405212402, -3.8057990074157715, -3.9253134727478027, -4.163276195526123, -4.253201961517334], [-1.676889181137085, -1.7829177379608154, -2.520017385482788, -2.558885335922241, -2.6836678981781006, -3.017717123031616, -3.4421937465667725, -3.7594993114471436, -3.9270122051239014, -3.9328248500823975], [-1.6271302700042725, -2.0437171459198, -2.1787803173065186, -3.6274306774139404, -3.655120611190796, -3.8943917751312256, -3.921315908432007, -3.9244611263275146, -3.95837664604187, -4.0679931640625], [-1.5130006074905396, -1.8624745607376099, -2.033294677734375, -2.2972774505615234, -2.386251449584961, -2.908761978149414, -3.4211244583129883, -3.703249931335449, -3.7507495880126953, -4.156055450439453], [-1.6267545223236084, -2.8464314937591553, -2.851717710494995, -2.965275526046753, -3.6086204051971436, -3.6173980236053467, -3.900682210922241, -3.9142215251922607, -4.067790985107422, -4.2412567138671875], [-1.4053449630737305, -1.7865419387817383, -1.8287897109985352, -2.1750059127807617, -2.9693117141723633, -4.159160614013672, -4.200133323669434, -4.419414520263672, -4.482483863830566, -4.928654670715332], [-1.3523486852645874, -2.4605298042297363, -3.025050640106201, -3.7384514808654785, -3.756857395172119, -3.975161075592041, -4.145287990570068, -4.262183666229248, -4.277891635894775, -4.477741718292236], [-0.721882164478302, -1.3703348636627197, -1.6177408695220947, -4.000242710113525, -5.56191873550415, -5.705062389373779, -5.8481879234313965, -5.887409687042236, -6.351351261138916, -6.564602375030518], [-1.7848109006881714, -1.9076975584030151, -2.217487335205078, -2.427689552307129, -2.4595909118652344, -3.414426803588867, -4.010401725769043, -4.072488784790039, -4.171804428100586, -4.178881645202637], [-0.4918416738510132, -1.3289846181869507, -3.8268885612487793, -4.372496128082275, -4.616797924041748, -4.649255275726318, -4.699805736541748, -5.220518589019775, -5.604609966278076, -5.665661334991455], [-1.507955551147461, -1.5775299072265625, -1.8154945373535156, -2.0315818786621094, -2.9167938232421875, -3.854890823364258, -4.084263801574707, -4.393097877502441, -4.407845497131348, -4.575604438781738], [-1.0771781206130981, -1.4442015886306763, -1.8497897386550903, -3.3666467666625977, -4.067984580993652, -4.1136274337768555, -4.340943336486816, -4.400140762329102, -4.504961967468262, -4.52553653717041], [-2.9895083904266357, -3.2652652263641357, -3.4258792400360107, -3.433453321456909, -3.4420716762542725, -3.630592107772827, -3.6535117626190186, -3.7153279781341553, -3.828826665878296, -3.9463508129119873], [-0.4473673701286316, -3.156538724899292, -3.358539342880249, -3.836015462875366, -3.9776809215545654, -4.332194805145264, -4.420750141143799, -4.484288692474365, -4.676207065582275, -4.696946620941162], [-0.8972546458244324, -1.8081223964691162, -2.260972738265991, -3.1062467098236084, -3.682629346847534, -4.131446361541748, -4.360449314117432, -4.366882801055908, -4.496982097625732, -4.864535808563232], [-3.235379934310913, -3.4206368923187256, -3.7216336727142334, -3.8431670665740967, -3.847602605819702, -3.8585917949676514, -4.099291801452637, -4.199166297912598, -4.216740608215332, -4.218635559082031], [-1.6213575601577759, -3.002864360809326, -3.1032814979553223, -3.773228168487549, -3.860832691192627, -4.067325115203857, -4.081918239593506, -4.27862024307251, -4.398836612701416, -4.525894641876221], [-1.1858876943588257, -1.3076452016830444, -3.337338924407959, -3.356942653656006, -3.5104575157165527, -3.5402941703796387, -3.672229290008545, -3.783324718475342, -4.001357555389404, -4.5104451179504395], [-2.3151187896728516, -2.856935501098633, -3.055508613586426, -3.1246137619018555, -3.718573570251465, -3.7835559844970703, -3.8328771591186523, -4.061017036437988, -4.1744842529296875, -4.245893478393555], [-2.9273228645324707, -3.070091724395752, -3.086207866668701, -3.3484292030334473, -3.864899158477783, -3.9319777488708496, -4.17405366897583, -4.213210582733154, -4.307279109954834, -4.438256740570068]], \"topKTokens\": [[\"The\", \"A\", \"\\\"\", \"I\", \"In\", \"This\", \"It\", \"As\", \"We\", \"If\"], [\" can\", \" may\", \" are\", \"'ve\", \" must\", \" have\", \" know\", \"'re\", \" don\", \" might\"], [\" up\", \" with\", \" me\", \" it\", \" this\", \" around\", \" the\", \" your\", \" us\", \" something\"], [\".\", \",\", \"!\", \" your\", \" big\", \" and\", \" the\", \" a\", \" my\", \"\\n\"], [\" you\", \" of\", \" your\", \" I\", \" the\", \" it\", \" this\", \",\", \" there\", \" we\"], [\"'re\", \" were\", \" didn\", \" are\", \" don\", \" have\", \" thought\", \" weren\", \" did\", \" had\"], [\" a\", \" not\", \" an\", \" human\", \" stupid\", \" too\", \" trying\", \" the\", \" in\", \" young\"], [\" a\", \" good\", \" paying\", \" in\", \" being\", \" ready\", \" listening\", \" trying\", \" smart\", \" using\"], [\" one\", \" kind\", \" person\", \" best\", \" only\", \" type\", \" right\", \" boss\", \" most\", \" real\"], [\" who\", \" in\", \" with\", \" that\", \" you\", \" making\", \" to\", \" being\", \" doing\", \".\"], [\"'s\", \" should\", \" has\", \" needs\", \" is\", \" made\", \" was\", \" wrote\", \" got\", \" gets\"], [\"'t\", \" fix\", \" change\", \" make\", \" control\", \" do\", \" be\", \" get\", \" see\", \" help\"], [\" the\", \" your\", \" everyone\", \" her\", \" yourself\", \" people\", \" a\", \" them\", \" someone\", \" all\"], [\".\", \",\", \"\\n\", \" and\", \"!\", \";\", \" best\", \" the\", \" or\", \" (\"], [\"\\n\", \" You\", \" She\", \"\\n\\n\", \" That\", \" It\", \" And\", \" I\", \" If\", \" But\"], [\"'re\", \" messed\", \" can\", \" don\", \"'ve\", \" are\", \" need\", \" fucked\", \" didn\", \" know\"], [\" not\", \" the\", \" just\", \" a\", \" supposed\", \" too\", \" her\", \" going\", \" only\", \" trying\"], [\" the\", \" her\", \" a\", \" in\", \" even\", \" going\", \" that\", \" supposed\", \" good\", \" as\"], [\" one\", \" guy\", \" man\", \" person\", \" only\", \" kind\", \" type\", \" best\", \" alpha\", \" girl\"], [\" who\", \" she\", \" that\", \" with\", \" to\", \" in\", \".\", \" whose\", \" you\", \" the\"], [\" can\", \"'s\", \" has\", \" is\", \" knows\", \" will\", \" gets\", \" makes\", \" understands\", \" could\"], [\" make\", \" give\", \" be\", \" get\", \" do\", \" please\", \" take\", \" keep\", \" show\", \" tell\"], [\" her\", \" him\", \" anyone\", \" yourself\", \" the\", \" you\", \" your\", \" herself\", \" me\", \" a\"], [\" because\", \".\", \",\", \" and\", \" by\", \" in\", \" with\", \" if\", \" for\", \" right\"], [\" the\", \" a\", \" bed\", \" any\", \" this\", \" her\", \" that\", \" your\", \" every\", \" ways\"], [\" that\", \" she\", \" you\", \" other\", \" he\", \" the\", \" her\", \" which\", \" your\", \" no\"], [\" she\", \" make\", \" are\", \" will\", \" you\", \" don\", \" satisfy\", \" aren\", \" feel\", \" can\"], [\"'t\", \" her\", \" the\", \"'\", \" your\", \" over\", \",\", \".\", \"\\ufffd\", \"\\u00b4\"], [\" hurt\", \" make\", \" be\", \" cause\", \" upset\", \" get\", \" piss\", \" break\", \" offend\", \" alien\"], [\" up\", \" with\", \" you\", \" her\", \" things\", \" everything\", \" it\", \" your\", \" anything\", \" the\"], [\" your\", \" the\", \" her\", \" everything\", \" this\", \" things\", \" what\", \" our\", \" you\", \" with\"], [\" relationship\", \" rest\", \" whole\", \" way\", \" other\", \" marriage\", \" family\", \" lives\", \" life\", \" relationships\"], [\".\", \",\", \" you\", \" or\", \" and\", \" with\", \" for\", \" in\", \" (\", \" between\"], [\" You\", \"\\n\", \" She\", \" And\", \" If\", \"\\n\\n\", \" I\", \" It\", \" So\", \" That\"], [\"\\n\", \"I\", \"The\", \"This\", \"You\", \"So\", \"It\", \"\\\"\", \"In\", \"And\"], [\"You\", \"I\", \"She\", \"If\", \"And\", \"It\", \"The\", \"That\", \"So\", \"This\"], [\"'re\", \" can\", \" messed\", \" don\", \" are\", \"'ve\", \" know\", \" fucked\", \" didn\", \" have\"], [\" because\", \".\", \",\", \" by\", \" when\", \" the\", \" if\", \" her\", \" in\", \" some\"], [\" argument\", \" game\", \" battle\", \" fight\", \" relationship\", \" lottery\", \" prize\", \" bet\", \" day\", \" love\"], [\" because\", \",\", \".\", \" when\", \" by\", \" and\", \" with\", \" in\", \" if\", \" for\"], [\" being\", \" not\", \" making\", \" doing\", \" pleasing\", \" showing\", \" giving\", \" listening\", \" trying\", \" taking\"], [\" her\", \" what\", \" your\", \" the\", \" a\", \" and\", \" that\", \" things\", \" something\", \" whatever\"], [\" relationship\", \" things\", \" path\", \" goals\", \" right\", \" goal\", \" one\", \" woman\", \" thing\", \" best\"], [\" that\", \" you\", \" she\", \" in\", \" which\", \" her\", \" your\", \" the\", \" where\", \" and\"], [\" make\", \" she\", \" will\", \" are\", \" you\", \" matter\", \" really\", \" work\", \" can\", \" bring\"], [\" her\", \" you\", \" the\", \" happiness\", \" your\", \" joy\", \" out\", \" pleasure\", \" them\", \" value\"], [\" closer\", \" joy\", \" pleasure\", \" happiness\", \" the\", \" both\", \" satisfaction\", \" together\", \" comfort\", \" and\"], [\".\", \",\", \" and\", \" in\", \" for\", \" (\", \" without\", \" instead\", \" or\", \":\"], [\" not\", \" by\", \" that\", \" pleasing\", \" avoiding\", \" making\", \" don\", \" ignoring\", \" the\", \" you\"], [\" work\", \"ening\", \"-\", \"en\", \" to\", \",\", \".\", \" things\", \" feelings\", \" truths\"], [\".\", \",\", \" and\", \" will\", \" that\", \" is\", \" to\", \" in\", \" (\", \" are\"], [\" not\", \" being\", \" by\", \" self\", \" patience\", \" you\", \" the\", \" honesty\", \" making\", \" good\"], [\" things\", \" right\", \" rest\", \" ability\", \" way\", \" people\", \" other\", \" love\", \" relationship\", \" little\"], [\" of\", \" that\", \" in\", \" you\", \" will\", \" is\", \" and\", \".\", \" she\", \",\"], [\" the\", \" your\", \" being\", \" love\", \" a\", \" doing\", \" making\", \" life\", \" friendship\", \" self\"], [\" fun\", \" sex\", \" things\", \" activities\", \",\", \" extras\", \" but\", \".\", \" play\", \" and\"]], \"correctTokenRank\": [29, 1043, 0, 42, 0, 0, 1, 11, 0, 0, 10, 242, 3, 0, 1, 0, 0, 0, 0, 0, 0, 5, 0, 5, 9, 0, 14, 0, 40, 0, 1, 0, 0, 1, 0, 0, 304, 5, 8, 4, 370, 3, 1, 0, 9, 1, 2, 2, 455, 0, 2, 6, 238, 0, 3283, 143], \"correctTokenLogProb\": [-5.325375556945801, -11.413673400878906, -0.3385504186153412, -6.563363075256348, -0.3124949634075165, -1.9465878009796143, -2.0617876052856445, -4.058414459228516, -2.3490042686462402, -1.2856601476669312, -4.016457557678223, -7.941215515136719, -2.3002328872680664, -0.5466644763946533, -1.864258050918579, -1.2675819396972656, -1.1716477870941162, -0.5044971704483032, -0.23151250183582306, -0.35710379481315613, -0.5801388025283813, -3.6470556259155273, -0.505101203918457, -3.933089256286621, -4.165536880493164, -0.44388991594314575, -5.011697769165039, -0.0006195771275088191, -5.7299017906188965, -0.5356545448303223, -1.5315890312194824, -1.050565242767334, -0.442756712436676, -1.700709581375122, -0.01150108128786087, -1.0488871335983276, -9.406062126159668, -3.017717123031616, -3.95837664604187, -2.386251449584961, -8.692229270935059, -2.1750059127807617, -2.4605298042297363, -0.721882164478302, -4.178881645202637, -1.3289846181869507, -1.8154945373535156, -1.8497897386550903, -8.412796020507812, -0.4473673701286316, -2.260972738265991, -4.099291801452637, -7.7220139503479, -1.1858876943588257, -11.546464920043945, -7.00099515914917]}\n","    )\n","    </script>"]},"metadata":{},"execution_count":7}],"source":["# example_prompt = model.generate(\n","#     \"You messed up because you're\",\n","#     stop_at_eos=False,  # avoids a bug on MPS\n","#     temperature=1,\n","#     verbose=True,\n","#     max_new_tokens=50,\n","# )\n","# logits, cache = model.run_with_cache(example_prompt)\n","# cv.logits.token_log_probs(\n","#     model.to_tokens(example_prompt),\n","#     model(example_prompt)[0].log_softmax(dim=-1),\n","#     model.to_string,\n","# )"]},{"cell_type":"markdown","metadata":{"id":"er3H1TDoOVHw"},"source":["# Training an SAE\n","\n","Now we're ready to train out SAE. We'll make a runner config, instantiate the runner and the rest is taken care of for us!\n","\n","During training, you use weights and biases to check key metrics which indicate how well we are able to optimize the variables we care about.\n","\n","To get a better sense of which variables to look at, you can read my (Joseph's) post [here](https://www.lesswrong.com/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream) and especially look at my weights and biases report [here](https://links-cdn.wandb.ai/wandb-public-images/links/jbloom/uue9i416.html).\n","\n","A few tips:\n","- Feel free to reorganize your wandb dashboard to put L0, CE_Loss_score, explained variance and other key metrics in one section at the top.\n","- Make a [run comparer](https://docs.wandb.ai/guides/app/features/panels/run-comparer) when tuning hyperparameters.\n","- You can download the resulting sparse autoencoder / sparsity estimate from wandb and upload them to huggingface if you want to share your SAE with other.\n","    - cfg.json (training config)\n","    - sae_weight.safetensors (model weights)\n","    - sparsity.safetensors (sparsity estimate)"]},{"cell_type":"markdown","metadata":{"id":"jCHtPycOOVHw"},"source":["## MLP Out\n","\n","I've tuned the hyperparameters below for a decent SAE which achieves 86% CE Loss recovered and an L0 of ~85, and runs in about 2 hours on an M3 Max. You can get an SAE that looks better faster if you only consider L0 and CE loss but it will likely have more dense features and more dead features. Here's a link to my output with two runs with two different L1's: https://wandb.ai/jbloom/sae_lens_tutorial ."]},{"cell_type":"markdown","source":["Paste wandb API key below"],"metadata":{"id":"arFFHcl7jXTU"}},{"cell_type":"code","execution_count":4,"metadata":{"id":"oAsZCAdJOVHw","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["bf50c3760f084f118d5fcd3dd80afee1","69124339cb954064806f38e26b59f662","40c13ac7d323443d9bae241f8d44a0fa","e8d1fad89c5f4ee0b5851a57006e59ad","245bd1eba4bf4848a41fa7e0fef91628","7e7c366b76534965a2094cab426359ae","551c46f2c4ed468696db157a1d167b9c","fb5a9ff6061a45a1bb3597b8c3a718b6"]},"executionInfo":{"status":"ok","timestamp":1715859207224,"user_tz":240,"elapsed":533740,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b98740de-b3e0-4999-bde9-1d9d6f3e8062"},"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 25600-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n","Loaded pretrained model gpt2-xl into HookedTransformer\n","Moving model to device:  cuda\n","Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 25600-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n","Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 25600-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240516_112501-mmh2cmuk</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5/runs/mmh2cmuk' target=\"_blank\">25600-L1-5-LR-5e-05-Tokens-4.096e+06</a></strong> to <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5/runs/mmh2cmuk' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5/runs/mmh2cmuk</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["1000| MSE Loss 0.073 | L1 19.948: 100%|██████████| 4096000/4096000 [08:10<00:00, 28219.64it/s]"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='312.803 MB of 312.803 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf50c3760f084f118d5fcd3dd80afee1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>███████████████████████████▇▅▄▃▂▁</td></tr><tr><td>details/n_training_tokens</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>losses/ghost_grad_loss</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/l1_loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/mse_loss</td><td>█▅▄▃▂▂▂▁▂▁▂▂▁▁▁▁▁▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>losses/overall_loss</td><td>█▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/CE_loss_score</td><td>▁▄█</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>▅▁█</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>▅▁█</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>▅▁█</td></tr><tr><td>metrics/explained_variance</td><td>▁▅▆▇▇████████████████████████████</td></tr><tr><td>metrics/explained_variance_std</td><td>█▇▅▄▃▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l0</td><td>█▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>metrics/l2_norm</td><td>█▁▂</td></tr><tr><td>metrics/l2_ratio</td><td>█▁▃</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>▁</td></tr><tr><td>sparsity/below_1e-5</td><td>▁</td></tr><tr><td>sparsity/below_1e-6</td><td>▁</td></tr><tr><td>sparsity/dead_features</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>details/current_learning_rate</td><td>0.0</td></tr><tr><td>details/n_training_tokens</td><td>4055040</td></tr><tr><td>losses/ghost_grad_loss</td><td>0.0</td></tr><tr><td>losses/l1_loss</td><td>4.13327</td></tr><tr><td>losses/mse_loss</td><td>0.07214</td></tr><tr><td>losses/overall_loss</td><td>20.73848</td></tr><tr><td>metrics/CE_loss_score</td><td>0.0786</td></tr><tr><td>metrics/ce_loss_with_ablation</td><td>2.85</td></tr><tr><td>metrics/ce_loss_with_sae</td><td>2.84974</td></tr><tr><td>metrics/ce_loss_without_sae</td><td>2.84449</td></tr><tr><td>metrics/explained_variance</td><td>-0.01918</td></tr><tr><td>metrics/explained_variance_std</td><td>0.04003</td></tr><tr><td>metrics/l0</td><td>152.01489</td></tr><tr><td>metrics/l2_norm</td><td>0.78042</td></tr><tr><td>metrics/l2_ratio</td><td>0.08546</td></tr><tr><td>metrics/mean_log10_feature_sparsity</td><td>-1.32719</td></tr><tr><td>sparsity/below_1e-5</td><td>0</td></tr><tr><td>sparsity/below_1e-6</td><td>0</td></tr><tr><td>sparsity/dead_features</td><td>0</td></tr><tr><td>sparsity/mean_passes_since_fired</td><td>0.0</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">25600-L1-5-LR-5e-05-Tokens-4.096e+06</strong> at: <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5/runs/mmh2cmuk' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5/runs/mmh2cmuk</a><br/> View project at: <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5</a><br/>Synced 5 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20240516_112501-mmh2cmuk/logs</code>"]},"metadata":{}}],"source":["# total_training_steps = 30_000  # probably we should do more\n","total_training_steps = 1000  # probably we should do more\n","batch_size = 4096\n","# batch_size = 4\n","total_training_tokens = total_training_steps * batch_size\n","\n","lr_warm_up_steps = 0\n","lr_decay_steps = total_training_steps // 5  # 20% of training\n","l1_warm_up_steps = total_training_steps // 20  # 5% of training\n","\n","cfg = LanguageModelSAERunnerConfig(\n","    # Data Generating Function (Model + Training Distibuion)\n","    model_name=\"gpt2-xl\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n","    # hook_point=\"blocks.20.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n","    # hook_point_layer=20,  # Only one layer in the model.\n","    hook_point=\"blocks.5.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n","    hook_point_layer=5,  # Only one layer in the model.\n","    # d_in=1024,  # the width of the mlp output.\n","    d_in=1600,  # the width of the mlp output.\n","    # dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n","    dataset_path=\"stas/openwebtext-10k\",\n","    is_dataset_tokenized=True,\n","    # streaming=True,  # we could pre-download the token dataset if it was small.\n","\n","    # SAE Parameters\n","    mse_loss_normalization=None,  # We won't normalize the mse loss,\n","    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n","    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n","    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n","    normalize_sae_decoder=False,\n","    # scale_sparsity_penalty_by_decoder_norm=True,\n","    # decoder_heuristic_init=True,\n","    # init_encoder_as_decoder_transpose=True,\n","    # normalize_activations=False,\n","\n","    # Training Parameters\n","    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n","    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n","    adam_beta2=0.999,\n","    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n","    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n","    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n","    l1_coefficient=5,  # will control how sparse the feature activations are\n","    # l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n","    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n","    # train_batch_size_tokens=batch_size,\n","    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n","    # Activation Store Parameters\n","    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n","    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n","    # store_batch_size_prompts=16,\n","\n","    # Resampling protocol\n","    use_ghost_grads=False,  # we don't use ghost grads anymore.\n","    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n","    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n","    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n","\n","    # WANDB\n","    log_to_wandb=True,  # always use wandb unless you are just testing code.\n","    # log_to_wandb=False,\n","    wandb_project=\"sae_lens_exploraTest_L5\",\n","    # wandb_project=\"sae_lens_tutorial\",\n","    wandb_log_frequency=30,\n","    # eval_every_n_wandb_logs=20,\n","\n","    # Misc\n","    device=device,\n","    seed=42,\n","    n_checkpoints=0,\n","    checkpoint_path=\"checkpoints\",\n","    dtype=torch.float32,\n",")\n","\n","# look at the next cell to see some instruction for what to do while this is running.\n","sparse_autoencoder_dictionary = language_model_sae_runner(cfg)"]},{"cell_type":"code","source":["# total_training_steps = 30_000  # probably we should do more\n","total_training_steps = 1000  # probably we should do more\n","batch_size = 4096\n","# batch_size = 4\n","total_training_tokens = total_training_steps * batch_size\n","\n","lr_warm_up_steps = 0\n","lr_decay_steps = total_training_steps // 5  # 20% of training\n","l1_warm_up_steps = total_training_steps // 20  # 5% of training\n","\n","cfg_2 = LanguageModelSAERunnerConfig(\n","    # Data Generating Function (Model + Training Distibuion)\n","    model_name=\"gpt2-xl\",  # our model (more options here: https://neelnanda-io.github.io/TransformerLens/generated/model_properties_table.html)\n","    # hook_point=\"blocks.20.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n","    # hook_point_layer=20,  # Only one layer in the model.\n","    hook_point=\"blocks.5.hook_mlp_out\",  # A valid hook point (see more details here: https://neelnanda-io.github.io/TransformerLens/generated/demos/Main_Demo.html#Hook-Points)\n","    hook_point_layer=5,  # Only one layer in the model.\n","    # d_in=1024,  # the width of the mlp output.\n","    d_in=6400,  # the width of the mlp output.\n","    # dataset_path=\"apollo-research/roneneldan-TinyStories-tokenizer-gpt2\",  # this is a tokenized language dataset on Huggingface for the Tiny Stories corpus.\n","    dataset_path=\"stas/openwebtext-10k\",\n","    is_dataset_tokenized=True,\n","    # streaming=True,  # we could pre-download the token dataset if it was small.\n","\n","    # SAE Parameters\n","    mse_loss_normalization=None,  # We won't normalize the mse loss,\n","    expansion_factor=16,  # the width of the SAE. Larger will result in better stats but slower training.\n","    b_dec_init_method=\"zeros\",  # The geometric median can be used to initialize the decoder weights.\n","    apply_b_dec_to_input=False,  # We won't apply the decoder weights to the input.\n","    normalize_sae_decoder=False,\n","    # scale_sparsity_penalty_by_decoder_norm=True,\n","    # decoder_heuristic_init=True,\n","    # init_encoder_as_decoder_transpose=True,\n","    # normalize_activations=False,\n","\n","    # Training Parameters\n","    lr=5e-5,  # lower the better, we'll go fairly high to speed up the tutorial.\n","    adam_beta1=0.9,  # adam params (default, but once upon a time we experimented with these.)\n","    adam_beta2=0.999,\n","    lr_scheduler_name=\"constant\",  # constant learning rate with warmup. Could be better schedules out there.\n","    lr_warm_up_steps=lr_warm_up_steps,  # this can help avoid too many dead features initially.\n","    lr_decay_steps=lr_decay_steps,  # this will help us avoid overfitting.\n","    l1_coefficient=5,  # will control how sparse the feature activations are\n","    # l1_warm_up_steps=l1_warm_up_steps,  # this can help avoid too many dead features initially.\n","    lp_norm=1.0,  # the L1 penalty (and not a Lp for p < 1)\n","    # train_batch_size_tokens=batch_size,\n","    context_size=256,  # will control the lenght of the prompts we feed to the model. Larger is better but slower. so for the tutorial we'll use a short one.\n","    # Activation Store Parameters\n","    n_batches_in_buffer=64,  # controls how many activations we store / shuffle.\n","    training_tokens=total_training_tokens,  # 100 million tokens is quite a few, but we want to see good stats. Get a coffee, come back.\n","    # store_batch_size_prompts=16,\n","\n","    # Resampling protocol\n","    use_ghost_grads=False,  # we don't use ghost grads anymore.\n","    feature_sampling_window=1000,  # this controls our reporting of feature sparsity stats\n","    dead_feature_window=1000,  # would effect resampling or ghost grads if we were using it.\n","    dead_feature_threshold=1e-4,  # would effect resampling or ghost grads if we were using it.\n","\n","    # WANDB\n","    log_to_wandb=True,  # always use wandb unless you are just testing code.\n","    # log_to_wandb=False,\n","    wandb_project=\"sae_lens_exploraTest_L5_v2\",\n","    # wandb_project=\"sae_lens_tutorial\",\n","    wandb_log_frequency=30,\n","    # eval_every_n_wandb_logs=20,\n","\n","    # Misc\n","    device=device,\n","    seed=42,\n","    n_checkpoints=0,\n","    checkpoint_path=\"checkpoints\",\n","    dtype=torch.float32,\n",")\n","\n","# look at the next cell to see some instruction for what to do while this is running.\n","sparse_autoencoder_dictionary_2 = language_model_sae_runner(cfg_2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Yt-vOSaJQuTh","executionInfo":{"status":"error","timestamp":1715861027229,"user_tz":240,"elapsed":30459,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a457044c-5826-4c01-883f-6088475e31f9"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 102400-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-xl into HookedTransformer\n","Moving model to device:  cuda\n","Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 102400-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n","Warning: We are initializing b_dec to zeros. This is probably not what you want.\n","Run name: 102400-L1-5-LR-5e-05-Tokens-4.096e+06\n","n_tokens_per_buffer (millions): 0.524288\n","Lower bound: n_contexts_per_buffer (millions): 0.002048\n","Total training steps: 1000\n","Total wandb updates: 33\n","n_tokens_per_feature_sampling_window (millions): 1048.576\n","n_tokens_per_dead_feature_window (millions): 1048.576\n","We will reset the sparsity calculation 1 times.\n","Number tokens in sparsity calculation window: 4.10e+06\n"]},{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","        window._wandbApiKey = new Promise((resolve, reject) => {\n","            function loadScript(url) {\n","            return new Promise(function(resolve, reject) {\n","                let newScript = document.createElement(\"script\");\n","                newScript.onerror = reject;\n","                newScript.onload = resolve;\n","                document.body.appendChild(newScript);\n","                newScript.src = url;\n","            });\n","            }\n","            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n","            const iframe = document.createElement('iframe')\n","            iframe.style.cssText = \"width:0;height:0;border:none\"\n","            document.body.appendChild(iframe)\n","            const handshake = new Postmate({\n","                container: iframe,\n","                url: 'https://wandb.ai/authorize'\n","            });\n","            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n","            handshake.then(function(child) {\n","                child.on('authorize', data => {\n","                    clearTimeout(timeout)\n","                    resolve(data)\n","                });\n","            });\n","            })\n","        });\n","    "]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":[" ··········\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.17.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20240516_120342-trnmxbwe</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2/runs/trnmxbwe' target=\"_blank\">102400-L1-5-LR-5e-05-Tokens-4.096e+06</a></strong> to <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2/runs/trnmxbwe' target=\"_blank\">https://wandb.ai/wlg100/sae_lens_exploraTest_L5_v2/runs/trnmxbwe</a>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["\rTraining SAE:   0%|          | 0/4096000 [00:00<?, ?it/s]"]},{"output_type":"error","ename":"RuntimeError","evalue":"The expanded size of the tensor (6400) must match the existing size (1600) at non-singleton dimension 3.  Target sizes: [32, 256, 1, 6400].  Tensor sizes: [32, 256, 1, 1600]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-18ef7df77e56>\u001b[0m in \u001b[0;36m<cell line: 76>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;31m# look at the next cell to see some instruction for what to do while this is running.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m \u001b[0msparse_autoencoder_dictionary_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlanguage_model_sae_runner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg_2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/lm_runner.py\u001b[0m in \u001b[0;36mlanguage_model_sae_runner\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# train SAE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     sparse_autoencoder = train_sae_on_language_model(\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0msparse_autoencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/train_sae_on_language_model.py\u001b[0m in \u001b[0;36mtrain_sae_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, dead_feature_threshold, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m \u001b[0mUse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtrain_sae_group_on_language_model\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0minstead\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mkept\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mbackward\u001b[0m \u001b[0mcompatibility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m     return train_sae_group_on_language_model(\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0msae_group\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/train_sae_on_language_model.py\u001b[0m in \u001b[0;36mtrain_sae_group_on_language_model\u001b[0;34m(model, sae_group, activation_store, batch_size, n_checkpoints, feature_sampling_window, use_wandb, wandb_log_frequency)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mn_training_tokens\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_training_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Do a training step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mlayer_acts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation_store\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mn_training_tokens\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py\u001b[0m in \u001b[0;36mnext_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;31m# Try to get the next batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m             \u001b[0;31m# If the DataLoader is exhausted, create a new one\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py\u001b[0m in \u001b[0;36mdataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py\u001b[0m in \u001b[0;36mget_data_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# 1. # create new buffer by mixing stored and new buffer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m         mixing_buffer = torch.cat(\n\u001b[0;32m--> 371\u001b[0;31m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_batches_in_buffer\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstorage_buffer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m             \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/activations_store.py\u001b[0m in \u001b[0;36mget_buffer\u001b[0;34m(self, n_batches_in_buffer)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mrefill_batch_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mrefill_activations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_activations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrefill_batch_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m             new_buffer[\n\u001b[0m\u001b[1;32m    346\u001b[0m                 \u001b[0mrefill_batch_idx_start\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mrefill_batch_idx_start\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m             ] = refill_activations\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (6400) must match the existing size (1600) at non-singleton dimension 3.  Target sizes: [32, 256, 1, 6400].  Tensor sizes: [32, 256, 1, 1600]"]}]},{"cell_type":"markdown","metadata":{"id":"khR_QkAJOVHw"},"source":["# Interpret SAE\n"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"b4sUumxZOVHw","colab":{"base_uri":"https://localhost:8080/","height":362},"executionInfo":{"status":"ok","timestamp":1715859443077,"user_tz":240,"elapsed":799,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"11cb0b28-e9b2-4c1d-fff2-7c8182401d33"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     18319     22701      9247     12551       14780    10417     2686   \\\n","0     izen     Ether       anza   Symbol         voc   Patton      anza   \n","1     Oath       Hau   harmless     lore  Registered   Fallon       INT   \n","2     Pact   Silence        nap     crit       house       ZA     WHERE   \n","3     ilee     Frame       alez   cyclop          MV       WN      itia   \n","4   Folder      Wire         iP      Hat       Maria    Panel        Mu   \n","5    Berry     Noble     opener      wig        uses   Somers       Blu   \n","6     asin      Ashe        lic     anni        stem     icio       ESC   \n","7      Sys      sans       Duck      Nap          Su     Kahn    Camera   \n","8   Minute       Lug        fav      mil          Mp       VD  eligible   \n","9    ember     Fargo      Chick     Grav     holders    Wilde       RES   \n","\n","       3515         5326         6577   \n","0      emate           FD          nar  \n","1        oqu          SEA         Reid  \n","2       Hast          Ley         idal  \n","3       veto         arte     Marriott  \n","4     eering       Marina          nar  \n","5   orically           FD   resolution  \n","6  SPONSORED        Ferry         prev  \n","7        mol          FTA      channel  \n","8       hast   Underworld          nan  \n","9    Whitman    emulation   resolution  "],"text/html":["\n","  <div id=\"df-87f2f784-1102-4653-9277-b31448b3cbe6\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>18319</th>\n","      <th>22701</th>\n","      <th>9247</th>\n","      <th>12551</th>\n","      <th>14780</th>\n","      <th>10417</th>\n","      <th>2686</th>\n","      <th>3515</th>\n","      <th>5326</th>\n","      <th>6577</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>izen</td>\n","      <td>Ether</td>\n","      <td>anza</td>\n","      <td>Symbol</td>\n","      <td>voc</td>\n","      <td>Patton</td>\n","      <td>anza</td>\n","      <td>emate</td>\n","      <td>FD</td>\n","      <td>nar</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Oath</td>\n","      <td>Hau</td>\n","      <td>harmless</td>\n","      <td>lore</td>\n","      <td>Registered</td>\n","      <td>Fallon</td>\n","      <td>INT</td>\n","      <td>oqu</td>\n","      <td>SEA</td>\n","      <td>Reid</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Pact</td>\n","      <td>Silence</td>\n","      <td>nap</td>\n","      <td>crit</td>\n","      <td>house</td>\n","      <td>ZA</td>\n","      <td>WHERE</td>\n","      <td>Hast</td>\n","      <td>Ley</td>\n","      <td>idal</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>ilee</td>\n","      <td>Frame</td>\n","      <td>alez</td>\n","      <td>cyclop</td>\n","      <td>MV</td>\n","      <td>WN</td>\n","      <td>itia</td>\n","      <td>veto</td>\n","      <td>arte</td>\n","      <td>Marriott</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Folder</td>\n","      <td>Wire</td>\n","      <td>iP</td>\n","      <td>Hat</td>\n","      <td>Maria</td>\n","      <td>Panel</td>\n","      <td>Mu</td>\n","      <td>eering</td>\n","      <td>Marina</td>\n","      <td>nar</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Berry</td>\n","      <td>Noble</td>\n","      <td>opener</td>\n","      <td>wig</td>\n","      <td>uses</td>\n","      <td>Somers</td>\n","      <td>Blu</td>\n","      <td>orically</td>\n","      <td>FD</td>\n","      <td>resolution</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>asin</td>\n","      <td>Ashe</td>\n","      <td>lic</td>\n","      <td>anni</td>\n","      <td>stem</td>\n","      <td>icio</td>\n","      <td>ESC</td>\n","      <td>SPONSORED</td>\n","      <td>Ferry</td>\n","      <td>prev</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Sys</td>\n","      <td>sans</td>\n","      <td>Duck</td>\n","      <td>Nap</td>\n","      <td>Su</td>\n","      <td>Kahn</td>\n","      <td>Camera</td>\n","      <td>mol</td>\n","      <td>FTA</td>\n","      <td>channel</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Minute</td>\n","      <td>Lug</td>\n","      <td>fav</td>\n","      <td>mil</td>\n","      <td>Mp</td>\n","      <td>VD</td>\n","      <td>eligible</td>\n","      <td>hast</td>\n","      <td>Underworld</td>\n","      <td>nan</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>ember</td>\n","      <td>Fargo</td>\n","      <td>Chick</td>\n","      <td>Grav</td>\n","      <td>holders</td>\n","      <td>Wilde</td>\n","      <td>RES</td>\n","      <td>Whitman</td>\n","      <td>emulation</td>\n","      <td>resolution</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-87f2f784-1102-4653-9277-b31448b3cbe6')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-87f2f784-1102-4653-9277-b31448b3cbe6 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-87f2f784-1102-4653-9277-b31448b3cbe6');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-dda31923-33df-4aef-86dd-5915c0656eda\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-dda31923-33df-4aef-86dd-5915c0656eda')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-dda31923-33df-4aef-86dd-5915c0656eda button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","\n","  <div id=\"id_8ed4c8c2-4cc4-43aa-83e2-bd07c35996d4\">\n","    <style>\n","      .colab-df-generate {\n","        background-color: #E8F0FE;\n","        border: none;\n","        border-radius: 50%;\n","        cursor: pointer;\n","        display: none;\n","        fill: #1967D2;\n","        height: 32px;\n","        padding: 0 0 0 0;\n","        width: 32px;\n","      }\n","\n","      .colab-df-generate:hover {\n","        background-color: #E2EBFA;\n","        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","        fill: #174EA6;\n","      }\n","\n","      [theme=dark] .colab-df-generate {\n","        background-color: #3B4455;\n","        fill: #D2E3FC;\n","      }\n","\n","      [theme=dark] .colab-df-generate:hover {\n","        background-color: #434B5C;\n","        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","        fill: #FFFFFF;\n","      }\n","    </style>\n","    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('top_10_logits_df')\"\n","            title=\"Generate code using this dataframe.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n","  </svg>\n","    </button>\n","    <script>\n","      (() => {\n","      const buttonEl =\n","        document.querySelector('#id_8ed4c8c2-4cc4-43aa-83e2-bd07c35996d4 button.colab-df-generate');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      buttonEl.onclick = () => {\n","        google.colab.notebook.generateWithVariable('top_10_logits_df');\n","      }\n","      })();\n","    </script>\n","  </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","variable_name":"top_10_logits_df","summary":"{\n  \"name\": \"top_10_logits_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": 18319,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" Minute\",\n          \" Oath\",\n          \"Berry\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 22701,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" Lug\",\n          \" Hau\",\n          \" Noble\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 9247,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" fav\",\n          \" harmless\",\n          \" opener\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 12551,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"mil\",\n          \"lore\",\n          \"wig\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 14780,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Mp\",\n          \"Registered\",\n          \"uses\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 10417,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"VD\",\n          \" Fallon\",\n          \" Somers\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 2686,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"eligible\",\n          \"INT\",\n          \"Blu\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 3515,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" hast\",\n          \"oqu\",\n          \"orically\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 5326,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \" Underworld\",\n          \"SEA\",\n          \" FD\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": 6577,\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"nan\",\n          \" Reid\",\n          \"resolution\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":5}],"source":["import pandas as pd\n","\n","# Let's start by getting the top 10 logits for each feature\n","\n","sparse_autoencoder = next(iter(sparse_autoencoder_dictionary))[1]\n","projection_onto_unembed = sparse_autoencoder.W_dec @ model.W_U\n","\n","\n","# get the top 10 logits.\n","vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)\n","\n","# get 10 random features\n","random_indices = torch.randint(0, projection_onto_unembed.shape[0], (10,))\n","\n","# Show the top 10 logits promoted by those features\n","top_10_logits_df = pd.DataFrame(\n","    [model.to_str_tokens(i) for i in inds[random_indices]],\n","    index=random_indices.tolist(),\n",").T\n","top_10_logits_df"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Let's start by getting the top 10 logits for each feature\n","\n","sparse_autoencoder = next(iter(sparse_autoencoder_dictionary_2))[1]\n","projection_onto_unembed = sparse_autoencoder.W_dec @ model.W_U\n","\n","\n","# get the top 10 logits.\n","vals, inds = torch.topk(projection_onto_unembed, 10, dim=1)\n","\n","# get 10 random features\n","random_indices = torch.randint(0, projection_onto_unembed.shape[0], (10,))\n","\n","# Show the top 10 logits promoted by those features\n","top_10_logits_df = pd.DataFrame(\n","    [model.to_str_tokens(i) for i in inds[random_indices]],\n","    index=random_indices.tolist(),\n",").T\n","top_10_logits_df"],"metadata":{"id":"q4eUo6A0RTyZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load SAE"],"metadata":{"id":"F1XTijzD_sYb"}},{"cell_type":"code","source":["# run = wandb.init(project=\"sae_lens_exploraTest_L5\", id=\"cguht60b\", resume=\"allow\")\n","# artifact = run.use_artifact('wlg100/sae_lens_exploraTest_L5/model:v0', type='model')\n","# model_dir = artifact.download()\n","\n","# # Load your model from the downloaded directory\n","# import torch\n","# model = torch.load(model_dir + \"/model.pth\")"],"metadata":{"id":"XE47NtFz_wWa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from sae_lens import LMSparseAutoencoderSessionloader\n","\n","# path =\"path/to/sparse_autoencoder.pt\"\n","# model, sparse_autoencoder, activations_loader = LMSparseAutoencoderSessionloader.load_session_from_pretrained(\n","#     path\n","# )"],"metadata":{"id":"PGTAcbfYBLWE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Steering Vector decomposition"],"metadata":{"id":"w2Wt05M-lofM"}},{"cell_type":"code","source":["# do this b/c anger is one token, calm is 2, so this pads anger with 50256\n","batch_input = [\"anger\", \"calm\"]\n","tokens = model.to_tokens(batch_input)\n","tokens"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuF5CoJOVVir","executionInfo":{"status":"ok","timestamp":1715860318516,"user_tz":240,"elapsed":525,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"952a5069-3267-4c16-b8bb-911a50f8a270"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[50256,  2564, 50256],\n","        [50256,  9948,    76]], device='cuda:0')"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["import torch as t"],"metadata":{"id":"e_jBumkFMbhX","executionInfo":{"status":"ok","timestamp":1715859476543,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["layer_name = 'blocks.5.mlp.hook_pre'"],"metadata":{"id":"mdm9eJRWMrBv","executionInfo":{"status":"ok","timestamp":1715860565588,"user_tz":240,"elapsed":12,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":51,"outputs":[]},{"cell_type":"code","source":["model.cfg.d_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M_o239wUPc7m","executionInfo":{"status":"ok","timestamp":1715860279698,"user_tz":240,"elapsed":11,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0d25d1cb-f171-4dd7-d838-15adc533625d"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1600"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["from jaxtyping import Float, Int\n","from torch import nn, Tensor"],"metadata":{"id":"ZDH3kqsHM0Be","executionInfo":{"status":"ok","timestamp":1715860061703,"user_tz":240,"elapsed":363,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def store_h_hook(\n","    pattern: Float[Tensor, \"batch seqlen d_mlp\"],\n","    # hook: HookPoint,\n","    hook\n","):\n","    # Store the result.\n","    # h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored\n","    # h_store.append(1) # if h_store = [], this will work\n","    h_store[:] = pattern  # this works b/c changes values, not replaces entire thing"],"metadata":{"id":"_yhMkZ2WMqep","executionInfo":{"status":"ok","timestamp":1715860500691,"user_tz":240,"elapsed":418,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":46,"outputs":[]},{"cell_type":"code","source":["seqLen = tokens.shape[1]\n","h_store = t.zeros((1, seqLen, model.cfg.d_mlp), device=model.cfg.device)\n","# h_store = t.zeros((len(batch_input), seqLen, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    tokens[0],\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")\n","\n","neg_h = t.clone(h_store)"],"metadata":{"id":"1DrKWZPXlpdM","colab":{"base_uri":"https://localhost:8080/","height":347},"executionInfo":{"status":"error","timestamp":1715860575595,"user_tz":240,"elapsed":50,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b6ebd4a6-be31-4bfc-98fc-0b2c5afb7fa9"},"execution_count":53,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The expanded size of the tensor (1600) must match the existing size (6400) at non-singleton dimension 2.  Target sizes: [2, 3, 1600].  Tensor sizes: [3, 6400]","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-53-bd5f2937361a>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mh_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseqLen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model.run_with_hooks(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mreturn_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_hooks_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_contexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     def add_caching_hooks(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    548\u001b[0m                     )\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    551\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1599\u001b[0m             )\n\u001b[1;32m   1600\u001b[0m             \u001b[0mnormalized_resid_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmlp_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1601\u001b[0;31m             \u001b[0mmlp_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_mlp_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormalized_resid_mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1602\u001b[0m             \u001b[0mresid_post\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhook_resid_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresid_mid\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmlp_out\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch, pos, d_model]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1603\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel_attn_mlp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     ) -> Float[torch.Tensor, \"batch pos d_model\"]:\n\u001b[1;32m   1271\u001b[0m         \u001b[0;31m# Technically, all these einsums could be done with a single matmul, but this is more readable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1272\u001b[0;31m         pre_act = self.hook_pre(\n\u001b[0m\u001b[1;32m   1273\u001b[0m             \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"batch pos d_model, d_model d_mlp -> batch pos d_mlp\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_in\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         )  # [batch, pos, d_mlp]\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1581\u001b[0;31m                         \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mhook_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mfull_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     75\u001b[0m             ):  # For a backwards hook, module_output is a tuple of (grad,) - I don't know why.\n\u001b[1;32m     76\u001b[0m                 \u001b[0mmodule_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         full_hook.__name__ = (\n","\u001b[0;32m<ipython-input-46-08f7ff908587>\u001b[0m in \u001b[0;36mstore_h_hook\u001b[0;34m(pattern, hook)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# h_store.append(1) # if h_store = [], this will work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mh_store\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpattern\u001b[0m  \u001b[0;31m# this works b/c changes values, not replaces entire thing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (1600) must match the existing size (6400) at non-singleton dimension 2.  Target sizes: [2, 3, 1600].  Tensor sizes: [3, 6400]"]}]},{"cell_type":"code","source":["seqLen = tokens.shape[1]\n","h_store = t.zeros((1, seqLen, model.cfg.d_mlp), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    tokens[1],\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")\n","\n","pos_h = t.clone(h_store)"],"metadata":{"id":"nnfc91dqU6da","executionInfo":{"status":"ok","timestamp":1715860070618,"user_tz":240,"elapsed":402,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["steer_vec = neg_h - pos_h\n","steer_vec.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_cODPH6aVBcj","executionInfo":{"status":"ok","timestamp":1715860072914,"user_tz":240,"elapsed":43,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"12223e6d-65c7-41a4-a215-b8f1aa47b679"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 6400])"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["import einops"],"metadata":{"id":"8gXBTMMbOvar","executionInfo":{"status":"ok","timestamp":1715860081907,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(steer_vec, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n","post_reshaped.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OW6bPnhcfyCZ","executionInfo":{"status":"ok","timestamp":1715860083649,"user_tz":240,"elapsed":43,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"113646d8-897a-4c13-8d54-ef612fc66e88"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2, 6400])"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["# sparse_autoencoder = next(iter(sparse_autoencoder_dictionary))[1]\n","sparse_autoencoder(post_reshaped)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":311},"id":"2qXfNW5LPAUG","executionInfo":{"status":"error","timestamp":1715860227326,"user_tz":240,"elapsed":12,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"bb572aee-3ccc-4c06-f833-4188975c4d41"},"execution_count":28,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (6400) must match the size of tensor b (1600) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-9d870f450d03>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sparse_autoencoder = next(iter(sparse_autoencoder_dictionary))[1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msparse_autoencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/sparse_autoencoder.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, dead_neuron_mask)\u001b[0m\n\u001b[1;32m    175\u001b[0m     ) -> ForwardOutput:\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mfeature_acts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_pre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_with_hidden_pre\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0msae_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_acts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sae_lens/training/sparse_autoencoder.py\u001b[0m in \u001b[0;36m_encode_with_hidden_pre\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         sae_in = self.hook_sae_in(\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb_dec\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_b_dec_to_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         )  # Remove decoder bias as per Anthropic\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6400) must match the size of tensor b (1600) at non-singleton dimension 2"]}]},{"cell_type":"code","source":["sparse_autoencoder_dictionary(post_reshaped)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":144},"id":"Kve22QVcO8OG","executionInfo":{"status":"error","timestamp":1715860139891,"user_tz":240,"elapsed":60,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"a7d0bc1c-9cfd-4402-fd6e-a1ebdece9511"},"execution_count":23,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"'SparseAutoencoderDictionary' object is not callable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-5cf0eebd3604>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msparse_autoencoder_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mTypeError\u001b[0m: 'SparseAutoencoderDictionary' object is not callable"]}]},{"cell_type":"code","source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","# output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":198},"id":"PXnogf1PXu0E","executionInfo":{"status":"error","timestamp":1715860123477,"user_tz":240,"elapsed":79,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b9ad459d-b9b2-4628-f7e1-32b198d0b8c3"},"execution_count":21,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'SparseAutoencoderDictionary' object has no attribute 'forward'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-fd403b906374>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use a fwd pass to compute ALL feature actvs for ALL this steering vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moutput_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msparse_autoencoder_dictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpost_reshaped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0macts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_tuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'SparseAutoencoderDictionary' object has no attribute 'forward'"]}]},{"cell_type":"code","source":["# Get the top k largest activations for feature neurons, not batch seq. use , dim=-1\n","feat_k = 5\n","top_acts_values, top_acts_indices = acts.topk(feat_k, dim=-1)"],"metadata":{"id":"CqaQZ6q_ftd_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["top_acts_indices"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D__HQHXtfFKO","executionInfo":{"status":"ok","timestamp":1714523147660,"user_tz":240,"elapsed":39,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c8a8c059-4458-4eac-ec5d-3ff834da7c25"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[  21, 3622, 3274, 4597, 1279],\n","         [ 144, 3016, 1272, 1681,  917]],\n","\n","        [[ 939, 4847,  594, 5196, 1176],\n","         [ 623, 3874, 4619,  663, 1692]],\n","\n","        [[   1,    0,    2,    4,    3],\n","         [   1,    0,    2,    4,    3]]], device='cuda:0')"]},"metadata":{},"execution_count":74}]},{"cell_type":"code","source":["all_tokens = model.to_tokens(batch_input, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rgnxAj8Eii4c","executionInfo":{"status":"ok","timestamp":1714523539562,"user_tz":240,"elapsed":240,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c50b61da-d55f-4a1d-ab36-a78cb8168b6a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([100, 34])"]},"metadata":{},"execution_count":95}]},{"cell_type":"code","source":["# get top samp_m tokens for all top feat_k feature neurons\n","samp_m = 5\n","for feature_idx in top_acts_indices[0][0]:\n","    feature_idx = feature_idx.item()\n","    ds_top_acts_indices, ds_top_acts_values = highest_activating_tokens(all_tokens, model, autoencoder, feature_idx,\n","                                                            autoencoder_B=False, k=samp_m, layer_name=layer_name)\n","    display_top_sequences(ds_top_acts_indices, ds_top_acts_values, all_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":817},"id":"IKGqt_lEiXWC","executionInfo":{"status":"ok","timestamp":1714523543055,"user_tz":240,"elapsed":249,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3146ca80-b474-471a-cb6e-4a51cb30586f"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[3m           Tokens which most activate feature 21            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.01      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.04      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mAnarchists in                  │ -0.11      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mFor today��                    │ -0.11      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mA magazine supplement with     │ -0.11      │\n","└─────────────────────────────────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">           Tokens which most activate feature 21            </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.01      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.04      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>Anarchists in                  │ -0.11      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>For today��                    │ -0.11      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>A magazine supplement with     │ -0.11      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m          Tokens which most activate feature 3622           \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.04      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.07      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mAnarchists in                  │ -0.12      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mFor today��                    │ -0.12      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mA magazine supplement with     │ -0.12      │\n","└─────────────────────────────────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Tokens which most activate feature 3622           </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.04      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.07      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>Anarchists in                  │ -0.12      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>For today��                    │ -0.12      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>A magazine supplement with     │ -0.12      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m                           Tokens which most activate feature 3274                            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                                                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\"                                   │ -0.03      │\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)                                    │ -0.04      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e                                       │ -0.09      │\n","│  of the Johns Hopkins Bl\u001b[1;4;38;5;208mo\u001b[0m<|endoftext|><|endoftext|><|endoftext|><|endoftext|> │ -0.09      │\n","│  the former J.L\u001b[1;4;38;5;208m.\u001b[0m Hudson��s                                                    │ -0.09      │\n","└───────────────────────────────────────────────────────────────────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                           Tokens which most activate feature 3274                            </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                                                      </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\"                                   │ -0.03      │\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)                                    │ -0.04      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e                                       │ -0.09      │\n","│  of the Johns Hopkins Bl<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">o</span>&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt; │ -0.09      │\n","│  the former J.L<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">.</span> Hudson��s                                                    │ -0.09      │\n","└───────────────────────────────────────────────────────────────────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m            Tokens which most activate feature 4597            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)     │ -0.05      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\"    │ -0.06      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e        │ -0.14      │\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked     │ -0.15      │\n","│  The world's best make\u001b[1;4;38;5;208m-\u001b[0mup artists reveal their │ -0.18      │\n","└────────────────────────────────────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Tokens which most activate feature 4597            </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)     │ -0.05      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\"    │ -0.06      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e        │ -0.14      │\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked     │ -0.15      │\n","│  The world's best make<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>up artists reveal their │ -0.18      │\n","└────────────────────────────────────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m          Tokens which most activate feature 1279           \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.00      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.05      │\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked  │ -0.10      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mFor today��                    │ -0.12      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mA magazine supplement with     │ -0.12      │\n","└─────────────────────────────────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Tokens which most activate feature 1279           </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.00      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.05      │\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked  │ -0.10      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>For today��                    │ -0.12      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>A magazine supplement with     │ -0.12      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}}]}]}