{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNpP5anp/5V4iwrwHxShc7r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5I3Ss__1VVEZ"},"outputs":[],"source":["import torch\n","import numpy as np\n","from sklearn.datasets import make_blobs, make_circles\n","from scipy.stats import norm\n","from tqdm import tqdm\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# %pip install einops\n","# from einops import einsum"]},{"cell_type":"code","source":["def get_ground_truth_feats(\n","    distrb_type: str = 'unif',\n","    model_dims: int = 256,\n","    feature_dims: int = 512,\n","    device: torch.device = torch.device('cpu'),\n","    num_clusters: int = 10,\n",") -> torch.Tensor:\n","    dtype = torch.float32\n","\n","    if distrb_type == 'unif':\n","        synth_features = torch.randn(model_dims, feature_dims, device=device, dtype=dtype)\n","    else:\n","        cluster_data, _ = make_blobs(n_samples=model_dims, centers=num_clusters, n_features=feature_dims)\n","        synth_features = torch.tensor(cluster_data, device=device, dtype=dtype)\n","    return synth_features"],"metadata":{"id":"Tdr-XiFEGDgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_synth_actvs(\n","        synth_features: torch.Tensor,\n","        total_data_points: int = 100000,\n","        avg_active_features: int = 16,\n","        batch_size: int = 1000\n","    ) -> torch.Tensor:\n","    device = synth_features.device\n","    dtype = torch.float32\n","\n","    h = synth_features.shape[0]  # model dimensions\n","    G = synth_features.shape[1]  # number of ground truth features\n","\n","    # created a random covariance matrix for a multivariate normal distribution with zero mean\n","    A = np.random.rand(G, G)  # rand correlations for each feature\n","    cov_matrix = np.dot(A, A.transpose())\n","    mean = np.zeros(G)\n","\n","    synth_actvs_batches = []\n","    for _ in tqdm(range(0, total_data_points, batch_size), desc=\"Generating Batches\"):\n","        # 1. Correlated: for each feature in sample vector of size G\n","        # a single sample from a correlated multivariate normal distribution and,\n","        batch_size_current = min(batch_size, total_data_points - len(synth_actvs_batches))\n","        # samples are on a scale defined by the normal distribution's probability density fn (PDF)\n","        samples = np.random.multivariate_normal(mean, cov_matrix, batch_size_current) # (batchNumSamps, G) with correlations\n","\n","        # for each dimension of that sample, found where that sample lay on the standard normal cumulative distribution function\n","        uniform_samples = norm.cdf(samples)  # (batchNumSamps, G) is where each samp lies on (0,1) range in cumulative dist fn (CDF)\n","\n","        # 2. Decayed: for each feature in sample vector of size G\n","        # probability of the G-dimensional random variable exponentially decayed with the feature’s index\n","        decay_rate = 0.99  # lambda (put this in loop for ease of code reading)\n","        indices = np.arange(G)\n","        decayed_probs = uniform_samples ** (indices * decay_rate)  # prob of each samp's feature expo decays to power of ind*0.99\n","\n","        # 3. Rescaled: for each feature in sample vector of size G\n","        # Rescale probabilities to ensure on avg only \"avg_active_features\" num of ground truth features are active at a time.\n","        # this changes the avg so (avg_active_features / G) are active\n","        # scaling_factor: denom is what to cancel out (replace) and numer is what to replace with\n","        mean_prob = np.mean(decayed_probs) #  calculated the mean probability of all features\n","        scaling_factor = (avg_active_features / G) / mean_prob #  calculated the ratio of the number of ground truth features that are active at a time to the mean probability\n","        rescaled_probs = decayed_probs * scaling_factor # multiplied each probability by this ratio to rescale them\n","        rescaled_probs_tensor = torch.tensor(rescaled_probs, device=device, dtype=dtype)\n","\n","        # 4. parameterize a vector of Bernoulli random variables (for sparse coefficients):\n","        # want expectation of this vector to have \"avg_active_features\" 1s\n","        # given probs for each index, bernoulli draws a vector of 0s and 1s using those probs\n","        binary_sparse_coeffs = torch.bernoulli(rescaled_probs_tensor)\n","\n","        # 5. use the sparse coefficients to linearly combine a sparse selection of the ground truth features\n","        synth_activations = torch.matmul(binary_sparse_coeffs, synth_features.T.to(dtype))\n","        # synth_activations = torch.einsum('ij,kj->ik', binary_sparse_coeffs, synth_features.to(dtype))\n","\n","        synth_actvs_batches.append(synth_activations)\n","\n","    return torch.cat(synth_actvs_batches, dim=0)  # stack batches along rows (dim=0)"],"metadata":{"id":"gXqOmVxLQIgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["grTrue_feats = get_ground_truth_feats('unif', 256, 512, device) # hxG\n","total_data_points = 10000000\n","avg_active_features = 32\n","\n","synth_activations = get_synth_actvs(grTrue_feats, total_data_points, avg_active_features)\n","print('\\n', synth_activations.shape)"],"metadata":{"id":"MwAavBmFn8HN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723044726697,"user_tz":-60,"elapsed":269077,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0d834e80-de51-47c7-db73-abc3a2f51f81"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Generating Batches: 100%|██████████| 1000/1000 [04:28<00:00,  3.72it/s]"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1000000, 256])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}