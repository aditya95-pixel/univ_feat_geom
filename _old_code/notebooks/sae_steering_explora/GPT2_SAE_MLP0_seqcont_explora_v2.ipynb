{"cells":[{"cell_type":"markdown","metadata":{"id":"JZRbtlLMLflI"},"source":["This notebook obtains the activations of GPT-2 MLP0 and passes tokens belonging to ordered sequences through them.\n","\n","We use this code instead of SAElens as the code for the SAEs is more explicitly shown in here."]},{"cell_type":"markdown","metadata":{"id":"hWzRfaBQu1_s"},"source":["\n","\n","> we perform a case study on the attention head (L12H0) with the maximal\n","successor score in Pythia-1.4B\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tjZ9z85eNwLm"},"source":["GPT-2 small has 124M parameters."]},{"cell_type":"markdown","metadata":{"id":"-WMqbetGrf4e"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hozpKjBenAph"},"outputs":[],"source":["%%capture\n","!pip install transformer_lens\n","!pip install datasets\n","!pip install zstandard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llAiwjkqwpqp"},"outputs":[],"source":["from transformer_lens import utils, HookedTransformer, ActivationCache\n","from dataclasses import dataclass\n","import torch as t\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from jaxtyping import Float, Int\n","from typing import Optional, Callable, Union, List, Tuple\n","import einops\n","from datasets import load_dataset\n","\n","from tqdm import tqdm\n","from rich.table import Table\n","from rich import print as rprint"]},{"cell_type":"markdown","metadata":{"id":"2LxnTmFDpb2z"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaLmq0KDockd"},"outputs":[],"source":["device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":417,"referenced_widgets":["c6be004161254063a334f491462a588b","3fc027b52b2f4f96ba92bbf081e02d8e","c4991bbe902449d3be1badb035a3af2e","f749e14c5927432080b00b06167f11a1","2d09609d0df047c581bffd0334f19fa3","16c26fef58a14265b31c0ccf874b2e31","a8c65c8ae4a34f2aa1c9bbef106b7e7e","66c6611c971e4d2d9787d7c550b1306d","b5b0dd95a26e47bebc0177377962b6b6","f577b79eeeef4e09b32fa9e6ec7b25c2","a75a3d6ef2e34c2e8c1b82eba305bc8c","5ef072d8003e475f9e6031163b032881","bbd8642f8bbf4f3da9dbd0abcfbcf1cf","d06343bcc0d04616b581667b01e8959e","08570c41a4b54317915f5fd564386aa5","0d878e98656f4898a3252ceb34737686","6f6d110dd66f446f8ab4ccc632e7277a","a982948245f4436dbf70cd1c06c971f4","e1046c659d9f4e6c930495259b39c01f","7933aae0deb3434aa3fb0d13da37fa95","238742d2c4cb44bab11a1c814e11030f","14139f4f3f5a4c59b3d3cfee3bde908e","1e878b7eea2645198b9b3076c0eb1934","0561dbb9a9084061bdcbe2c4a0092ccd","17838ce0847d4003bdf05d2aa26c9b80","d317338ba49649aa8357c2f2e87a7dd1","c15720aa633246a98b9d4ba477da6014","73b2e3a84dea4118b4893abbfd48830d","0e28f3373eab4fdfb97303a95a9f0491","ee9561eb6c654a438328bf707bd5701e","bcd4601a5d2e4d0d96a96bde08d65d28","7b204e99f3924f99b23ff19be25156f7","6b6091271c114dcc8e5ea22ce37e8fa3","492f1eb2fb6d4adabddc82eaeb1902b1","abe75850e9c54bd98213b677411c6d99","bfe7959b4de24da284ad23363bea3c9f","cab1736352924c4dacf00811cf193872","8b08790c1d82430e98acc87a0ecd6825","75d9c56d3b4d40959fbfb93baa5ae40f","7fc9a44b78674a86acc94e587b9e1454","f4abd280b1a5463e8bc9736f57971c90","275336b09d6743ba8e550cf32001db6c","459fdb1dfaf44258b6f0dcac98e4ce2e","135800d65f64410e86cf0c93044b13b4","0c43619bff6e4dc289d6f437f965953b","4d7585bf748e463bbdaf30827196ea1a","4b0fb3bcdf3743eea30dcfc633d11383","ee71f19b20b7460fa0a68ef17321dffb","ac7c472b680043f2b39f87bc77a92a30","8fb456119c464ff4b29fa01f0ce6caf6","a9e9e7c8265b4d4c9673931cc265c6db","b7750a1387a843f0b799b6f01a7bca7d","7b4c9cde6bd14221974693b9dfd3e6c2","95e9abbc8977434b8abd5d3ff6ccb111","cfe54412268d4ce6b38d761f3878ee77","9efa5d9dfd99462c9b0f3f97a01db879","54837d43fd01482d9fe7ad8685e1b390","aa1b95d048a14fb7a054a0e8e3e80211","07b2fe72c2cb44aa9f4d7054423a30e4","45f3b59c010a4f64908bdcd02e13d423","f8d5c7d511534271a1b8915bf9ccc60f","4364d6282b284e4f9cc8312d61cb79ad","9aea74abb1e34e1ba35ebe74b6120d54","55fd0463f99f422a9fcaf3657b948ed1","b44acad5c2244394a572ab23967f1edd","129941992f91426b887131ffdc313dab","66b5c87644114653b59980888d66add8","5173f6d88dc34dc889793ccdb351490a","f728f9fd0e1f4c71a849b03d4b6f3cec","d2358c69a6904c3a8016c785dea40422","2d296c7e813d46308d959f45eabee25b","df0649af8d5c402299a71f83eb804096","0e10d5eb56a44fbaae986a60f74a499e","7f4a7d668ab745819245e58ad2e8dfd2","16e1db2ebdbc4774a256d1c04a89ce09","546b8eb58000427b8efdfbda37627b49","3960f260fdc44293b2a0b6457e2f61a5"]},"executionInfo":{"elapsed":19876,"status":"ok","timestamp":1716366175455,"user":{"displayName":"mike lan","userId":"00221534718597437140"},"user_tz":240},"id":"zjFa1PqDrOto","outputId":"5b52768e-e123-4fd7-ec99-f7ece6d0b2f2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6be004161254063a334f491462a588b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ef072d8003e475f9e6031163b032881"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e878b7eea2645198b9b3076c0eb1934"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492f1eb2fb6d4adabddc82eaeb1902b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c43619bff6e4dc289d6f437f965953b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9efa5d9dfd99462c9b0f3f97a01db879"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"66b5c87644114653b59980888d66add8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-medium into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    # \"gpt2-small\",\n","    \"gpt2-medium\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"2MD88v4Zvw-r"},"source":["# Autoencoder Training"]},{"cell_type":"markdown","metadata":{"id":"1jCAYFKmz92O"},"source":["## Class Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjziwoIb4Tvb"},"outputs":[],"source":["@dataclass\n","class AutoEncoderConfig:\n","    n_instances: int\n","    n_input_ae: int\n","    n_hidden_ae: int\n","    l1_coeff: float = 0.5\n","    tied_weights: bool = False\n","    weight_normalize_eps: float = 1e-8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRNZvnQQ6wwS"},"outputs":[],"source":["def linear_lr(step, steps):\n","    return (1 - (step / steps))\n","\n","def constant_lr(*_):\n","    return 1.0\n","\n","def cosine_decay_lr(step, steps):\n","    return np.cos(0.5 * np.pi * step / (steps - 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CNZX3tR3j8h"},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","    W_enc: Float[Tensor, \"n_instances n_input_ae n_hidden_ae\"]\n","    W_dec: Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]\n","    b_enc: Float[Tensor, \"n_instances n_hidden_ae\"]\n","    b_dec: Float[Tensor, \"n_instances n_input_ae\"]\n","\n","    def __init__(self, cfg: AutoEncoderConfig, h):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        self.model_h = h\n","\n","        self.W_enc = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_input_ae, cfg.n_hidden_ae))))\n","        if not(cfg.tied_weights):\n","            self.W_dec = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden_ae, cfg.n_input_ae))))\n","\n","        self.b_enc = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden_ae))\n","        self.b_dec = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_input_ae))\n","\n","        self.to(device)\n","\n","    def normalize_and_return_W_dec(self) -> Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]:\n","        '''\n","        If self.cfg.tied_weights = True, we return the normalized & transposed encoder weights.\n","        If self.cfg.tied_weights = False, we normalize the decoder weights in-place, and return them.\n","\n","        Normalization should be over the `n_input_ae` dimension, i.e. each feature should have a noramlized decoder weight.\n","        '''\n","        if self.cfg.tied_weights:\n","            return self.W_enc.transpose(-1, -2) / (self.W_enc.transpose(-1, -2).norm(dim=1, keepdim=True) + self.cfg.weight_normalize_eps)\n","        else:\n","            self.W_dec.data = self.W_dec.data / (self.W_dec.data.norm(dim=2, keepdim=True) + self.cfg.weight_normalize_eps)\n","            return self.W_dec\n","\n","    def forward(self, h: Float[Tensor, \"batch_size n_instances n_input_ae\"]):\n","\n","        # Compute activations\n","        h_cent = h - self.b_dec\n","        acts = einops.einsum(\n","            h_cent, self.W_enc,\n","            \"batch_size n_instances n_input_ae, n_instances n_input_ae n_hidden_ae -> batch_size n_instances n_hidden_ae\"\n","        )\n","        acts = F.relu(acts + self.b_enc)\n","\n","        # Compute reconstructed input\n","        h_reconstructed = einops.einsum(\n","            acts, self.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + self.b_dec\n","\n","        # Compute loss, return values\n","        l2_loss = (h_reconstructed - h).pow(2).mean(-1) # shape [batch_size n_instances]\n","        l1_loss = acts.abs().sum(-1) # shape [batch_size n_instances]\n","        loss = (self.cfg.l1_coeff * l1_loss + l2_loss).mean(0).sum() # scalar\n","\n","        return l1_loss, l2_loss, loss, acts, h_reconstructed\n","\n","    @t.no_grad()\n","    def resample_neurons(\n","        self,\n","        h: Float[Tensor, \"batch_size n_instances n_input_ae\"],\n","        frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n","        neuron_resample_scale: float,\n","    ) -> Tuple[List[List[str]], str]:\n","        '''\n","        Resamples neurons that have been dead for `dead_neuron_window` steps, according to `frac_active`.\n","        '''\n","        pass # See below for a solution to this function\n","\n","    def optimize(\n","        self,\n","        # model: Model,\n","        batch_size: int = 1024,\n","        steps: int = 10_000,\n","        log_freq: int = 100,\n","        lr: float = 1e-3,\n","        lr_scale: Callable[[int, int], float] = constant_lr,\n","        neuron_resample_window: Optional[int] = None,\n","        dead_neuron_window: Optional[int] = None,\n","        neuron_resample_scale: float = 0.2,\n","    ):\n","        '''\n","        Optimizes the autoencoder using the given hyperparameters.\n","\n","        This function should take a trained model as input.\n","        '''\n","        if neuron_resample_window is not None:\n","            assert (dead_neuron_window is not None) and (dead_neuron_window < neuron_resample_window)\n","\n","        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n","        frac_active_list = []\n","        progress_bar = tqdm(range(steps))\n","\n","        # Create lists to store data we'll eventually be plotting\n","        data_log = {\"W_enc\": [], \"W_dec\": [], \"colors\": [], \"titles\": [], \"frac_active\": []}\n","        colors = None\n","        title = \"no resampling yet\"\n","\n","        for step in progress_bar:\n","\n","            # Resample dead neurons\n","            # if (neuron_resample_window is not None) and ((step + 1) % neuron_resample_window == 0):\n","            #     # Get the fraction of neurons active in the previous window\n","            #     frac_active_in_window = t.stack(frac_active_list[-neuron_resample_window:], dim=0)\n","            #     # Compute batch of hidden activations which we'll use in resampling\n","            #     batch = model.generate_batch(batch_size)\n","            #     h = einops.einsum(\n","            #         batch, model.W,\n","            #         \"batch_size instances features, instances hidden features -> batch_size instances hidden\"\n","            #     )\n","            #     # Resample\n","            #     colors, title = self.resample_neurons(h, frac_active_in_window, neuron_resample_scale)\n","\n","            # Update learning rate\n","            step_lr = lr * lr_scale(step, steps)\n","            for group in optimizer.param_groups:\n","                group['lr'] = step_lr\n","\n","            ### MODIFY THIS to use h,  activations from transformerlens ###\n","            # Get a batch of hidden activations from the model\n","            # with t.inference_mode():\n","                # features = model.generate_batch(batch_size)\n","                # h = einops.einsum(\n","                #     features, model.W,\n","                #     \"... instances features, instances hidden features -> ... instances hidden\"\n","                # )\n","\n","            h = self.model_h\n","\n","            # Optimize\n","            l1_loss, l2_loss, loss, acts, _ = self.forward(h)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Calculate the mean sparsities over batch dim for each (instance, feature)\n","            frac_active = (acts.abs() > 1e-8).float().mean(0)\n","            frac_active_list.append(frac_active)\n","\n","            # Display progress bar, and append new values for plotting\n","            if step % log_freq == 0 or (step + 1 == steps):\n","                progress_bar.set_postfix(l1_loss=self.cfg.l1_coeff * l1_loss.mean(0).sum().item(), l2_loss=l2_loss.mean(0).sum().item(), lr=step_lr)\n","                data_log[\"W_enc\"].append(self.W_enc.detach().cpu().clone())\n","                data_log[\"W_dec\"].append(self.normalize_and_return_W_dec().detach().cpu().clone())\n","                data_log[\"colors\"].append(colors)\n","                data_log[\"titles\"].append(f\"Step {step}/{steps}: {title}\")\n","                data_log[\"frac_active\"].append(frac_active.detach().cpu().clone())\n","\n","        return data_log"]},{"cell_type":"markdown","metadata":{"id":"CF4mUxhMvw-s"},"source":["Return a dictionary `data_log` containing data which is useful for visualizing the training process"]},{"cell_type":"markdown","metadata":{"id":"TioC1OirumVa"},"source":["## Training data"]},{"cell_type":"markdown","metadata":{"id":"puaa0aYEIvK7"},"source":["> We train the SAE using number tokens from 0 to 500, both with and without a space (‘123’ and\n","‘ 123’), alongside other tasks, such as number words, cardinal words, days, months, etc. 90% of\n","these tokens go into the train set, and the remaining 10% to the validation set. Even with the other\n","tasks, the dataset is dominated by numbers, but creating a more balanced dataset would give us less\n","data to work with, and without enough data, the SAE fails to generalize to the validation set. Hence,\n","we only concern ourselves with the features that the SAE learns for number tokens, and we then\n","separately check whether these features generalize to the other tasks on the basis of logits, rather\n","than SAE activations.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTHX63k9IyR2"},"outputs":[],"source":["input_as_str = [str(i) for i in range(500)]"]},{"cell_type":"markdown","metadata":{"id":"TEaFrpYW9z3v"},"source":["## Get activations to train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-ovsHYQ9z31"},"outputs":[],"source":["layer_name = 'blocks.0.hook_resid_post'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1716366685208,"user":{"displayName":"mike lan","userId":"00221534718597437140"},"user_tz":240},"id":"QoTE9eI09z31","outputId":"754a31e6-186d-40eb-eae8-4b674460ea87"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2])"]},"metadata":{},"execution_count":17}],"source":["# https://neelnanda-io.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformer.html\n","\n","tokens = model.to_tokens(input_as_str)\n","tokens.shape"]},{"cell_type":"markdown","metadata":{"id":"1qsJO-lp9z31"},"source":["Seq Len is number of tokens, not string max len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHwEo2BB9z31"},"outputs":[],"source":["# h_store = t.zeros(model_cache['blocks.5.mlp.hook_post'].shape, device=model.cfg.device)\n","seqLen = tokens.shape[1]\n","h_store = t.zeros((len(input_as_str), seqLen, model.cfg.d_model), device=model.cfg.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1716366685208,"user":{"displayName":"mike lan","userId":"00221534718597437140"},"user_tz":240},"id":"e583Pfgr9z31","outputId":"4ecb006d-d51d-402f-c357-120ae7a19ea9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2, 1024])"]},"metadata":{},"execution_count":19}],"source":["h_store.shape"]},{"cell_type":"markdown","metadata":{"id":"ZGBISlgk9z31"},"source":["Use hook fn to avoid storing all activations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNWEV_Mn9z31"},"outputs":[],"source":["def store_h_hook(\n","    pattern: Float[Tensor, \"batch seqlen d_model\"],\n","    # hook: HookPoint,\n","    hook\n","):\n","    # Store the result.\n","    # h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored\n","    # h_store.append(1) # if h_store = [], this will work\n","    h_store[:] = pattern  # this works b/c changes values, not replaces entire thing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":5,"status":"error","timestamp":1716366685747,"user":{"displayName":"mike lan","userId":"00221534718597437140"},"user_tz":240},"id":"_wfOCpZB9z32","colab":{"base_uri":"https://localhost:8080/","height":376},"outputId":"dfeaed57-4d2a-4310-9677-0c3e8dfa7930"},"outputs":[{"output_type":"error","ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU ","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-7dfb32585a5e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m model.run_with_hooks(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mreturn_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     fwd_hooks=[\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstore_h_hook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/hook_points.py\u001b[0m in \u001b[0;36mrun_with_hooks\u001b[0;34m(self, fwd_hooks, bwd_hooks, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbwd_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_hooks_end\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_contexts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mhooked_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     def add_caching_hooks(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/HookedTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, padding_side, start_at_layer, tokens, shortformer_pos_embed, attention_mask, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    548\u001b[0m                     )\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m                 residual = block(\n\u001b[0m\u001b[1;32m    551\u001b[0m                     \u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m                     \u001b[0;31m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry, attention_mask)\u001b[0m\n\u001b[1;32m   1583\u001b[0m             \u001b[0;31m# queries, keys and values, independently.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1584\u001b[0m             \u001b[0;31m# Then take the layer norm of these inputs, and pass these to the attention module.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1585\u001b[0;31m             self.attn(\n\u001b[0m\u001b[1;32m   1586\u001b[0m                 \u001b[0mquery_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mshortformer_pos_embed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query_input, key_input, value_input, past_kv_cache_entry, additive_attention_mask, attention_mask)\u001b[0m\n\u001b[1;32m    533\u001b[0m         \"\"\"\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 535\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_qkv_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_kv_cache_entry\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformer_lens/components.py\u001b[0m in \u001b[0;36mcalculate_qkv_matrices\u001b[0;34m(self, query_input, key_input, value_input)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             k = self.hook_k(\n\u001b[0;32m--> 705\u001b[0;31m                 einsum(\n\u001b[0m\u001b[1;32m    706\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mqkv_einops_string\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_model\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                     \u001b[0;34m->\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0mpos\u001b[0m \u001b[0mhead_index\u001b[0m \u001b[0md_head\u001b[0m\u001b[0;31m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(equation, *operands)\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0mnew_equation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_equation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_equation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/fancy_einsum/__init__.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(self, equation, *operands)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "]}],"source":["model.run_with_hooks(\n","    tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPX3yC8w9z32"},"outputs":[],"source":["# h_store  # check actvs are stored"]},{"cell_type":"markdown","metadata":{"id":"hen48PGn9z32"},"source":["## Train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C287ClLd9z32"},"outputs":[],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","print(h_store.shape)\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], model.cfg.d_model)\n","h_store = h_store.unsqueeze(1)\n","print(h_store.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ni0wWp8Y9z32"},"outputs":[],"source":["# h_store has \"grad_fn=<UnsqueezeBackward0>)\", so get rid of it\n","h = h_store.detach()  # Detaches values from the computation graph\n","# h"]},{"cell_type":"markdown","metadata":{"id":"DL02yd5GSviI"},"source":["\n","\n","> s.heads: Training a sparse auto-encoder with D features and regularization coefficient λ... We used the hyperparameters D = 512 and λ = 0.3, with a batch size of 64, and trained for 100 epochs\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nE__IUY-9z32"},"outputs":[],"source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 1, # 8\n","    n_input_ae = h.shape[-1],  # model's n_hidden\n","    n_hidden_ae = 2 * h.shape[-1],  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    # n_hidden_ae = 512,\n","    l1_coeff = 0.1,\n",")\n","\n","autoencoder = AutoEncoder(ae_cfg, h)\n","\n","data_log = autoencoder.optimize(\n","    steps = 10000, # 100\n","    log_freq = 200,\n",")"]},{"cell_type":"markdown","metadata":{"id":"SI7MSzupOlxn"},"source":["### reconstruction loss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AMg-l9vP0TB-"},"outputs":[],"source":["@t.no_grad()\n","def get_reconstruction_loss(\n","    tokens: Int[Tensor, \"batch seq\"],\n","    model: HookedTransformer,\n","    autoencoder: AutoEncoder,\n","    layer_name: str = 'blocks.0.hook_resid_post',\n",") -> Tuple[float, float]:\n","    '''\n","    Returns the reconstruction loss of each autoencoder instance on the given batch of tokens (i.e.\n","    the L2 loss between the activations and the autoencoder's reconstructions, averaged over all tokens).\n","    '''\n","    batch_size, seq_len = tokens.shape\n","\n","    logits, cache = model.run_with_cache(tokens, names_filter = [layer_name])\n","    post = cache[layer_name]\n","    assert post.shape == (batch_size, seq_len, model.cfg.d_model)\n","\n","    post_reshaped = einops.repeat(post, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","    assert post_reshaped.shape == (batch_size * seq_len, 2, model.cfg.d_model)\n","\n","    _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped)\n","    assert l2_loss.shape == (batch_size * seq_len, 2) # shape is [datapoints n_instances=2]\n","    assert post_reconstructed.shape == (batch_size * seq_len, 2, model.cfg.d_model) # shape is [datapoints n_instances=2 d_mlp]\n","\n","    # Print out the avg L2 norm of activations\n","    print(\"Avg L2 norm of acts: \", einops.reduce(post_reshaped.pow(2), \"batch inst d_model -> inst\", \"mean\").tolist())\n","    # Print out the cosine similarity between original neuron activations & reconstructions (averaged over neurons)\n","    print(\"Avg cos sim of neuron reconstructions: \", t.cosine_similarity(post_reconstructed, post_reshaped, dim=0).mean(-1).tolist())\n","\n","    return l2_loss.mean(0).tolist()\n","\n","layer_name = 'blocks.0.hook_resid_post'\n","reconstruction_loss = get_reconstruction_loss(tokens, model, autoencoder, layer_name)\n","print(reconstruction_loss)"]},{"cell_type":"markdown","metadata":{"id":"H3uEs3WFP-KZ"},"source":["The list length corresponds to the number of SAE instances trained.\n","\n","The loss should be very small (closer to 0) and the cosine sim should be high (closer to 1). If not, then re-train with different params."]},{"cell_type":"markdown","metadata":{"id":"H39lcJjMA4lm"},"source":["### save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zRORmrFHBDof"},"outputs":[],"source":["from google.colab import files\n","\n","# Save the model's state dictionary\n","model_path = 'autoencoder.pth'\n","t.save(autoencoder.state_dict(), model_path)\n","\n","# Download the model file\n","files.download(model_path)"]},{"cell_type":"markdown","metadata":{"id":"bFHHPyiRCJDE"},"source":["## load sae"]},{"cell_type":"markdown","metadata":{"id":"OwuXdwbRX8WZ"},"source":["Must run \"Get activations to train SAE\" of this section before loading to get h_store"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"srElnr4bXzLC"},"outputs":[],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], model.cfg.d_model)\n","h_store = h_store.unsqueeze(1)\n","h = h_store.detach()  # Detaches values from the computation graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tHAaTGxBw2u"},"outputs":[],"source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 2, # 8\n","    n_input_ae = model.cfg.d_model,  # model's n_hidden\n","    n_hidden_ae = 2 * model.cfg.d_model,  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    l1_coeff = 0.5,\n",")\n","\n","autoencoder = AutoEncoder(ae_cfg, h)\n","\n","# Load the model's state dictionary\n","model_path = 'autoencoder.pth'\n","autoencoder.load_state_dict(t.load(model_path))"]},{"cell_type":"markdown","metadata":{"id":"K-wGX_O3xaH9"},"source":["# Find most impt features"]},{"cell_type":"markdown","metadata":{"id":"FN_4C4TZxt2O"},"source":["Most important: highest change in output probability after ablation"]},{"cell_type":"markdown","metadata":{"id":"t4wR6xKR-zwQ"},"source":["## Ablate a SAE feature"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":559,"status":"ok","timestamp":1716347120107,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"jOGPYPdW-zwi","outputId":"db494b88-ea33-4c2a-b11e-5997098558dd"},"outputs":[{"data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":75,"status":"ok","timestamp":1716347120108,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"wIchjUJa-zwi","outputId":"36c38620-eb72-4276-cad4-0b15e663731d"},"outputs":[{"data":{"text/plain":["torch.Size([10, 2])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_lstEzp-zwj"},"outputs":[],"source":["h_store = t.zeros((10, 2, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71,"status":"ok","timestamp":1716347120108,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"1bi96TH3-zwj","outputId":"d3f4b9e0-976f-49e5-f42d-1c6af04d75f2"},"outputs":[{"data":{"text/plain":["torch.Size([20, 2, 768])"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":68,"status":"ok","timestamp":1716347120108,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"c4OS7bSn-zwj","outputId":"5afedda5-b5d9-4ed3-9cd6-092da596213f"},"outputs":[{"data":{"text/plain":["torch.Size([20, 2, 3840])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpdUOKoW-zwj"},"outputs":[],"source":["# ablate a feature (idx = 0) by setting it to 0\n","acts[:, :, 0] = 0\n","# acts[:, :, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":66,"status":"ok","timestamp":1716347120109,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"n8pzNqmyaLMx","outputId":"679210b8-8d82-4354-9183-e451ae0d566b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of 0s: 153600\n","Number of nonzeroes: 0\n","Percentage of 1s over 0s: 0.00%\n"]}],"source":["# Count the number of 0s in the tensor\n","num_zeros = (acts == 0).sum().item()\n","\n","# Count the number of nonzeroes in the tensor\n","num_ones = (acts > 0).sum().item()\n","\n","# Calculate the percentage of 1s over 0s\n","if num_zeros > 0:\n","    percentage_ones_over_zeros = (num_ones / num_zeros) * 100\n","else:\n","    percentage_ones_over_zeros = float('inf')  # Handle division by zero\n","\n","print(f\"Number of 0s: {num_zeros}\")\n","print(f\"Number of nonzeroes: {num_ones}\")\n","print(f\"Percentage of 1s over 0s: {percentage_ones_over_zeros:.2f}%\")"]},{"cell_type":"markdown","metadata":{"id":"WehBqPPAJ23M"},"source":["## Reconstruct"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QcHi8GGg-zwj"},"outputs":[],"source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":64,"status":"ok","timestamp":1716347120109,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"3cD0L6zD-zwk","outputId":"26eb21e6-474c-4e04-9888-e3d8e025456d"},"outputs":[{"data":{"text/plain":["torch.Size([20, 2, 768])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["h_reconstructed.shape"]},{"cell_type":"markdown","metadata":{"id":"GpZqQzcS-zwk"},"source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62,"status":"ok","timestamp":1716347120109,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"rkmEN6ZX-zwk","outputId":"2a0394e8-f699-48b4-da93-cec7983ac914"},"outputs":[{"data":{"text/plain":["torch.Size([20, 768])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":60,"status":"ok","timestamp":1716347120109,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"kW3g_cx2-zwk","outputId":"e199df44-d5d4-4d76-c9f4-0b47c540e00c"},"outputs":[{"data":{"text/plain":["torch.Size([10, 2, 768])"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch.shape"]},{"cell_type":"markdown","metadata":{"id":"me90OFs5-pDg"},"source":["## Replace LLM actvs with decoder output"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eRF7nMRQCM9X"},"outputs":[],"source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","\n","layer_name = 'blocks.0.hook_resid_post'\n","\n","def patch_mlp_vectors(\n","    orig_MLP_vector: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_MLP_vector[:, :, :] = LLM_patch\n","    return orig_MLP_vector\n","\n","hook_fn = partial(\n","        patch_mlp_vectors,\n","        LLM_patch=LLM_patch,\n","        layer_to_patch = 0\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model.run_with_hooks(all_tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )"]},{"cell_type":"markdown","metadata":{"id":"sb7TeOZq-LoJ"},"source":["# OV Scores with just successor head"]},{"cell_type":"markdown","metadata":{"id":"kyk-1Wht4Gev"},"source":["## Unablated"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":393,"status":"ok","timestamp":1716347200935,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"86dPnCt1zZPB","outputId":"d123a936-4ac4-4953-e077-803e091f1325"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:cache_some is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"]}],"source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","cache = {}\n","model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","model(input_text)\n","# z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","z_0 = model.blocks[1].attn.hook_z(cache[\"blocks.0.hook_resid_post\"])\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","logits = model.unembed(model.ln_final(o))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YiH_zptR3orO"},"outputs":[],"source":["# pred_tokens = [\n","#                 model.tokenizer.decode(token)\n","#                 for token in torch.topk(\n","#                     logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","#                 ).indices\n","#             ]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1716347201440,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"5OpC8OMm4A1Q","outputId":"848d7f8d-6161-40f0-867b-1578b2e24126"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'4'"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"]},{"cell_type":"markdown","metadata":{"id":"3nTIqcZe-UgM"},"source":["## Ablated"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zaqbn0ao-UgV"},"outputs":[],"source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","z_0 = model.blocks[1].attn.hook_z(LLM_patch)\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","ablated_logits = model.unembed(model.ln_final(o))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1716347201440,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"008j5g7e-UgW","outputId":"9fc57f20-2366-49f7-a673-9526ec9bd839"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["' innumerable'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"]},{"cell_type":"markdown","metadata":{"id":"770L0w9RVpJ7"},"source":["# Loop through features to ablate"]},{"cell_type":"markdown","metadata":{"id":"j7cslBI_WCUy"},"source":["## get feature actvs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1716341531503,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"bWAgwpnOVzYS","outputId":"207d9ff1-01e1-4831-ae0f-8703a4103f9f"},"outputs":[{"data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1716341531503,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"caacpYy-VzYS","outputId":"127e4928-891c-492f-9b8d-6fd273746df9"},"outputs":[{"data":{"text/plain":["torch.Size([10, 2])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lFG2i8mCVzYS"},"outputs":[],"source":["h_store = t.zeros((10, 2, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1716341531506,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"o7ilT9euVzYS","outputId":"32918cd2-596b-466c-e17c-762a05e7f45d"},"outputs":[{"data":{"text/plain":["torch.Size([20, 2, 768])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1716341531506,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"7E_8yUU_VzYS","outputId":"163d74f9-4f35-4b82-a701-48f86c5bb1c4"},"outputs":[{"data":{"text/plain":["torch.Size([20, 2, 1536])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"markdown","metadata":{"id":"rIsysv3QVzYR"},"source":["## ablate"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1716341532939,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"BCEXEL2CZCfd","outputId":"360e3980-d096-4a7b-bf83-fac4a4000cfa"},"outputs":[{"data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.]], device='cuda:0', grad_fn=<SelectBackward0>)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["# before ablating, see which feature coeffs > 0\n","# acts[:, :, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":45,"status":"ok","timestamp":1716341532939,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"1Vsy4MaaVzYS","outputId":"579608e3-7ee8-4a1a-9d54-d5a7dc51ab7c"},"outputs":[{"data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.]], device='cuda:0', grad_fn=<SelectBackward0>)"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["for i in range(len(acts_clone.shape[2])):\n","    # ablate a feature (idx) by setting it to 0\n","\n","    acts_clone = acts.clone().detach()\n","\n","    acts_clone[:, :, i] = 0\n","    # acts[:, :, 0]\n","\n","    h_reconstructed = einops.einsum(\n","            acts_clone, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec\n","\n","    h_reconstructed_1 = h_reconstructed[:, 0, :]  # get the first SAE instance\n","\n","    LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","\n","    hook_fn = partial(\n","        patch_mlp_vectors,\n","        LLM_patch=LLM_patch,\n","        layer_to_patch = 0\n","    )\n","\n","    # if you use run_with_cache, you need to add_hook before\n","    # if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","\n","    # rerun clean inputs on ablated model\n","    ablated_logits = model.run_with_hooks(all_tokens,\n","                        fwd_hooks=[\n","                            (layer_name, hook_fn),\n","                        ]\n","                    )\n","    next_token = ablated_logits[0, -1].argmax(dim=-1)\n","    next_char = model.to_string(next_token)\n","    next_char"]}],"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c6be004161254063a334f491462a588b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3fc027b52b2f4f96ba92bbf081e02d8e","IPY_MODEL_c4991bbe902449d3be1badb035a3af2e","IPY_MODEL_f749e14c5927432080b00b06167f11a1"],"layout":"IPY_MODEL_2d09609d0df047c581bffd0334f19fa3"}},"3fc027b52b2f4f96ba92bbf081e02d8e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_16c26fef58a14265b31c0ccf874b2e31","placeholder":"​","style":"IPY_MODEL_a8c65c8ae4a34f2aa1c9bbef106b7e7e","value":"config.json: 100%"}},"c4991bbe902449d3be1badb035a3af2e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66c6611c971e4d2d9787d7c550b1306d","max":718,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b5b0dd95a26e47bebc0177377962b6b6","value":718}},"f749e14c5927432080b00b06167f11a1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f577b79eeeef4e09b32fa9e6ec7b25c2","placeholder":"​","style":"IPY_MODEL_a75a3d6ef2e34c2e8c1b82eba305bc8c","value":" 718/718 [00:00&lt;00:00, 65.7kB/s]"}},"2d09609d0df047c581bffd0334f19fa3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16c26fef58a14265b31c0ccf874b2e31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a8c65c8ae4a34f2aa1c9bbef106b7e7e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66c6611c971e4d2d9787d7c550b1306d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5b0dd95a26e47bebc0177377962b6b6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f577b79eeeef4e09b32fa9e6ec7b25c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a75a3d6ef2e34c2e8c1b82eba305bc8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ef072d8003e475f9e6031163b032881":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bbd8642f8bbf4f3da9dbd0abcfbcf1cf","IPY_MODEL_d06343bcc0d04616b581667b01e8959e","IPY_MODEL_08570c41a4b54317915f5fd564386aa5"],"layout":"IPY_MODEL_0d878e98656f4898a3252ceb34737686"}},"bbd8642f8bbf4f3da9dbd0abcfbcf1cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f6d110dd66f446f8ab4ccc632e7277a","placeholder":"​","style":"IPY_MODEL_a982948245f4436dbf70cd1c06c971f4","value":"model.safetensors: 100%"}},"d06343bcc0d04616b581667b01e8959e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e1046c659d9f4e6c930495259b39c01f","max":1519984962,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7933aae0deb3434aa3fb0d13da37fa95","value":1519984962}},"08570c41a4b54317915f5fd564386aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_238742d2c4cb44bab11a1c814e11030f","placeholder":"​","style":"IPY_MODEL_14139f4f3f5a4c59b3d3cfee3bde908e","value":" 1.52G/1.52G [00:03&lt;00:00, 435MB/s]"}},"0d878e98656f4898a3252ceb34737686":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f6d110dd66f446f8ab4ccc632e7277a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a982948245f4436dbf70cd1c06c971f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e1046c659d9f4e6c930495259b39c01f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7933aae0deb3434aa3fb0d13da37fa95":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"238742d2c4cb44bab11a1c814e11030f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14139f4f3f5a4c59b3d3cfee3bde908e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1e878b7eea2645198b9b3076c0eb1934":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0561dbb9a9084061bdcbe2c4a0092ccd","IPY_MODEL_17838ce0847d4003bdf05d2aa26c9b80","IPY_MODEL_d317338ba49649aa8357c2f2e87a7dd1"],"layout":"IPY_MODEL_c15720aa633246a98b9d4ba477da6014"}},"0561dbb9a9084061bdcbe2c4a0092ccd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_73b2e3a84dea4118b4893abbfd48830d","placeholder":"​","style":"IPY_MODEL_0e28f3373eab4fdfb97303a95a9f0491","value":"generation_config.json: 100%"}},"17838ce0847d4003bdf05d2aa26c9b80":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee9561eb6c654a438328bf707bd5701e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bcd4601a5d2e4d0d96a96bde08d65d28","value":124}},"d317338ba49649aa8357c2f2e87a7dd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7b204e99f3924f99b23ff19be25156f7","placeholder":"​","style":"IPY_MODEL_6b6091271c114dcc8e5ea22ce37e8fa3","value":" 124/124 [00:00&lt;00:00, 10.9kB/s]"}},"c15720aa633246a98b9d4ba477da6014":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73b2e3a84dea4118b4893abbfd48830d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e28f3373eab4fdfb97303a95a9f0491":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee9561eb6c654a438328bf707bd5701e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bcd4601a5d2e4d0d96a96bde08d65d28":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7b204e99f3924f99b23ff19be25156f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b6091271c114dcc8e5ea22ce37e8fa3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"492f1eb2fb6d4adabddc82eaeb1902b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_abe75850e9c54bd98213b677411c6d99","IPY_MODEL_bfe7959b4de24da284ad23363bea3c9f","IPY_MODEL_cab1736352924c4dacf00811cf193872"],"layout":"IPY_MODEL_8b08790c1d82430e98acc87a0ecd6825"}},"abe75850e9c54bd98213b677411c6d99":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75d9c56d3b4d40959fbfb93baa5ae40f","placeholder":"​","style":"IPY_MODEL_7fc9a44b78674a86acc94e587b9e1454","value":"tokenizer_config.json: 100%"}},"bfe7959b4de24da284ad23363bea3c9f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4abd280b1a5463e8bc9736f57971c90","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_275336b09d6743ba8e550cf32001db6c","value":26}},"cab1736352924c4dacf00811cf193872":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_459fdb1dfaf44258b6f0dcac98e4ce2e","placeholder":"​","style":"IPY_MODEL_135800d65f64410e86cf0c93044b13b4","value":" 26.0/26.0 [00:00&lt;00:00, 1.52kB/s]"}},"8b08790c1d82430e98acc87a0ecd6825":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75d9c56d3b4d40959fbfb93baa5ae40f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7fc9a44b78674a86acc94e587b9e1454":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f4abd280b1a5463e8bc9736f57971c90":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"275336b09d6743ba8e550cf32001db6c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"459fdb1dfaf44258b6f0dcac98e4ce2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"135800d65f64410e86cf0c93044b13b4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0c43619bff6e4dc289d6f437f965953b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4d7585bf748e463bbdaf30827196ea1a","IPY_MODEL_4b0fb3bcdf3743eea30dcfc633d11383","IPY_MODEL_ee71f19b20b7460fa0a68ef17321dffb"],"layout":"IPY_MODEL_ac7c472b680043f2b39f87bc77a92a30"}},"4d7585bf748e463bbdaf30827196ea1a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8fb456119c464ff4b29fa01f0ce6caf6","placeholder":"​","style":"IPY_MODEL_a9e9e7c8265b4d4c9673931cc265c6db","value":"vocab.json: 100%"}},"4b0fb3bcdf3743eea30dcfc633d11383":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7750a1387a843f0b799b6f01a7bca7d","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7b4c9cde6bd14221974693b9dfd3e6c2","value":1042301}},"ee71f19b20b7460fa0a68ef17321dffb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95e9abbc8977434b8abd5d3ff6ccb111","placeholder":"​","style":"IPY_MODEL_cfe54412268d4ce6b38d761f3878ee77","value":" 1.04M/1.04M [00:00&lt;00:00, 27.2MB/s]"}},"ac7c472b680043f2b39f87bc77a92a30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8fb456119c464ff4b29fa01f0ce6caf6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9e9e7c8265b4d4c9673931cc265c6db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b7750a1387a843f0b799b6f01a7bca7d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7b4c9cde6bd14221974693b9dfd3e6c2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"95e9abbc8977434b8abd5d3ff6ccb111":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cfe54412268d4ce6b38d761f3878ee77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9efa5d9dfd99462c9b0f3f97a01db879":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54837d43fd01482d9fe7ad8685e1b390","IPY_MODEL_aa1b95d048a14fb7a054a0e8e3e80211","IPY_MODEL_07b2fe72c2cb44aa9f4d7054423a30e4"],"layout":"IPY_MODEL_45f3b59c010a4f64908bdcd02e13d423"}},"54837d43fd01482d9fe7ad8685e1b390":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8d5c7d511534271a1b8915bf9ccc60f","placeholder":"​","style":"IPY_MODEL_4364d6282b284e4f9cc8312d61cb79ad","value":"merges.txt: 100%"}},"aa1b95d048a14fb7a054a0e8e3e80211":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9aea74abb1e34e1ba35ebe74b6120d54","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_55fd0463f99f422a9fcaf3657b948ed1","value":456318}},"07b2fe72c2cb44aa9f4d7054423a30e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b44acad5c2244394a572ab23967f1edd","placeholder":"​","style":"IPY_MODEL_129941992f91426b887131ffdc313dab","value":" 456k/456k [00:00&lt;00:00, 1.90MB/s]"}},"45f3b59c010a4f64908bdcd02e13d423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f8d5c7d511534271a1b8915bf9ccc60f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4364d6282b284e4f9cc8312d61cb79ad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9aea74abb1e34e1ba35ebe74b6120d54":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"55fd0463f99f422a9fcaf3657b948ed1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b44acad5c2244394a572ab23967f1edd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129941992f91426b887131ffdc313dab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66b5c87644114653b59980888d66add8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5173f6d88dc34dc889793ccdb351490a","IPY_MODEL_f728f9fd0e1f4c71a849b03d4b6f3cec","IPY_MODEL_d2358c69a6904c3a8016c785dea40422"],"layout":"IPY_MODEL_2d296c7e813d46308d959f45eabee25b"}},"5173f6d88dc34dc889793ccdb351490a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df0649af8d5c402299a71f83eb804096","placeholder":"​","style":"IPY_MODEL_0e10d5eb56a44fbaae986a60f74a499e","value":"tokenizer.json: 100%"}},"f728f9fd0e1f4c71a849b03d4b6f3cec":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f4a7d668ab745819245e58ad2e8dfd2","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_16e1db2ebdbc4774a256d1c04a89ce09","value":1355256}},"d2358c69a6904c3a8016c785dea40422":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_546b8eb58000427b8efdfbda37627b49","placeholder":"​","style":"IPY_MODEL_3960f260fdc44293b2a0b6457e2f61a5","value":" 1.36M/1.36M [00:00&lt;00:00, 1.47MB/s]"}},"2d296c7e813d46308d959f45eabee25b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df0649af8d5c402299a71f83eb804096":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e10d5eb56a44fbaae986a60f74a499e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f4a7d668ab745819245e58ad2e8dfd2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16e1db2ebdbc4774a256d1c04a89ce09":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"546b8eb58000427b8efdfbda37627b49":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3960f260fdc44293b2a0b6457e2f61a5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}