{"cells":[{"cell_type":"markdown","metadata":{"id":"JZRbtlLMLflI"},"source":["This notebook obtains the activations of GPT-2 MLP0 and passes tokens belonging to ordered sequences through them.\n","\n","We use this code instead of SAElens as the code for the SAEs is more explicitly shown in here."]},{"cell_type":"markdown","source":["\n","\n","> we perform a case study on the attention head (L12H0) with the maximal\n","successor score in Pythia-1.4B\n","\n"],"metadata":{"id":"hWzRfaBQu1_s"}},{"cell_type":"markdown","metadata":{"id":"-WMqbetGrf4e"},"source":["# Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hozpKjBenAph"},"outputs":[],"source":["%%capture\n","!pip install transformer_lens\n","!pip install datasets\n","!pip install zstandard"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"llAiwjkqwpqp"},"outputs":[],"source":["from transformer_lens import utils, HookedTransformer, ActivationCache\n","from dataclasses import dataclass\n","import torch as t\n","from torch import nn, Tensor\n","import torch.nn.functional as F\n","from jaxtyping import Float, Int\n","from typing import Optional, Callable, Union, List, Tuple\n","import einops\n","from datasets import load_dataset\n","\n","from tqdm import tqdm\n","from rich.table import Table\n","from rich import print as rprint"]},{"cell_type":"markdown","metadata":{"id":"2LxnTmFDpb2z"},"source":["## Load Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gaLmq0KDockd"},"outputs":[],"source":["device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","referenced_widgets":["74353f0b87d24ac4b39c80aea0f121cd","6bc5449b4067474b9fb3f1357ca2d86f","422ce902d8814484998f208ac60d19d1","81eaabfbdbd24aecb66a268abcd28430","7115294f177e4e8186b3b4e58b60c289","f286476a04b546b4991549766a1e9e0b","f50c38d861f344ff82a4ab83e8368ec3","0e1da54d08e74a458cfd82acdcc57830","c1bea1566eab42ed96e48390fde871d8","53dd3c6197564e3bbf5a38616027f241","2a24d50b98dc4118a0eccdcf3467d3c7","a134aed1bd7d46c9ac63cc9e06d7d563","8bf62a3e4880462b9eb919c46a2d6534","759160f59976479eb7cd3f4dc6e9add6","8082da7b7a6f4cedb36f75ba716d1d03","14d1956b4d7640459c3bef915f3eae4e","c811e3a020364d39876a20a39a416e43","4323ec043289405aa53a3d651fa5f117","d19db1d3aa5e4c0c8efdd37c81408184","da319db376cf45948c8bc22bfdd3c46d","19d6b5a2a4c14a619279f84d4942ec6d","72425ee47c274d4fa38394c12fff484e","060bdd839aa942e29ea503a985ecf0df","54fed247b8194fbdb300d79ca8208604","58c48753e2d04ab980945ee4bbc988cb","9050b94d638149beaaf600eec5d97f29","3e1c1c65523b4d8fa88a71d8ae6a6a20","1f2bb69c744b483083221d316965d840","87228e91a0794c67ba5d146af1b33a64","3504a4fbc95f4a758b75627a2329e88e","8450168844d24e5eaa30f7609645eca1","567865120cef4ea5b0d09451c655a6b7","a9ab8139437d4f128d9deff4388d9fa5","056702453e324120bee12836ee7ddc74","9b00644d201a4d3e8d54ce6236622326","566c03a6fd3940bea2899588f95f348e","df0210da08b849d990eead52a64e3373","ad3fdff4172b4d16a3cf0ffd46cecaeb","d3842210b5534db09cc6d071192aed3e","d939b7d285544f5fb19ce1921f6ba7fc","64464fee388f4941a48605bd6c8162cb","ad86ec32ed2c4972b88ee4f67a965f38","0d4718fdf2ad4b6db785319a13393da2","1b3d163ce2354172bac1f5fce04996c1","c6c29f8f9b6d4af5b67e88fab508b51d","d800265b60d34325a9d0d0d67d592d16","8728d665282845799d6d9ca7c6d7e07e","fd6713ce2a894d82b8a48b1b8bb8c4cf","d5a9e4e4e9c04737a6f041058db67669","93e1e6db87bd40f8bc78d8a159c52a00","0f533db1de2945ac96b9cdd39a397a84","8e31ed7f91e6465e829bacd72d937788","51387d00e2624ab1a38b861305fd000b","af52350079a6490b89afedc1fec13ae6","233e5bb9250c448a981b52d0dfa870d1","856fd623b3fb455eba9eb042977e17ac","2a99069d928a43bcb6e631237eec39d5","4b7ca037df494b80bc6365d1250eb229","c07a570abb74491faa21675922404455","62cd4f781e0d4e32822747839dae5eb5","08430cd03a224cf08dbc0e9241f77a70","3482b19c81064f5183a90a3d94e5d4b3","9cf5c339644c467da694401bc07ebff6","4107620ad0d64e399e34e1a089d50968","5cf2d8519b244859899ea0593db39bcb","40e71fde282e4f4b93529433d1a8d6db","5a6c82e8397442688bec7c3c87101cd8","bd2449ce91464b01a3e557dc45a2969c","14732b33384a4d33ad80979dc5435fb6","64695457c963408ea375e3a7103cb82d","e1f3c3c6659946fa8216a58de455d895","129b64be0ba8469985d02ff6b3ddf31d","69483f5a764649498a806961ef6fb152","5ae4c4375dc04468aa7152d301fb8e17","73bdcad70c884f038bcc68381e4eef9e","ba6df6abdb3e404586588640f9c6181b","d5953f9f56954b158107c6b932e2d897"]},"executionInfo":{"elapsed":11301,"status":"ok","timestamp":1716341334656,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"zjFa1PqDrOto","outputId":"0909bac6-9bf5-4590-b131-6cafe2cd2ff2"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74353f0b87d24ac4b39c80aea0f121cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a134aed1bd7d46c9ac63cc9e06d7d563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"060bdd839aa942e29ea503a985ecf0df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"056702453e324120bee12836ee7ddc74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6c29f8f9b6d4af5b67e88fab508b51d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"856fd623b3fb455eba9eb042977e17ac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a6c82e8397442688bec7c3c87101cd8"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Loaded pretrained model gpt2-small into HookedTransformer\n"]}],"source":["model = HookedTransformer.from_pretrained(\n","    \"gpt2-small\",\n","    # \"gpt2-xl\",\n","    center_unembed=True,\n","    center_writing_weights=True,\n","    fold_ln=True,\n","    refactor_factored_attn_matrices=True,\n",")"]},{"cell_type":"markdown","metadata":{"id":"2MD88v4Zvw-r"},"source":["# Autoencoder Training"]},{"cell_type":"markdown","metadata":{"id":"1jCAYFKmz92O"},"source":["## Class Setup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WjziwoIb4Tvb"},"outputs":[],"source":["@dataclass\n","class AutoEncoderConfig:\n","    n_instances: int\n","    n_input_ae: int\n","    n_hidden_ae: int\n","    l1_coeff: float = 0.5\n","    tied_weights: bool = False\n","    weight_normalize_eps: float = 1e-8"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iRNZvnQQ6wwS"},"outputs":[],"source":["def linear_lr(step, steps):\n","    return (1 - (step / steps))\n","\n","def constant_lr(*_):\n","    return 1.0\n","\n","def cosine_decay_lr(step, steps):\n","    return np.cos(0.5 * np.pi * step / (steps - 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4CNZX3tR3j8h"},"outputs":[],"source":["class AutoEncoder(nn.Module):\n","    W_enc: Float[Tensor, \"n_instances n_input_ae n_hidden_ae\"]\n","    W_dec: Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]\n","    b_enc: Float[Tensor, \"n_instances n_hidden_ae\"]\n","    b_dec: Float[Tensor, \"n_instances n_input_ae\"]\n","\n","    def __init__(self, cfg: AutoEncoderConfig, h):\n","        super().__init__()\n","        self.cfg = cfg\n","\n","        self.model_h = h\n","\n","        self.W_enc = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_input_ae, cfg.n_hidden_ae))))\n","        if not(cfg.tied_weights):\n","            self.W_dec = nn.Parameter(nn.init.xavier_normal_(t.empty((cfg.n_instances, cfg.n_hidden_ae, cfg.n_input_ae))))\n","\n","        self.b_enc = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_hidden_ae))\n","        self.b_dec = nn.Parameter(t.zeros(cfg.n_instances, cfg.n_input_ae))\n","\n","        self.to(device)\n","\n","    def normalize_and_return_W_dec(self) -> Float[Tensor, \"n_instances n_hidden_ae n_input_ae\"]:\n","        '''\n","        If self.cfg.tied_weights = True, we return the normalized & transposed encoder weights.\n","        If self.cfg.tied_weights = False, we normalize the decoder weights in-place, and return them.\n","\n","        Normalization should be over the `n_input_ae` dimension, i.e. each feature should have a noramlized decoder weight.\n","        '''\n","        if self.cfg.tied_weights:\n","            return self.W_enc.transpose(-1, -2) / (self.W_enc.transpose(-1, -2).norm(dim=1, keepdim=True) + self.cfg.weight_normalize_eps)\n","        else:\n","            self.W_dec.data = self.W_dec.data / (self.W_dec.data.norm(dim=2, keepdim=True) + self.cfg.weight_normalize_eps)\n","            return self.W_dec\n","\n","    def forward(self, h: Float[Tensor, \"batch_size n_instances n_input_ae\"]):\n","\n","        # Compute activations\n","        h_cent = h - self.b_dec\n","        acts = einops.einsum(\n","            h_cent, self.W_enc,\n","            \"batch_size n_instances n_input_ae, n_instances n_input_ae n_hidden_ae -> batch_size n_instances n_hidden_ae\"\n","        )\n","        acts = F.relu(acts + self.b_enc)\n","\n","        # Compute reconstructed input\n","        h_reconstructed = einops.einsum(\n","            acts, self.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + self.b_dec\n","\n","        # Compute loss, return values\n","        l2_loss = (h_reconstructed - h).pow(2).mean(-1) # shape [batch_size n_instances]\n","        l1_loss = acts.abs().sum(-1) # shape [batch_size n_instances]\n","        loss = (self.cfg.l1_coeff * l1_loss + l2_loss).mean(0).sum() # scalar\n","\n","        return l1_loss, l2_loss, loss, acts, h_reconstructed\n","\n","    @t.no_grad()\n","    def resample_neurons(\n","        self,\n","        h: Float[Tensor, \"batch_size n_instances n_input_ae\"],\n","        frac_active_in_window: Float[Tensor, \"window n_instances n_hidden_ae\"],\n","        neuron_resample_scale: float,\n","    ) -> Tuple[List[List[str]], str]:\n","        '''\n","        Resamples neurons that have been dead for `dead_neuron_window` steps, according to `frac_active`.\n","        '''\n","        pass # See below for a solution to this function\n","\n","    def optimize(\n","        self,\n","        # model: Model,\n","        batch_size: int = 1024,\n","        steps: int = 10_000,\n","        log_freq: int = 100,\n","        lr: float = 1e-3,\n","        lr_scale: Callable[[int, int], float] = constant_lr,\n","        neuron_resample_window: Optional[int] = None,\n","        dead_neuron_window: Optional[int] = None,\n","        neuron_resample_scale: float = 0.2,\n","    ):\n","        '''\n","        Optimizes the autoencoder using the given hyperparameters.\n","\n","        This function should take a trained model as input.\n","        '''\n","        if neuron_resample_window is not None:\n","            assert (dead_neuron_window is not None) and (dead_neuron_window < neuron_resample_window)\n","\n","        optimizer = t.optim.Adam(list(self.parameters()), lr=lr)\n","        frac_active_list = []\n","        progress_bar = tqdm(range(steps))\n","\n","        # Create lists to store data we'll eventually be plotting\n","        data_log = {\"W_enc\": [], \"W_dec\": [], \"colors\": [], \"titles\": [], \"frac_active\": []}\n","        colors = None\n","        title = \"no resampling yet\"\n","\n","        for step in progress_bar:\n","\n","            # Resample dead neurons\n","            # if (neuron_resample_window is not None) and ((step + 1) % neuron_resample_window == 0):\n","            #     # Get the fraction of neurons active in the previous window\n","            #     frac_active_in_window = t.stack(frac_active_list[-neuron_resample_window:], dim=0)\n","            #     # Compute batch of hidden activations which we'll use in resampling\n","            #     batch = model.generate_batch(batch_size)\n","            #     h = einops.einsum(\n","            #         batch, model.W,\n","            #         \"batch_size instances features, instances hidden features -> batch_size instances hidden\"\n","            #     )\n","            #     # Resample\n","            #     colors, title = self.resample_neurons(h, frac_active_in_window, neuron_resample_scale)\n","\n","            # Update learning rate\n","            step_lr = lr * lr_scale(step, steps)\n","            for group in optimizer.param_groups:\n","                group['lr'] = step_lr\n","\n","            ### MODIFY THIS to use h,  activations from transformerlens ###\n","            # Get a batch of hidden activations from the model\n","            # with t.inference_mode():\n","                # features = model.generate_batch(batch_size)\n","                # h = einops.einsum(\n","                #     features, model.W,\n","                #     \"... instances features, instances hidden features -> ... instances hidden\"\n","                # )\n","\n","            h = self.model_h\n","\n","            # Optimize\n","            l1_loss, l2_loss, loss, acts, _ = self.forward(h)\n","            loss.backward()\n","            optimizer.step()\n","            optimizer.zero_grad()\n","\n","            # Calculate the mean sparsities over batch dim for each (instance, feature)\n","            frac_active = (acts.abs() > 1e-8).float().mean(0)\n","            frac_active_list.append(frac_active)\n","\n","            # Display progress bar, and append new values for plotting\n","            if step % log_freq == 0 or (step + 1 == steps):\n","                progress_bar.set_postfix(l1_loss=self.cfg.l1_coeff * l1_loss.mean(0).sum().item(), l2_loss=l2_loss.mean(0).sum().item(), lr=step_lr)\n","                data_log[\"W_enc\"].append(self.W_enc.detach().cpu().clone())\n","                data_log[\"W_dec\"].append(self.normalize_and_return_W_dec().detach().cpu().clone())\n","                data_log[\"colors\"].append(colors)\n","                data_log[\"titles\"].append(f\"Step {step}/{steps}: {title}\")\n","                data_log[\"frac_active\"].append(frac_active.detach().cpu().clone())\n","\n","        return data_log"]},{"cell_type":"markdown","metadata":{"id":"CF4mUxhMvw-s"},"source":["Return a dictionary `data_log` containing data which is useful for visualizing the training process"]},{"cell_type":"markdown","metadata":{"id":"TioC1OirumVa"},"source":["## Training data"]},{"cell_type":"markdown","metadata":{"id":"puaa0aYEIvK7"},"source":["> We train the SAE using number tokens from 0 to 500, both with and without a space (‘123’ and\n","‘ 123’), alongside other tasks, such as number words, cardinal words, days, months, etc. 90% of\n","these tokens go into the train set, and the remaining 10% to the validation set. Even with the other\n","tasks, the dataset is dominated by numbers, but creating a more balanced dataset would give us less\n","data to work with, and without enough data, the SAE fails to generalize to the validation set. Hence,\n","we only concern ourselves with the features that the SAE learns for number tokens, and we then\n","separately check whether these features generalize to the other tasks on the basis of logits, rather\n","than SAE activations.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xTHX63k9IyR2"},"outputs":[],"source":["input_as_str = [str(i) for i in range(500)]"]},{"cell_type":"markdown","metadata":{"id":"DvsIRn9p0ujR"},"source":["Future code will do this more efficient (not passing in batch all at once to get h)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uSxh2sb1uoU4"},"outputs":[],"source":["# dataset = load_dataset(\"stas/openwebtext-10k\", split='train', streaming=True)\n","# # dataset = load_dataset(\"EleutherAI/pile\", split='train', streaming=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sgROQC9rPaQK"},"outputs":[],"source":["# total_len = 0\n","# i = 0\n","# for sample in dataset:\n","#     total_len += len(sample[\"text\"])\n","#     i += 1\n","#     # if i == 1000:\n","#     #     break\n","# print(total_len / i)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANNUB6U01Dj1"},"outputs":[],"source":["# strMaxLen = 100 # 100\n","# batchLen = 100 # 1000\n","# batch_input = []\n","# for sample in dataset:\n","#     input_sample = sample[\"text\"][:strMaxLen]\n","#     batch_input.append(input_sample)\n","#     if len(batch_input) == batchLen:\n","#         break\n","# print(len(batch_input))\n","# # print(input_sample)"]},{"cell_type":"markdown","metadata":{"id":"IhiRijN-5Yfc"},"source":["## Get activations to train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pzX1AFUhde2_"},"outputs":[],"source":["layer_name = 'blocks.0.mlp.hook_post'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":80,"status":"ok","timestamp":1716302342942,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"ChAYwHG7_Vv_","outputId":"690b6eb7-3808-421b-c624-8be59e31cdf4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2])"]},"metadata":{},"execution_count":13}],"source":["# https://neelnanda-io.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformer.html\n","\n","tokens = model.to_tokens(input_as_str)\n","tokens.shape"]},{"cell_type":"markdown","metadata":{"id":"sSy0z7xNASfe"},"source":["Seq Len is number of tokens, not string max len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ct5KiRoU9Buc"},"outputs":[],"source":["# h_store = t.zeros(model_cache['blocks.5.mlp.hook_post'].shape, device=model.cfg.device)\n","seqLen = tokens.shape[1]\n","h_store = t.zeros((len(input_as_str), seqLen, model.cfg.d_mlp), device=model.cfg.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":79,"status":"ok","timestamp":1716302342942,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"yGm22v0N9KF2","outputId":"6c2012d4-1222-465f-cde8-f21386d0e6a6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2, 3072])"]},"metadata":{},"execution_count":15}],"source":["h_store.shape"]},{"cell_type":"markdown","metadata":{"id":"HB3UOrNl6kps"},"source":["Use hook fn to avoid storing all activations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cGfVKqfV7Jd6"},"outputs":[],"source":["def store_h_hook(\n","    pattern: Float[Tensor, \"batch seqlen dmlp\"],\n","    # hook: HookPoint,\n","    hook\n","):\n","    # Store the result.\n","    # h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored\n","    # h_store.append(1) # if h_store = [], this will work\n","    h_store[:] = pattern  # this works b/c changes values, not replaces entire thing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d697f26W6U9U"},"outputs":[],"source":["model.run_with_hooks(\n","    tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cmJDHHQBC6a2"},"outputs":[],"source":["# h_store  # check actvs are stored"]},{"cell_type":"markdown","metadata":{"id":"sICthFXwz8oM"},"source":["## Train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1716302343265,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"MoSj7woIF1AP","outputId":"178a9474-d2e7-4231-bb70-963e03123182"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([500, 2, 3072])\n","torch.Size([1000, 1, 3072])\n"]}],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","print(h_store.shape)\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], 3072)\n","h_store = h_store.unsqueeze(1)\n","print(h_store.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IH8qFcpHKjgJ"},"outputs":[],"source":["# h_store has \"grad_fn=<UnsqueezeBackward0>)\", so get rid of it\n","h = h_store.detach()  # Detaches values from the computation graph\n","# h"]},{"cell_type":"markdown","metadata":{"id":"5MA6N5ZFcZ1r"},"source":[">"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16237,"status":"ok","timestamp":1716302359491,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"V-LlNABTF1AQ","outputId":"f79a3374-d18d-4c70-f206-8e846360e3c4"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:15<00:00,  6.63it/s, l1_loss=2.83e-7, l2_loss=0.147, lr=0.001]\n"]}],"source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 2, # 8\n","    n_input_ae = h.shape[-1],  # model's n_hidden\n","    n_hidden_ae = 2 * h.shape[-1],  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    l1_coeff = 0.5,\n",")\n","\n","autoencoder = AutoEncoder(ae_cfg, h)\n","\n","data_log = autoencoder.optimize(\n","    steps = 100, # 10_000\n","    log_freq = 200,\n",")"]},{"cell_type":"markdown","metadata":{"id":"MIa9W966RDGv"},"source":["# Find features that actv highest for sample X"]},{"cell_type":"markdown","source":["Most important means highest change in output after ablating. But here, we look for highest activations on these tokens. However, this doesn't mean much because certain features may fire highly for all numbers in general! So use the paper's definition of 'most important'"],"metadata":{"id":"jzDHBcvHluWp"}},{"cell_type":"markdown","metadata":{"id":"L9WXzBk2RHgn"},"source":["## Test on features from class 3"]},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715898806550,"user_tz":240,"elapsed":218,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5104ea66-cb91-46c6-8dba-df614191621d","id":"GIEO35gckOqz"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1715898807511,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"04267e37-0fe1-4c6b-8f31-6785f64d3f36","id":"niI_edc8kOq6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":29}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_mlp), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"oiRaP1s4kfz0"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1715899426950,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"9c85d681-cbd7-46e5-e358-4ea267c64160","id":"Yx4K91T1j9om"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 3072])"]},"metadata":{},"execution_count":36}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":225,"status":"ok","timestamp":1715899437436,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"566b8953-ebd3-4299-c160-8e9d30750042","id":"wXD4a5CSj9om"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 6144])"]},"metadata":{},"execution_count":37}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AiB2-J9j9on"},"outputs":[],"source":["# Get the top k largest activations for feature neurons, not batch seq. use , dim=-1\n","feat_k = 5\n","top_acts_values, top_acts_indices = acts.topk(feat_k, dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":199,"status":"ok","timestamp":1715899443459,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"068126b6-f53a-41bc-ac11-cd94396308fe","id":"bcfDM_Lcj9on"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]]], device='cuda:0')"]},"metadata":{},"execution_count":39}],"source":["top_acts_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1715899469701,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"34443084-535b-46aa-94f0-0baaf1ee2028","id":"nHOET1JCj9on"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2])"]},"metadata":{},"execution_count":41}],"source":["all_tokens = model.to_tokens(input_as_str, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":460,"status":"ok","timestamp":1715899474321,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"eaeac6ee-bf80-443f-e235-d26136e77fb8","id":"IP8Fr1ikj9on"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[3m   Tokens which most activate    \u001b[0m\n","\u001b[3m            feature 1            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <|endoftext|>\u001b[1;4;38;5;208m364\u001b[0m │ -1.01      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m412\u001b[0m │ -1.04      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m327\u001b[0m │ -1.08      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m473\u001b[0m │ -1.13      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m401\u001b[0m │ -1.16      │\n","└──────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   Tokens which most activate    </span>\n","<span style=\"font-style: italic\">            feature 1            </span>\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence         </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">364</span> │ -1.01      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">412</span> │ -1.04      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">327</span> │ -1.08      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">473</span> │ -1.13      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">401</span> │ -1.16      │\n","└──────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m   Tokens which most activate    \u001b[0m\n","\u001b[3m            feature 0            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <|endoftext|>\u001b[1;4;38;5;208m364\u001b[0m │ -0.95      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m412\u001b[0m │ -0.98      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m327\u001b[0m │ -1.04      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m414\u001b[0m │ -1.09      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m473\u001b[0m │ -1.10      │\n","└──────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   Tokens which most activate    </span>\n","<span style=\"font-style: italic\">            feature 0            </span>\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence         </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">364</span> │ -0.95      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">412</span> │ -0.98      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">327</span> │ -1.04      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">414</span> │ -1.09      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">473</span> │ -1.10      │\n","└──────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 2           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.35      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 2           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.35      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 4           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.31      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 4           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.31      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 3           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.30      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 3           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.30      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["# get top samp_m tokens for all top feat_k feature neurons\n","samp_m = 5\n","for feature_idx in top_acts_indices[0][0]:\n","    feature_idx = feature_idx.item()\n","    ds_top_acts_indices, ds_top_acts_values = highest_activating_tokens(all_tokens, model, autoencoder, feature_idx,\n","                                                            autoencoder_B=False, k=samp_m, layer_name=layer_name)\n","    display_top_sequences(ds_top_acts_indices, ds_top_acts_values, all_tokens)"]},{"cell_type":"markdown","metadata":{"id":"b0_UTee0lQL3"},"source":["## Test on features from class 6"]},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('6')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1715899544763,"user_tz":240,"elapsed":234,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b869c28f-7eb1-4580-966d-e59c68f50138","id":"9bI0cMLylQMA"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['6', '16', '26', '36', '46', '56', '66', '76', '86', '96']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1715899545118,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"73fcc5de-aa8b-4a0e-8e70-9ff7247f947f","id":"2boWArlmlQMB"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":44}],"source":["# all_tokens = model.to_tokens(input_as_str, prepend_bos=True)\n","\n","all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_mlp), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"eWpCYVuUlQMB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36,"status":"ok","timestamp":1715899545118,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"bd706f77-7b6e-44a8-85b3-c53bb9ba56f8","id":"qBHq1bjHlQMB"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 3072])"]},"metadata":{},"execution_count":46}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1715899545119,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"75709c08-cdbe-40c6-d40d-9fbc7a818544","id":"XNRJe38BlQMB"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 6144])"]},"metadata":{},"execution_count":47}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N9yFcyMElQMC"},"outputs":[],"source":["# Get the top k largest activations for feature neurons, not batch seq. use , dim=-1\n","feat_k = 5\n","top_acts_values, top_acts_indices = acts.topk(feat_k, dim=-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1715899545119,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"5203f796-cdf4-42fd-b7b6-73604afa8af7","id":"gFyzbBjOlQMC"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]],\n","\n","        [[1, 0, 2, 4, 3],\n","         [1, 0, 2, 4, 3]]], device='cuda:0')"]},"metadata":{},"execution_count":49}],"source":["top_acts_indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1715899545119,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"e765e234-b344-4407-c8e2-4572269be4cf","id":"y7Ln6FODlQMC"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2])"]},"metadata":{},"execution_count":50}],"source":["all_tokens = model.to_tokens(input_as_str, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":216,"status":"ok","timestamp":1715899545308,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"187f9fe6-2d95-4af4-9b09-492f71b5690b","id":"goN0XxBolQMC"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[3m   Tokens which most activate    \u001b[0m\n","\u001b[3m            feature 1            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <|endoftext|>\u001b[1;4;38;5;208m364\u001b[0m │ -1.01      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m412\u001b[0m │ -1.04      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m327\u001b[0m │ -1.08      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m473\u001b[0m │ -1.13      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m401\u001b[0m │ -1.16      │\n","└──────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   Tokens which most activate    </span>\n","<span style=\"font-style: italic\">            feature 1            </span>\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence         </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">364</span> │ -1.01      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">412</span> │ -1.04      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">327</span> │ -1.08      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">473</span> │ -1.13      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">401</span> │ -1.16      │\n","└──────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m   Tokens which most activate    \u001b[0m\n","\u001b[3m            feature 0            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <|endoftext|>\u001b[1;4;38;5;208m364\u001b[0m │ -0.95      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m412\u001b[0m │ -0.98      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m327\u001b[0m │ -1.04      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m414\u001b[0m │ -1.09      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m473\u001b[0m │ -1.10      │\n","└──────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   Tokens which most activate    </span>\n","<span style=\"font-style: italic\">            feature 0            </span>\n","┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence         </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">364</span> │ -0.95      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">412</span> │ -0.98      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">327</span> │ -1.04      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">414</span> │ -1.09      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">473</span> │ -1.10      │\n","└──────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 2           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.35      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.35      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 2           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.35      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.35      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 4           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.31      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.31      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 4           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.31      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.31      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["\u001b[3m  Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 3           \u001b[0m\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence      \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m1 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m0 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m2 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m4 │ -0.30      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0m3 │ -0.30      │\n","└────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">  Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 3           </span>\n","┏━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence       </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>1 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>0 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>2 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>4 │ -0.30      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>3 │ -0.30      │\n","└────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["# get top samp_m tokens for all top feat_k feature neurons\n","samp_m = 5\n","for feature_idx in top_acts_indices[0][0]:\n","    feature_idx = feature_idx.item()\n","    ds_top_acts_indices, ds_top_acts_values = highest_activating_tokens(all_tokens, model, autoencoder, feature_idx,\n","                                                            autoencoder_B=False, k=samp_m, layer_name=layer_name)\n","    display_top_sequences(ds_top_acts_indices, ds_top_acts_values, all_tokens)"]},{"cell_type":"markdown","metadata":{"id":"BdgIFHMcuJu5"},"source":["# Get top samples for a feature"]},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uYHcZPYoh867","executionInfo":{"status":"ok","timestamp":1715898806550,"user_tz":240,"elapsed":218,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5104ea66-cb91-46c6-8dba-df614191621d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":28}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1715898807511,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"RkRGm-yqOKDe","outputId":"04267e37-0fe1-4c6b-8f31-6785f64d3f36"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":29}],"source":["# all_tokens = model.to_tokens(input_as_str, prepend_bos=True)\n","\n","all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"executionInfo":{"elapsed":383,"status":"ok","timestamp":1715898810109,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"0l2gB4IWSqjp","outputId":"a19b8d0c-d788-49ea-9720-f986e276b6c3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["\u001b[3m   Tokens which most activate   \u001b[0m\n","\u001b[3m           feature 0            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <|endoftext|>\u001b[1;4;38;5;208m93\u001b[0m │ -1.55      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m53\u001b[0m │ -1.58      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m23\u001b[0m │ -1.58      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m3\u001b[0m  │ -1.58      │\n","│ <|endoftext|>\u001b[1;4;38;5;208m83\u001b[0m │ -1.59      │\n","└─────────────────┴────────────┘\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">   Tokens which most activate   </span>\n","<span style=\"font-style: italic\">           feature 0            </span>\n","┏━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence        </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">93</span> │ -1.55      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">53</span> │ -1.58      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">23</span> │ -1.58      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">3</span>  │ -1.58      │\n","│ &lt;|endoftext|&gt;<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">83</span> │ -1.59      │\n","└─────────────────┴────────────┘\n","</pre>\n"]},"metadata":{}}],"source":["@t.inference_mode()\n","def highest_activating_tokens(\n","    tokens: Int[Tensor, \"batch seq\"],\n","    model: HookedTransformer,\n","    autoencoder: AutoEncoder,\n","    feature_idx: int,\n","    autoencoder_B: bool = False,\n","    k: int = 10,\n","    layer_name: str = 'blocks.5.mlp.hook_post',\n",") -> Tuple[Int[Tensor, \"k 2\"], Float[Tensor, \"k\"]]:\n","    '''\n","    Returns the indices & values for the highest-activating tokens in the given batch of data.\n","    '''\n","    batch_size, seq_len = tokens.shape\n","    instance_idx = 1 if autoencoder_B else 0\n","\n","    # Get the LLM model's post activations from the clean run\n","    cache = model.run_with_cache(tokens, names_filter=[layer_name])[1]\n","    post = cache[layer_name]\n","    post_reshaped = einops.rearrange(post, \"batch seq d_mlp -> (batch seq) d_mlp\")\n","\n","    # Compute SAE activations (not from a fwd pass, but explicitly, by taking only the feature we want)\n","    # This code is copied from the first part of the 'forward' method of the AutoEncoder class\n","    h_cent = post_reshaped - autoencoder.b_dec[instance_idx]\n","    acts = einops.einsum(\n","        h_cent, autoencoder.W_enc[instance_idx, :, feature_idx],\n","        \"batch_size n_input_ae, n_input_ae -> batch_size\"\n","    )\n","\n","    # Get the top k SAE largest activations for that SAE feature\n","    top_acts_values, top_acts_indices = acts.topk(k)\n","\n","    # Convert the indices into (batch, seq) indices\n","    top_acts_batch = top_acts_indices // seq_len\n","    top_acts_seq = top_acts_indices % seq_len\n","\n","    return t.stack([top_acts_batch, top_acts_seq], dim=-1), top_acts_values\n","\n","\n","def display_top_sequences(top_acts_indices, top_acts_values, tokens):\n","    table = Table(\"Sequence\", \"Activation\", title=\"Tokens which most activate feature \" + str(feature_idx))\n","    # indices is that highest token in (sampNum, pos) pair\n","    for (batch_idx, seq_idx), value in zip(top_acts_indices, top_acts_values):\n","        # Get the sequence as a string (with some padding on either side of our sequence)\n","        seq = \"\"\n","        # around the token's pos as center, loop thru window of 10, or bounds of sequence\n","        for i in range(max(seq_idx-5, 0), min(seq_idx+5, all_tokens.shape[1])):\n","            # the curr token in the loop thru the window of the seq\n","            new_str_token = model.to_single_str_token(tokens[batch_idx, i].item()).replace(\"\\n\", \"\\\\n\")\n","            # Highlight the token with the high activation\n","            if i == seq_idx: new_str_token = f\"[b u dark_orange]{new_str_token}[/]\"\n","            # add all tokens in len-10 window to the row to display\n","            seq += new_str_token\n","        # Print the sequence, and the activation value\n","        table.add_row(seq, f'{value:.2f}')\n","    rprint(table)\n","\n","tokens = all_tokens[:200]\n","feature_idx = 0\n","k = 5 # all_tokens.shape[0]\n","top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx, autoencoder_B=False, k=k, layer_name=layer_name)\n","display_top_sequences(top_acts_indices, top_acts_values, tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":195,"status":"ok","timestamp":1714522164864,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"MSBquLojO-WL","outputId":"f532dbb5-d36a-4c96-f784-030a0175d7de"},"outputs":[{"data":{"text/plain":["torch.Size([2, 3072, 6144])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["autoencoder.W_enc.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":977},"executionInfo":{"elapsed":1299,"status":"ok","timestamp":1714522168835,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"id":"DT7tvL3qGpJL","outputId":"acbcda86-0af9-43fe-9283-2f1b440f6997"},"outputs":[{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">            Tokens which most activate feature 0            </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.05      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.10      │\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked  │ -0.11      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e     │ -0.14      │\n","│ �t really give a<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> shit</span> that most of the      │ -0.14      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m            Tokens which most activate feature 0            \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.05      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.10      │\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked  │ -0.11      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e     │ -0.14      │\n","│ �t really give a\u001b[1;4;38;5;208m shit\u001b[0m that most of the      │ -0.14      │\n","└─────────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                             Tokens which most activate feature 1                             </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                                                      </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked                                    │ -0.10      │\n","│  of the Johns Hopkins Bl<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">o</span>&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt;&lt;|endoftext|&gt; │ -0.15      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\"                                   │ -0.16      │\n","│  The world's best make<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>up artists reveal their                                │ -0.17      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e                                       │ -0.21      │\n","└───────────────────────────────────────────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m                             Tokens which most activate feature 1                             \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                                                     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked                                    │ -0.10      │\n","│  of the Johns Hopkins Bl\u001b[1;4;38;5;208mo\u001b[0m<|endoftext|><|endoftext|><|endoftext|><|endoftext|> │ -0.15      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\"                                   │ -0.16      │\n","│  The world's best make\u001b[1;4;38;5;208m-\u001b[0mup artists reveal their                                │ -0.17      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e                                       │ -0.21      │\n","└───────────────────────────────────────────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                   Tokens which most activate feature 2                    </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                                   </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked                 │ -0.12      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\"                │ -0.18      │\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)                 │ -0.19      │\n","│ �t really give a<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> shit</span> that most of the                     │ -0.22      │\n","│ &lt;|endoftext|&gt;SHARE THIS ARTICLE Share<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> Tweet</span> Post Email\\n\\n │ -0.22      │\n","└────────────────────────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m                   Tokens which most activate feature 2                    \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                                  \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked                 │ -0.12      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\"                │ -0.18      │\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)                 │ -0.19      │\n","│ �t really give a\u001b[1;4;38;5;208m shit\u001b[0m that most of the                     │ -0.22      │\n","│ <|endoftext|>SHARE THIS ARTICLE Share\u001b[1;4;38;5;208m Tweet\u001b[0m Post Email\\n\\n │ -0.22      │\n","└────────────────────────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">        Tokens which most activate feature 6141         </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>For today��                │ -0.04      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>A magazine supplement with │ -0.04      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>Anarchists in              │ -0.04      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>New drunk-driving          │ -0.04      │\n","│ <span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">&lt;|endoftext|&gt;</span>The 45-year                │ -0.04      │\n","└─────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m        Tokens which most activate feature 6141         \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mFor today��                │ -0.04      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mA magazine supplement with │ -0.04      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mAnarchists in              │ -0.04      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mNew drunk-driving          │ -0.04      │\n","│ \u001b[1;4;38;5;208m<|endoftext|>\u001b[0mThe 45-year                │ -0.04      │\n","└─────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Tokens which most activate feature 6142           </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.05      │\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked  │ -0.08      │\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.11      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e     │ -0.12      │\n","│ �t really give a<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> shit</span> that most of the      │ -0.13      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m          Tokens which most activate feature 6142           \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.05      │\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked  │ -0.08      │\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.11      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e     │ -0.12      │\n","│ �t really give a\u001b[1;4;38;5;208m shit\u001b[0m that most of the      │ -0.13      │\n","└─────────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">          Tokens which most activate feature 6143           </span>\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃<span style=\"font-weight: bold\"> Sequence                                    </span>┃<span style=\"font-weight: bold\"> Activation </span>┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> writing</span> in early June)  │ -0.04      │\n","│  a dozen — \"Americans<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> for</span> Real Good Coffee\" │ -0.09      │\n","│ It's not clear how<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> or</span> why two men attacked  │ -0.10      │\n","│  the sauce in their carry<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\">-</span>on luggage, e     │ -0.13      │\n","│ �t really give a<span style=\"color: #ff8700; text-decoration-color: #ff8700; font-weight: bold; text-decoration: underline\"> shit</span> that most of the      │ -0.15      │\n","└─────────────────────────────────────────────┴────────────┘\n","</pre>\n"],"text/plain":["\u001b[3m          Tokens which most activate feature 6143           \u001b[0m\n","┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n","┃\u001b[1m \u001b[0m\u001b[1mSequence                                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mActivation\u001b[0m\u001b[1m \u001b[0m┃\n","┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n","│ at the time of this\u001b[1;4;38;5;208m writing\u001b[0m in early June)  │ -0.04      │\n","│  a dozen — \"Americans\u001b[1;4;38;5;208m for\u001b[0m Real Good Coffee\" │ -0.09      │\n","│ It's not clear how\u001b[1;4;38;5;208m or\u001b[0m why two men attacked  │ -0.10      │\n","│  the sauce in their carry\u001b[1;4;38;5;208m-\u001b[0mon luggage, e     │ -0.13      │\n","│ �t really give a\u001b[1;4;38;5;208m shit\u001b[0m that most of the      │ -0.15      │\n","└─────────────────────────────────────────────┴────────────┘\n"]},"metadata":{},"output_type":"display_data"}],"source":["# for feature_idx in range(model.cfg.d_mlp*2):\n","for feature_idx in range(3):\n","    top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx, autoencoder_B=False, k=k, layer_name=layer_name)\n","    display_top_sequences(top_acts_indices, top_acts_values, tokens)\n","\n","for feature_idx in range(model.cfg.d_mlp*2 -3, model.cfg.d_mlp*2):\n","    top_acts_indices, top_acts_values = highest_activating_tokens(tokens, model, autoencoder, feature_idx, autoencoder_B=False, k=k, layer_name=layer_name)\n","    display_top_sequences(top_acts_indices, top_acts_values, tokens)"]},{"cell_type":"markdown","source":["# Find most impt features"],"metadata":{"id":"K-wGX_O3xaH9"}},{"cell_type":"markdown","source":["Most important: highest change in output probability after ablation"],"metadata":{"id":"FN_4C4TZxt2O"}},{"cell_type":"markdown","source":["## Ablate a SAE feature, then reconstruct"],"metadata":{"id":"RT4-liF2-thv"}},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716302376624,"user_tz":240,"elapsed":321,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2663ae9c-1db9-4aef-a71e-4e14616c65c0","id":"buNX5ZCf2G2l"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1716302377421,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"dabc715b-7340-4d5e-de43-c7da209dc188","id":"6p72xmKS2G2y"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":23}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_mlp), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"3wtshcLA2G2z"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1716302377421,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"153be76e-de2f-4191-f4cf-8b4ea762e867","id":"9o_bzPuT2G2z"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 3072])"]},"metadata":{},"execution_count":25}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_mlp -> (batch seq) instances d_mlp\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1716302377421,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"e4c047fd-46b4-4285-813c-92b81c627fcd","id":"5q3n3uEZ2G2z"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 6144])"]},"metadata":{},"execution_count":26}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","source":["# ablate a feature by setting it to 0\n","acts[:, :, 0] = 0\n","# acts[:, :, 0]"],"metadata":{"id":"Um5BvoEt7KNI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"qlxG8x7T7RC1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_reconstructed.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hCEqgtIkAZcG","executionInfo":{"status":"ok","timestamp":1716302377422,"user_tz":240,"elapsed":27,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e1f05991-6e85-4c8e-8302-858a2e34f857"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 3072])"]},"metadata":{},"execution_count":30}]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"IO2OfnUJPFPD"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UQtKHcWIPTxm","executionInfo":{"status":"ok","timestamp":1716302377422,"user_tz":240,"elapsed":25,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0f6acc02-775a-4f54-bf5a-a0b2283f67f7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 3072])"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_mlp -> batch seq d_mlp\", batch=10)\n","LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYhsXPc3OwzD","executionInfo":{"status":"ok","timestamp":1716302377422,"user_tz":240,"elapsed":24,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"efbeaccf-227d-4258-eb86-f9e23ed1bbef"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 3072])"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["## Replace LLM actvs with decoder output"],"metadata":{"id":"me90OFs5-pDg"}},{"cell_type":"code","source":["# replace LLM actvs in that layer with decoder output\n","\n","from transformer_lens.hook_points import HookPoint\n","from functools import partial\n","\n","layer_name = 'blocks.0.mlp.hook_post'\n","\n","def patch_mlp_vectors(\n","    orig_MLP_vector: Float[Tensor, \"batch pos d_model\"],\n","    hook: HookPoint,\n","    LLM_patch: Float[Tensor, \"batch pos d_model\"],\n","    layer_to_patch: int,\n",") -> Float[Tensor, \"batch pos d_model\"]:\n","    if layer_to_patch == hook.layer():\n","        orig_MLP_vector[:, :, :] = LLM_patch\n","    return orig_MLP_vector\n","\n","hook_fn = partial(\n","        patch_mlp_vectors,\n","        LLM_patch=LLM_patch,\n","        layer_to_patch = 0\n","    )\n","\n","# if you use run_with_cache, you need to add_hook before\n","# if you use run_with_hooks, you dont need add_hook, just add it in fwd_hooks arg\n","\n","# rerun clean inputs on ablated model\n","ablated_logits = model.run_with_hooks(all_tokens,\n","                    fwd_hooks=[\n","                        (layer_name, hook_fn),\n","                    ]\n","                )"],"metadata":{"id":"eRF7nMRQCM9X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Find preds after ablation"],"metadata":{"id":"vFYYG4ZDqkSo"}},{"cell_type":"code","source":["model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","orig_logits = model(all_tokens)"],"metadata":{"id":"bkQbY4ck0CFh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = orig_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"gCD7t3Ntr8_G","executionInfo":{"status":"ok","timestamp":1716303951858,"user_tz":240,"elapsed":562,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cb911323-6f07-4444-b05d-668a7d1674f9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"60FySCAtqmYh","executionInfo":{"status":"ok","timestamp":1716303933836,"user_tz":240,"elapsed":67,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"12767b35-f923-43cd-81f8-d9d813b5ab39"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["','"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["NOTE: successor head can turn ‘3’ into ‘4’, but the model itself will turn ‘3’ into ‘.’ if there is no sequence in the prompt."],"metadata":{"id":"FzufXC871ccP"}},{"cell_type":"code","source":["example_prompt = \"3\"\n","example_answer = \" 4\"\n","# need prepend_bos=False to prev adding EOS token in front\n","utils.test_prompt(example_prompt, example_answer, model, prepend_bos=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"VVcPH2ru0IcU","executionInfo":{"status":"ok","timestamp":1716306404117,"user_tz":240,"elapsed":1059,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"cc7276b0-8165-4f3d-e8d6-57f332781ee1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized prompt: ['3']\n","Tokenized answer: [' 4']\n"]},{"output_type":"display_data","data":{"text/plain":["Performance on answer token:\n","\u001b[1mRank: \u001b[0m\u001b[1;36m79\u001b[0m\u001b[1m       Logit:  \u001b[0m\u001b[1;36m5.96\u001b[0m\u001b[1m Prob:  \u001b[0m\u001b[1;36m0.12\u001b[0m\u001b[1m% Token: | \u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m|\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Performance on answer token:\n","<span style=\"font-weight: bold\">Rank: </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span><span style=\"font-weight: bold\">       Logit:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5.96</span><span style=\"font-weight: bold\"> Prob:  </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.12</span><span style=\"font-weight: bold\">% Token: | </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">|</span>\n","</pre>\n"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Top 0th token. Logit: 10.38 Prob: 10.16% Token: |.|\n","Top 1th token. Logit:  9.72 Prob:  5.27% Token: |,|\n","Top 2th token. Logit:  9.45 Prob:  4.01% Token: |\n","|\n","Top 3th token. Logit:  9.08 Prob:  2.76% Token: |-|\n","Top 4th token. Logit:  8.60 Prob:  1.72% Token: |:|\n","Top 5th token. Logit:  8.47 Prob:  1.50% Token: | and|\n","Top 6th token. Logit:  8.18 Prob:  1.12% Token: |)|\n","Top 7th token. Logit:  8.14 Prob:  1.08% Token: | of|\n","Top 8th token. Logit:  8.12 Prob:  1.06% Token: | to|\n","Top 9th token. Logit:  8.11 Prob:  1.05% Token: |/|\n"]},{"output_type":"display_data","data":{"text/plain":["\u001b[1mRanks of the answer tokens:\u001b[0m \u001b[1m[\u001b[0m\u001b[1m(\u001b[0m\u001b[32m' 4'\u001b[0m, \u001b[1;36m79\u001b[0m\u001b[1m)\u001b[0m\u001b[1m]\u001b[0m\n"],"text/html":["<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Ranks of the answer tokens:</span> <span style=\"font-weight: bold\">[(</span><span style=\"color: #008000; text-decoration-color: #008000\">' 4'</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">79</span><span style=\"font-weight: bold\">)]</span>\n","</pre>\n"]},"metadata":{}}]},{"cell_type":"markdown","source":["## Find highest logit diff"],"metadata":{"id":"nujZ_4lk6pxR"}},{"cell_type":"markdown","source":["See if '3' predict '4' after putting it through the successor head now. Find the correct ID of '4'"],"metadata":{"id":"1e8UFNhiRirJ"}},{"cell_type":"code","source":["corrTok_ID = model.to_tokens('4', prepend_bos=False).item()\n","corrTok_ID"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kGsRlyErnDPs","executionInfo":{"status":"ok","timestamp":1716303453322,"user_tz":240,"elapsed":8,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"49e93d50-916b-4963-f7f0-f7afe1065329"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["19"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["ablated_logits_corrTok: Float[Tensor, \"batch\"] = ablated_logits[:, -1, corrTok_ID]\n","ablated_logits_corrTok.shape"],"metadata":{"id":"oXOvCeGnRhvO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716302729575,"user_tz":240,"elapsed":9,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"335bd1ab-36b4-4888-dadb-9cd3eda6033a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10])"]},"metadata":{},"execution_count":44}]},{"cell_type":"code","source":["orig_logits_corrTok: Float[Tensor, \"batch\"] = orig_logits[:, -1, corrTok_ID]\n","orig_logits_corrTok.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tuLGifmKnm3A","executionInfo":{"status":"ok","timestamp":1716303470308,"user_tz":240,"elapsed":799,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"e0bf2480-b083-415d-a93f-795a37a3eaf7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["answer_logit_diff = orig_logits_corrTok - ablated_logits_corrTok\n","answer_logit_diff"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vJmyw_i9qKcQ","executionInfo":{"status":"ok","timestamp":1716303500544,"user_tz":240,"elapsed":11,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b525b7fe-4050-41b1-b9cd-a25189479c44"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([-1.0787,  1.5292,  1.3172,  1.6084,  1.6472,  1.3106,  1.4058,  1.6542,\n","         1.3492,  1.3374], device='cuda:0', grad_fn=<SubBackward0>)"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["ablated_logits_corrTok"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sWH-xkvqY8D","executionInfo":{"status":"ok","timestamp":1716303535989,"user_tz":240,"elapsed":468,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"529fb468-db28-444a-ee9e-5c930cc8067a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([4.2815, 3.6545, 3.9122, 4.2024, 4.0652, 4.4122, 4.1850, 3.8846, 4.1671,\n","        4.4934], device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["# def get_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], dataset: Dataset, per_prompt=False):\n","#     '''\n","#     '''\n","#     corr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.corr_tokenIDs]\n","#     incorr_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), dataset.word_idx[\"end\"], dataset.incorr_tokenIDs]\n","#     answer_logit_diff = corr_logits - incorr_logits\n","#     return answer_logit_diff if per_prompt else answer_logit_diff.mean()"],"metadata":{"id":"rH4lI8SZQtnW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# heads_not_ablate = []  # ablate all heads but not MLPs\n","# mlps_not_ablate = []  # ablate all MLPs\n","\n","# # ablate\n","\n","# model.reset_hooks(including_permanent=True)  #must do this after running with mean ablation hook\n","\n","# model = add_ablation_hook_MLP_head(model, dataset_2, heads_not_ablate, mlps_not_ablate)\n","# logits_minimal = model(dataset.toks)\n","\n","# # find logit diff\n","# new_score = get_logit_diff(logits_minimal, dataset)"],"metadata":{"id":"8qnrwXk1x884"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# OV Scores with just successor head"],"metadata":{"id":"101rz2AEzWb6"}},{"cell_type":"markdown","source":["## Unablated"],"metadata":{"id":"kyk-1Wht4Gev"}},{"cell_type":"code","source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","cache = {}\n","model.cache_some(cache, lambda x: x == \"blocks.0.hook_resid_post\")\n","model(input_text)\n","# z_0 = model.blocks[1].attn.ln1(cache[\"blocks.0.hook_resid_post\"])\n","z_0 = model.blocks[1].attn.hook_z(cache[\"blocks.0.hook_resid_post\"])\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","logits = model.unembed(model.ln_final(o))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"86dPnCt1zZPB","executionInfo":{"status":"ok","timestamp":1716306986891,"user_tz":240,"elapsed":470,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"047a75a9-01fa-4039-ad8b-85edbdbb8e6b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:root:cache_some is deprecated and will eventually be removed, use add_caching_hooks or run_with_cache\n"]}]},{"cell_type":"code","source":["# pred_tokens = [\n","#                 model.tokenizer.decode(token)\n","#                 for token in torch.topk(\n","#                     logits[seq_idx, dataset.word_idx[word][seq_idx]], k\n","#                 ).indices\n","#             ]"],"metadata":{"id":"YiH_zptR3orO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"5OpC8OMm4A1Q","executionInfo":{"status":"ok","timestamp":1716307114691,"user_tz":240,"elapsed":111,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"61d018fe-2047-44b7-8031-5c4dc632ad8d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":67}]},{"cell_type":"markdown","source":["## Ablated"],"metadata":{"id":"LyuTKLtK4Ifh"}},{"cell_type":"code","source":["cache[\"blocks.0.hook_resid_post\"].shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rCLN23cZ4JWm","executionInfo":{"status":"ok","timestamp":1716307311247,"user_tz":240,"elapsed":79,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ebd49751-e8b0-4fbd-8a6f-a28667cb2723"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 2, 768])"]},"metadata":{},"execution_count":68}]},{"cell_type":"code","source":["LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-uPLH22t40P8","executionInfo":{"status":"ok","timestamp":1716307352284,"user_tz":240,"elapsed":618,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"545a4a65-f127-4c37-8eec-ea01a8f89d0b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 3072])"]},"metadata":{},"execution_count":69}]},{"cell_type":"code","source":["print(model.cfg.d_mlp)\n","model.cfg.d_model"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2OG1myRw6EIG","executionInfo":{"status":"ok","timestamp":1716307680225,"user_tz":240,"elapsed":737,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"2a711347-8669-4444-e22c-3370b91359a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3072\n"]},{"output_type":"execute_result","data":{"text/plain":["768"]},"metadata":{},"execution_count":73}]},{"cell_type":"markdown","source":["OV scores may not use the same approach as successor heads.\n","\n","'blocks.0.mlp.hook_post'\n","\n","vs\n","\n","\"blocks.0.hook_resid_post\""],"metadata":{"id":"ZZlCVHXg76Y6"}},{"cell_type":"markdown","source":["# Re-train SAE on blocks.0.hook_resid_post activations"],"metadata":{"id":"HBQwfMOp8iRs"}},{"cell_type":"markdown","metadata":{"id":"TEaFrpYW9z3v"},"source":["## Get activations to train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I-ovsHYQ9z31"},"outputs":[],"source":["layer_name = 'blocks.0.hook_resid_post'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1716341389383,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"d7cc30a7-1843-45e1-dec4-38b95e577524","id":"QoTE9eI09z31"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2])"]},"metadata":{},"execution_count":13}],"source":["# https://neelnanda-io.github.io/TransformerLens/generated/code/transformer_lens.HookedTransformer.html\n","\n","tokens = model.to_tokens(input_as_str)\n","tokens.shape"]},{"cell_type":"markdown","metadata":{"id":"1qsJO-lp9z31"},"source":["Seq Len is number of tokens, not string max len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RHwEo2BB9z31"},"outputs":[],"source":["# h_store = t.zeros(model_cache['blocks.5.mlp.hook_post'].shape, device=model.cfg.device)\n","seqLen = tokens.shape[1]\n","h_store = t.zeros((len(input_as_str), seqLen, model.cfg.d_model), device=model.cfg.device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1716341389383,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"b2e2bd85-331c-424a-a98a-b2d15bbc6b52","id":"e583Pfgr9z31"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([500, 2, 768])"]},"metadata":{},"execution_count":15}],"source":["h_store.shape"]},{"cell_type":"markdown","metadata":{"id":"ZGBISlgk9z31"},"source":["Use hook fn to avoid storing all activations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tNWEV_Mn9z31"},"outputs":[],"source":["def store_h_hook(\n","    pattern: Float[Tensor, \"batch seqlen d_model\"],\n","    # hook: HookPoint,\n","    hook\n","):\n","    # Store the result.\n","    # h_store = pattern  # this won't work b/c replaces entire thing, so won't be stored\n","    # h_store.append(1) # if h_store = [], this will work\n","    h_store[:] = pattern  # this works b/c changes values, not replaces entire thing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_wfOCpZB9z32"},"outputs":[],"source":["model.run_with_hooks(\n","    tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pPX3yC8w9z32"},"outputs":[],"source":["# h_store  # check actvs are stored"]},{"cell_type":"markdown","metadata":{"id":"hen48PGn9z32"},"source":["## Train SAE"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96,"status":"ok","timestamp":1716308696320,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"31f57619-8288-41f5-f5be-67bae4fe0dd5","id":"C287ClLd9z32"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([500, 2, 768])\n","torch.Size([1000, 1, 768])\n"]}],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","print(h_store.shape)\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], model.cfg.d_model)\n","h_store = h_store.unsqueeze(1)\n","print(h_store.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ni0wWp8Y9z32"},"outputs":[],"source":["# h_store has \"grad_fn=<UnsqueezeBackward0>)\", so get rid of it\n","h = h_store.detach()  # Detaches values from the computation graph\n","# h"]},{"cell_type":"markdown","metadata":{"id":"BUnI5PG-9z32"},"source":[">"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1495,"status":"ok","timestamp":1716308704210,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"579eadf2-07e6-468f-8f3e-4f0a2c58ab5d","id":"nE__IUY-9z32"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 100/100 [00:01<00:00, 97.12it/s, l1_loss=1.15e-6, l2_loss=39.8, lr=0.001]\n"]}],"source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 2, # 8\n","    n_input_ae = h.shape[-1],  # model's n_hidden\n","    n_hidden_ae = 2 * h.shape[-1],  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    l1_coeff = 0.5,\n",")\n","\n","autoencoder = AutoEncoder(ae_cfg, h)\n","\n","data_log = autoencoder.optimize(\n","    steps = 100, # 10_000\n","    log_freq = 200,\n",")"]},{"cell_type":"markdown","source":["## Ablate a SAE feature, then reconstruct"],"metadata":{"id":"t4wR6xKR-zwQ"}},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716309125058,"user_tz":240,"elapsed":954,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"0089ccde-4cb5-4b30-f5f1-2ea9ebdff934","id":"jOGPYPdW-zwi"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":85}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1716309125460,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"8bf5a002-d6d7-4586-fb38-1dab973f6a07","id":"wIchjUJa-zwi"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":86}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"d_lstEzp-zwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":37,"status":"ok","timestamp":1716309125461,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"9747708d-56fa-46a0-a703-5cd9157d0f16","id":"1bi96TH3-zwj"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 768])"]},"metadata":{},"execution_count":88}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":35,"status":"ok","timestamp":1716309125461,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"040b9b35-5b09-4fc5-8a7f-1ff2a1f44ee8","id":"c4OS7bSn-zwj"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 1536])"]},"metadata":{},"execution_count":89}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","source":["# ablate a feature (idx = 0) by setting it to 0\n","acts[:, :, 0] = 0\n","# acts[:, :, 0]"],"metadata":{"id":"qpdUOKoW-zwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"QcHi8GGg-zwj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_reconstructed.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716309125461,"user_tz":240,"elapsed":33,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c933a90b-b877-48b4-f339-527f6b96c451","id":"3cD0L6zD-zwk"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 768])"]},"metadata":{},"execution_count":92}]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"GpZqQzcS-zwk"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716309125462,"user_tz":240,"elapsed":33,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7495cc89-d2e4-48fd-be4b-aa13b4973f63","id":"rkmEN6ZX-zwk"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 768])"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716309125462,"user_tz":240,"elapsed":31,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"12ac9293-bc4f-44eb-bb24-0da0d12d6ef9","id":"kW3g_cx2-zwk"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 768])"]},"metadata":{},"execution_count":94}]},{"cell_type":"markdown","source":["## Save model and actvs"],"metadata":{"id":"H39lcJjMA4lm"}},{"cell_type":"code","source":["from google.colab import files\n","\n","# Save the model's state dictionary\n","model_path = 'autoencoder.pth'\n","torch.save(autoencoder.state_dict(), model_path)\n","\n","# Download the model file\n","files.download(model_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"zRORmrFHBDof","executionInfo":{"status":"ok","timestamp":1716309511016,"user_tz":240,"elapsed":364,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c54aee57-25f2-4fa8-f41e-9154cac9db62"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_9c1a32d0-ea49-43ac-89c4-f09e02229f55\", \"autoencoder.pth\", 18894896)"]},"metadata":{}}]},{"cell_type":"code","source":["# Save the tensor\n","tensor_path = 'LLM_patch.pt'\n","torch.save(LLM_patch, tensor_path)\n","\n","# Download the tensor file\n","files.download(tensor_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"XfMAPFrlBAji","executionInfo":{"status":"ok","timestamp":1716309795297,"user_tz":240,"elapsed":879,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5b72c2eb-b641-4e0b-c69a-061bc2b73892"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_844d5154-c0bf-483f-bc9c-062ea181d9b9\", \"LLM_patch.pt\", 124070)"]},"metadata":{}}]},{"cell_type":"markdown","source":["### load sae"],"metadata":{"id":"bFHHPyiRCJDE"}},{"cell_type":"markdown","source":["Must run \"Get activations to train SAE\" of this section before loading to get h_store"],"metadata":{"id":"OwuXdwbRX8WZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"srElnr4bXzLC"},"outputs":[],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], model.cfg.d_model)\n","h_store = h_store.unsqueeze(1)\n","h = h_store.detach()  # Detaches values from the computation graph"]},{"cell_type":"code","source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 2, # 8\n","    n_input_ae = model.cfg.d_model,  # model's n_hidden\n","    n_hidden_ae = 2 * model.cfg.d_model,  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    l1_coeff = 0.5,\n",")\n","\n","autoencoder_testLoad = AutoEncoder(ae_cfg, h)\n","\n","# Load the model's state dictionary\n","model_path = 'autoencoder.pth'\n","autoencoder_testLoad.load_state_dict(t.load(model_path))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9tHAaTGxBw2u","executionInfo":{"status":"ok","timestamp":1716341475936,"user_tz":240,"elapsed":506,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5f064abb-9ba2-4c18-a361-bee3be76185b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":20}]},{"cell_type":"markdown","source":["### test loading"],"metadata":{"id":"rBixDjDmYTUb"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1ZLGoReCKgt"},"outputs":[],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder_testLoad.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","source":["# ablate a feature (idx = 0) by setting it to 0\n","acts[:, :, 0] = 0\n","# acts[:, :, 0]"],"metadata":{"id":"d_NX2EXDCKg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"5Fj7Yn3BCKg1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_reconstructed.shape"],"metadata":{"id":"1N_3vN0-CKg1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"7aYwbOVOCKg2"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"id":"ahEhzFBMCKg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["LLM_patch_testLoad = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch_testLoad.shape"],"metadata":{"id":"WAsRrsoTCKg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.equal(LLM_patch, LLM_patch_testLoad)"],"metadata":{"id":"SgkYgkOcCEux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tensor_path = 'LLM_patch.pt'\n","LLM_patch_testLoad_2 = torch.load(tensor_path)\n","\n","torch.equal(LLM_patch, LLM_patch_testLoad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":180},"id":"OfdUuwAbCodD","executionInfo":{"status":"error","timestamp":1716315976121,"user_tz":240,"elapsed":492,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"51759180-4769-402b-de3c-e2517f7f362d"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'LLM_patch' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-67-58b160a0c7d7>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mLLM_patch_testLoad_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLM_patch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLLM_patch_testLoad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'LLM_patch' is not defined"]}]},{"cell_type":"markdown","source":["# OV Scores with just successor head (0.hook_resid)"],"metadata":{"id":"sb7TeOZq-LoJ"}},{"cell_type":"markdown","source":["## Ablated"],"metadata":{"id":"3nTIqcZe-UgM"}},{"cell_type":"code","source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","z_0 = model.blocks[1].attn.hook_z(LLM_patch)\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","ablated_logits = model.unembed(model.ln_final(o))"],"metadata":{"id":"zaqbn0ao-UgV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716309176243,"user_tz":240,"elapsed":138,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ebe8c951-7e44-41dc-8acd-ec35b054ec0c","id":"008j5g7e-UgW"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' third'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":96}]},{"cell_type":"markdown","source":["# loop through features to ablate"],"metadata":{"id":"770L0w9RVpJ7"}},{"cell_type":"code","source":["autoencoder = autoencoder_testLoad"],"metadata":{"id":"RFl0Pqy-YpIg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## get feature actvs"],"metadata":{"id":"j7cslBI_WCUy"}},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341531503,"user_tz":240,"elapsed":29,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"207d9ff1-01e1-4831-ae0f-8703a4103f9f","id":"bWAgwpnOVzYS"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27,"status":"ok","timestamp":1716341531503,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"127e4928-891c-492f-9b8d-6fd273746df9","id":"caacpYy-VzYS"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":23}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"lFG2i8mCVzYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":29,"status":"ok","timestamp":1716341531506,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"32918cd2-596b-466c-e17c-762a05e7f45d","id":"o7ilT9euVzYS"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 768])"]},"metadata":{},"execution_count":25}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1716341531506,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"163d74f9-4f35-4b82-a701-48f86c5bb1c4","id":"7E_8yUU_VzYS"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 1536])"]},"metadata":{},"execution_count":26}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","source":["# Count the number of 0s in the tensor\n","num_zeros = (acts == 0).sum().item()\n","\n","# Count the number of nonzeroes in the tensor\n","num_ones = (acts > 0).sum().item()\n","\n","# Calculate the percentage of 1s over 0s\n","if num_zeros > 0:\n","    percentage_ones_over_zeros = (num_ones / num_zeros) * 100\n","else:\n","    percentage_ones_over_zeros = float('inf')  # Handle division by zero\n","\n","print(f\"Number of 0s: {num_zeros}\")\n","print(f\"Number of nonzeroes: {num_ones}\")\n","print(f\"Percentage of 1s over 0s: {percentage_ones_over_zeros:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n8pzNqmyaLMx","executionInfo":{"status":"ok","timestamp":1716341532231,"user_tz":240,"elapsed":753,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"ccc24152-c453-4222-f608-4b451ab8d717"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of 0s: 61440\n","Number of nonzeroes: 0\n","Percentage of 1s over 0s: 0.00%\n"]}]},{"cell_type":"code","source":["post_reshaped == output_tuple[-1]"],"metadata":{"id":"82UJOww5ZJGc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"fo1vV5w-asMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["autoencoder.b_dec"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4769gT-_ay-v","executionInfo":{"status":"ok","timestamp":1716341532231,"user_tz":240,"elapsed":34,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"778929af-7303-4dba-e143-82b4dbbc776c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Parameter containing:\n","tensor([[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","        [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","       device='cuda:0', requires_grad=True)"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["h_reconstructed"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532231,"user_tz":240,"elapsed":31,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"3465ca97-b68e-4079-fc3c-5e5a715ec5bb","id":"JcwrbrWHasMi"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","\n","        [[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","\n","        [[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","\n","        ...,\n","\n","        [[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","\n","        [[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]],\n","\n","        [[ 0.0167, -0.0189,  0.0253,  ...,  0.0348,  0.0081, -0.0144],\n","         [ 0.0118, -0.0120,  0.0297,  ...,  0.0317,  0.0027, -0.0172]]],\n","       device='cuda:0', grad_fn=<AddBackward0>)"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"4G7bMP8lasMj"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532231,"user_tz":240,"elapsed":29,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"b10a923a-d463-4154-e1a5-ae28ff9a7c5a","id":"6wslbCaGasMj"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 768])"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":734,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"21461e8c-cf3e-4d73-8e64-3fa03df27764","id":"15CrGUPRasMj"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 768])"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","z_0 = model.blocks[1].attn.hook_z(LLM_patch)\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","ablated_logits = model.unembed(model.ln_final(o))"],"metadata":{"id":"MHyuRmoWbAVQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":49,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f71e73f2-8728-4e2c-d1f8-39a81a665336","id":"WrzKHmBobAVY"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' third'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","source":["## Ablate a SAE feature, then reconstruct"],"metadata":{"id":"rIsysv3QVzYR"}},{"cell_type":"code","source":["acts[:, :, 0]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BCEXEL2CZCfd","executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":48,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"360e3980-d096-4a7b-bf83-fac4a4000cfa"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.]], device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# ablate a feature (idx) by setting it to 0\n","\n","acts_clone = acts.clone().detach()\n","\n","acts_clone[:, :, 0] = 0\n","acts[:, :, 0]"],"metadata":{"executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":45,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"colab":{"base_uri":"https://localhost:8080/"},"id":"1Vsy4MaaVzYS","outputId":"579608e3-7ee8-4a1a-9d54-d5a7dc51ab7c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.],\n","        [0., 0.]], device='cuda:0', grad_fn=<SelectBackward0>)"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"nrNKGbOdVzYS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["h_reconstructed.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":42,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"5da943a7-6e1b-4373-c464-1ba450e81ef7","id":"lO4Ng88QVzYS"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 768])"]},"metadata":{},"execution_count":39}]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"v7YOvETJVzYT"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532939,"user_tz":240,"elapsed":39,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"394dc3cb-3dcd-4486-d5c6-2b2db2a98ed4","id":"OAtggrN4VzYT"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 768])"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341532940,"user_tz":240,"elapsed":38,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"31377176-d265-4903-96c2-df27fba1156a","id":"p9kx3FLeVzYT"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 768])"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","metadata":{"id":"sGWrvLffbfpy"},"source":["# Train SAE more steps"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":423,"status":"ok","timestamp":1716341723170,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"d1726f1b-034b-4f08-84fd-04e10e1c7517","id":"OYV8_BRPbfpy"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([10, 2, 768])\n","torch.Size([20, 1, 768])\n"]}],"source":["# convert to h dim: \"batch_size * seq_len, n_instances, n_input_ae\"\n","print(h_store.shape)\n","h_store = h_store.reshape(h_store.shape[0] * h_store.shape[1], model.cfg.d_model)\n","h_store = h_store.unsqueeze(1)\n","print(h_store.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M2smLcjWbfpz"},"outputs":[],"source":["# h_store has \"grad_fn=<UnsqueezeBackward0>)\", so get rid of it\n","h = h_store.detach()  # Detaches values from the computation graph\n","# h"]},{"cell_type":"markdown","metadata":{"id":"S4bZZiEdbfpz"},"source":[">"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25760,"status":"ok","timestamp":1716341749273,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"698f789b-78c3-442f-8ae0-32324c60f94e","id":"y66Fl0jvbfpz"},"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10000/10000 [00:26<00:00, 382.32it/s, l1_loss=0.121, l2_loss=29.7, lr=0.001]\n"]}],"source":["ae_cfg = AutoEncoderConfig(\n","    n_instances = 2, # 8\n","    n_input_ae = h.shape[-1],  # model's n_hidden\n","    n_hidden_ae = 2 * h.shape[-1],  # require n_hidden_ae >= n_features. can use R * n_input_ae\n","    l1_coeff = 0.5,\n",")\n","\n","autoencoder = AutoEncoder(ae_cfg, h)\n","\n","data_log = autoencoder.optimize(\n","    steps = 10000, # 10_000\n","    log_freq = 200,\n",")"]},{"cell_type":"markdown","source":["## get feature actvs"],"metadata":{"id":"3QmdTiHgb09M"}},{"cell_type":"code","source":["mod_10_class_3 = [str(i) for i in range(101) if str(i).endswith('3')]\n","mod_10_class_3"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341750319,"user_tz":240,"elapsed":1086,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c6e0ef75-7ad3-49b0-d605-a1ada6dd6838","id":"3KQ5_0s9b09M"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['3', '13', '23', '33', '43', '53', '63', '73', '83', '93']"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":918,"status":"ok","timestamp":1716341750320,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"3814debc-92b5-4f90-85c7-fac08b515df0","id":"QSYxddlxb09N"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2])"]},"metadata":{},"execution_count":60}],"source":["all_tokens = model.to_tokens(mod_10_class_3, prepend_bos=True)\n","all_tokens = all_tokens.to(device)\n","all_tokens.shape"]},{"cell_type":"code","source":["h_store = t.zeros((10, 2, model.cfg.d_model), device=model.cfg.device)\n","\n","model.run_with_hooks(\n","    all_tokens,\n","    return_type = None,\n","    fwd_hooks=[\n","        (layer_name, store_h_hook),\n","    ]\n",")"],"metadata":{"id":"IPEtvgpXb09N"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":909,"status":"ok","timestamp":1716341750321,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"2ce653ed-5393-4b83-e91a-f34ff6ffcd72","id":"SFkZrtSmb09N"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 768])"]},"metadata":{},"execution_count":62}],"source":["# get LLM activs for steering vec\n","post_reshaped = einops.repeat(h_store, \"batch seq d_model -> (batch seq) instances d_model\", instances=2)\n","post_reshaped.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":907,"status":"ok","timestamp":1716341750322,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"},"user_tz":240},"outputId":"aa178a85-41ca-48fc-fa2b-fbd2eda8f9a8","id":"kjcrzTQkb09N"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 2, 1536])"]},"metadata":{},"execution_count":63}],"source":["# use a fwd pass to compute ALL feature actvs for ALL this steering vec\n","output_tuple = autoencoder.forward(post_reshaped)\n","acts = output_tuple[3]\n","acts.shape"]},{"cell_type":"code","source":["# Count the number of 0s in the tensor\n","num_zeros = (acts == 0).sum().item()\n","\n","# Count the number of nonzeroes in the tensor\n","num_ones = (acts > 0).sum().item()\n","\n","# Calculate the percentage of 1s over 0s\n","if num_zeros > 0:\n","    percentage_ones_over_zeros = (num_ones / num_zeros) * 100\n","else:\n","    percentage_ones_over_zeros = float('inf')  # Handle division by zero\n","\n","print(f\"Number of 0s: {num_zeros}\")\n","print(f\"Number of nonzeroes: {num_ones}\")\n","print(f\"Percentage of 1s over 0s: {percentage_ones_over_zeros:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341750322,"user_tz":240,"elapsed":904,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"037ec294-129c-4de3-d18d-8c91d72a6316","id":"B72LZOTCb09N"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of 0s: 61435\n","Number of nonzeroes: 5\n","Percentage of 1s over 0s: 0.01%\n"]}]},{"cell_type":"code","source":["# reconstruct the output\n","\n","# _, l2_loss, _, _, post_reconstructed = autoencoder.forward(post_reshaped) # this doesn't use the ablated actvs\n","\n","h_reconstructed = einops.einsum(\n","            acts, autoencoder.normalize_and_return_W_dec(),\n","            \"batch_size n_instances n_hidden_ae, n_instances n_hidden_ae n_input_ae -> batch_size n_instances n_input_ae\"\n","        ) + autoencoder.b_dec"],"metadata":{"id":"4dsp3Q_mb09O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is the output of two SAEs. We only need one, so `h_reconstructed[:, 0, :]`. Then, we can rearrange it to LLM dims (before, could not do this with two SAEs)."],"metadata":{"id":"VoBcsw8Jb09O"}},{"cell_type":"code","source":["h_reconstructed_1 = h_reconstructed[:, 0, :]\n","h_reconstructed_1.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341750323,"user_tz":240,"elapsed":901,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"1f38a521-f570-46a0-92c2-30a2b66edb06","id":"hvPz18-cb09O"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([20, 768])"]},"metadata":{},"execution_count":66}]},{"cell_type":"code","source":["LLM_patch = einops.rearrange(h_reconstructed_1, \"(batch seq) d_model -> batch seq d_model\", batch=10)\n","LLM_patch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1716341750323,"user_tz":240,"elapsed":896,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"c96f276f-18de-4199-b41b-5c37d5a9917d","id":"O6GSVDH7b09O"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([10, 2, 768])"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["import torch\n","layer, head = 9, 1\n","input_text = '3'\n","\n","z_0 = model.blocks[1].attn.hook_z(LLM_patch)\n","\n","v = torch.einsum(\"eab,bc->eac\", z_0, model.blocks[layer].attn.W_V[head])\n","v += model.blocks[layer].attn.b_V[head].unsqueeze(0).unsqueeze(0)\n","\n","o = torch.einsum(\"sph,hd->spd\", v, model.blocks[layer].attn.W_O[head])\n","ablated_logits = model.unembed(model.ln_final(o))"],"metadata":{"id":"Y0GbX1dHb09P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["next_token = ablated_logits[0, -1].argmax(dim=-1)\n","next_char = model.to_string(next_token)\n","next_char"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1716341750324,"user_tz":240,"elapsed":892,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"f44ef50b-c537-4989-d4ef-77d413bd2fc3","id":"DWNuLlxbb09P"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'must'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["10000 steps: sometimes is all 0, sometimes there are 5 VALUES out of 61435 (0.01%) that are non zero (these aren’t counting rows). This predicts ‘must’. Reconstruction is bad- because the ablation didn’t do much (first row likely all 0s anyways) it’s the reconstruction that messes up."],"metadata":{"id":"LeRXEYeK83qx"}},{"cell_type":"code","source":["from google.colab import files\n","model_path = 'autoencoder_2.pth'\n","torch.save(autoencoder.state_dict(), model_path)\n","files.download(model_path)"],"metadata":{"id":"NUUCTMNr8-ci","executionInfo":{"status":"ok","timestamp":1716341979968,"user_tz":240,"elapsed":381,"user":{"displayName":"Michael Lan","userId":"13558259605338023275"}},"outputId":"7a90ab01-c694-4cf0-d258-d261d8c6e930","colab":{"base_uri":"https://localhost:8080/","height":34}},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.Javascript object>"],"application/javascript":["download(\"download_356d7b61-8d16-40f3-942d-14676e91d82c\", \"autoencoder_2.pth\", 18894912)"]},"metadata":{}}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-WMqbetGrf4e","1jCAYFKmz92O","TioC1OirumVa","b0_UTee0lQL3","BdgIFHMcuJu5","vFYYG4ZDqkSo","nujZ_4lk6pxR","kyk-1Wht4Gev","LyuTKLtK4Ifh","hen48PGn9z32"],"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyP2372Ye4EGMVR8wOihqU7M"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"74353f0b87d24ac4b39c80aea0f121cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6bc5449b4067474b9fb3f1357ca2d86f","IPY_MODEL_422ce902d8814484998f208ac60d19d1","IPY_MODEL_81eaabfbdbd24aecb66a268abcd28430"],"layout":"IPY_MODEL_7115294f177e4e8186b3b4e58b60c289"}},"6bc5449b4067474b9fb3f1357ca2d86f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f286476a04b546b4991549766a1e9e0b","placeholder":"​","style":"IPY_MODEL_f50c38d861f344ff82a4ab83e8368ec3","value":"config.json: 100%"}},"422ce902d8814484998f208ac60d19d1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e1da54d08e74a458cfd82acdcc57830","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1bea1566eab42ed96e48390fde871d8","value":665}},"81eaabfbdbd24aecb66a268abcd28430":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_53dd3c6197564e3bbf5a38616027f241","placeholder":"​","style":"IPY_MODEL_2a24d50b98dc4118a0eccdcf3467d3c7","value":" 665/665 [00:00&lt;00:00, 47.3kB/s]"}},"7115294f177e4e8186b3b4e58b60c289":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f286476a04b546b4991549766a1e9e0b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f50c38d861f344ff82a4ab83e8368ec3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0e1da54d08e74a458cfd82acdcc57830":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1bea1566eab42ed96e48390fde871d8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"53dd3c6197564e3bbf5a38616027f241":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a24d50b98dc4118a0eccdcf3467d3c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a134aed1bd7d46c9ac63cc9e06d7d563":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8bf62a3e4880462b9eb919c46a2d6534","IPY_MODEL_759160f59976479eb7cd3f4dc6e9add6","IPY_MODEL_8082da7b7a6f4cedb36f75ba716d1d03"],"layout":"IPY_MODEL_14d1956b4d7640459c3bef915f3eae4e"}},"8bf62a3e4880462b9eb919c46a2d6534":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c811e3a020364d39876a20a39a416e43","placeholder":"​","style":"IPY_MODEL_4323ec043289405aa53a3d651fa5f117","value":"model.safetensors: 100%"}},"759160f59976479eb7cd3f4dc6e9add6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d19db1d3aa5e4c0c8efdd37c81408184","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_da319db376cf45948c8bc22bfdd3c46d","value":548105171}},"8082da7b7a6f4cedb36f75ba716d1d03":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_19d6b5a2a4c14a619279f84d4942ec6d","placeholder":"​","style":"IPY_MODEL_72425ee47c274d4fa38394c12fff484e","value":" 548M/548M [00:02&lt;00:00, 200MB/s]"}},"14d1956b4d7640459c3bef915f3eae4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c811e3a020364d39876a20a39a416e43":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4323ec043289405aa53a3d651fa5f117":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d19db1d3aa5e4c0c8efdd37c81408184":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da319db376cf45948c8bc22bfdd3c46d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"19d6b5a2a4c14a619279f84d4942ec6d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72425ee47c274d4fa38394c12fff484e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"060bdd839aa942e29ea503a985ecf0df":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_54fed247b8194fbdb300d79ca8208604","IPY_MODEL_58c48753e2d04ab980945ee4bbc988cb","IPY_MODEL_9050b94d638149beaaf600eec5d97f29"],"layout":"IPY_MODEL_3e1c1c65523b4d8fa88a71d8ae6a6a20"}},"54fed247b8194fbdb300d79ca8208604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f2bb69c744b483083221d316965d840","placeholder":"​","style":"IPY_MODEL_87228e91a0794c67ba5d146af1b33a64","value":"generation_config.json: 100%"}},"58c48753e2d04ab980945ee4bbc988cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3504a4fbc95f4a758b75627a2329e88e","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8450168844d24e5eaa30f7609645eca1","value":124}},"9050b94d638149beaaf600eec5d97f29":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_567865120cef4ea5b0d09451c655a6b7","placeholder":"​","style":"IPY_MODEL_a9ab8139437d4f128d9deff4388d9fa5","value":" 124/124 [00:00&lt;00:00, 9.18kB/s]"}},"3e1c1c65523b4d8fa88a71d8ae6a6a20":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1f2bb69c744b483083221d316965d840":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"87228e91a0794c67ba5d146af1b33a64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3504a4fbc95f4a758b75627a2329e88e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8450168844d24e5eaa30f7609645eca1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"567865120cef4ea5b0d09451c655a6b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9ab8139437d4f128d9deff4388d9fa5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"056702453e324120bee12836ee7ddc74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9b00644d201a4d3e8d54ce6236622326","IPY_MODEL_566c03a6fd3940bea2899588f95f348e","IPY_MODEL_df0210da08b849d990eead52a64e3373"],"layout":"IPY_MODEL_ad3fdff4172b4d16a3cf0ffd46cecaeb"}},"9b00644d201a4d3e8d54ce6236622326":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3842210b5534db09cc6d071192aed3e","placeholder":"​","style":"IPY_MODEL_d939b7d285544f5fb19ce1921f6ba7fc","value":"tokenizer_config.json: 100%"}},"566c03a6fd3940bea2899588f95f348e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_64464fee388f4941a48605bd6c8162cb","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ad86ec32ed2c4972b88ee4f67a965f38","value":26}},"df0210da08b849d990eead52a64e3373":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d4718fdf2ad4b6db785319a13393da2","placeholder":"​","style":"IPY_MODEL_1b3d163ce2354172bac1f5fce04996c1","value":" 26.0/26.0 [00:00&lt;00:00, 1.89kB/s]"}},"ad3fdff4172b4d16a3cf0ffd46cecaeb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3842210b5534db09cc6d071192aed3e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d939b7d285544f5fb19ce1921f6ba7fc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"64464fee388f4941a48605bd6c8162cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ad86ec32ed2c4972b88ee4f67a965f38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d4718fdf2ad4b6db785319a13393da2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1b3d163ce2354172bac1f5fce04996c1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c6c29f8f9b6d4af5b67e88fab508b51d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d800265b60d34325a9d0d0d67d592d16","IPY_MODEL_8728d665282845799d6d9ca7c6d7e07e","IPY_MODEL_fd6713ce2a894d82b8a48b1b8bb8c4cf"],"layout":"IPY_MODEL_d5a9e4e4e9c04737a6f041058db67669"}},"d800265b60d34325a9d0d0d67d592d16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93e1e6db87bd40f8bc78d8a159c52a00","placeholder":"​","style":"IPY_MODEL_0f533db1de2945ac96b9cdd39a397a84","value":"vocab.json: 100%"}},"8728d665282845799d6d9ca7c6d7e07e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e31ed7f91e6465e829bacd72d937788","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_51387d00e2624ab1a38b861305fd000b","value":1042301}},"fd6713ce2a894d82b8a48b1b8bb8c4cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_af52350079a6490b89afedc1fec13ae6","placeholder":"​","style":"IPY_MODEL_233e5bb9250c448a981b52d0dfa870d1","value":" 1.04M/1.04M [00:00&lt;00:00, 9.32MB/s]"}},"d5a9e4e4e9c04737a6f041058db67669":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"93e1e6db87bd40f8bc78d8a159c52a00":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f533db1de2945ac96b9cdd39a397a84":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e31ed7f91e6465e829bacd72d937788":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51387d00e2624ab1a38b861305fd000b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"af52350079a6490b89afedc1fec13ae6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"233e5bb9250c448a981b52d0dfa870d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"856fd623b3fb455eba9eb042977e17ac":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a99069d928a43bcb6e631237eec39d5","IPY_MODEL_4b7ca037df494b80bc6365d1250eb229","IPY_MODEL_c07a570abb74491faa21675922404455"],"layout":"IPY_MODEL_62cd4f781e0d4e32822747839dae5eb5"}},"2a99069d928a43bcb6e631237eec39d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_08430cd03a224cf08dbc0e9241f77a70","placeholder":"​","style":"IPY_MODEL_3482b19c81064f5183a90a3d94e5d4b3","value":"merges.txt: 100%"}},"4b7ca037df494b80bc6365d1250eb229":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9cf5c339644c467da694401bc07ebff6","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4107620ad0d64e399e34e1a089d50968","value":456318}},"c07a570abb74491faa21675922404455":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5cf2d8519b244859899ea0593db39bcb","placeholder":"​","style":"IPY_MODEL_40e71fde282e4f4b93529433d1a8d6db","value":" 456k/456k [00:00&lt;00:00, 8.42MB/s]"}},"62cd4f781e0d4e32822747839dae5eb5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08430cd03a224cf08dbc0e9241f77a70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3482b19c81064f5183a90a3d94e5d4b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9cf5c339644c467da694401bc07ebff6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4107620ad0d64e399e34e1a089d50968":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5cf2d8519b244859899ea0593db39bcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40e71fde282e4f4b93529433d1a8d6db":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a6c82e8397442688bec7c3c87101cd8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bd2449ce91464b01a3e557dc45a2969c","IPY_MODEL_14732b33384a4d33ad80979dc5435fb6","IPY_MODEL_64695457c963408ea375e3a7103cb82d"],"layout":"IPY_MODEL_e1f3c3c6659946fa8216a58de455d895"}},"bd2449ce91464b01a3e557dc45a2969c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_129b64be0ba8469985d02ff6b3ddf31d","placeholder":"​","style":"IPY_MODEL_69483f5a764649498a806961ef6fb152","value":"tokenizer.json: 100%"}},"14732b33384a4d33ad80979dc5435fb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ae4c4375dc04468aa7152d301fb8e17","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_73bdcad70c884f038bcc68381e4eef9e","value":1355256}},"64695457c963408ea375e3a7103cb82d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba6df6abdb3e404586588640f9c6181b","placeholder":"​","style":"IPY_MODEL_d5953f9f56954b158107c6b932e2d897","value":" 1.36M/1.36M [00:00&lt;00:00, 15.7MB/s]"}},"e1f3c3c6659946fa8216a58de455d895":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"129b64be0ba8469985d02ff6b3ddf31d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69483f5a764649498a806961ef6fb152":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ae4c4375dc04468aa7152d301fb8e17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"73bdcad70c884f038bcc68381e4eef9e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba6df6abdb3e404586588640f9c6181b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5953f9f56954b158107c6b932e2d897":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}